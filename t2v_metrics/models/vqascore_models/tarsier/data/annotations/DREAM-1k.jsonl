{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character emerges from smoke and fire, then half knelt on the ground and made a gesture to give instructions, while another group of characters rushed forward and grabbed two green monsters.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 1, "extra_info": {"duration": 5.09, "events": ["A character emerges from smoke and fire.", "The character half kneels on the ground.", "The character makes a gesture to give instructions.", "A group of characters rushes forward.", "The group of characters grabs two green monsters."], "n_events": 5, "n_shots": 2, "n_subjects": 4}, "vid": 1}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One character raises their hand and then puts it down. The other character walks away and is followed by the former, both characters go up a staircase together.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 2, "extra_info": {"duration": 9.67, "events": ["One character raises their hand.", "The same character puts their hand down.", "The other character walks away.", "The first character follows the other.", "Both characters go up a staircase together."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 2}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Several animals jumped into a yellow pipe one by one, and the last brown bird jumped directly off the platform.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 3, "extra_info": {"duration": 4.88, "events": ["Animals jumped into a yellow pipe", "A brown bird jumped off the platform"], "n_events": 2, "n_shots": 1, "n_subjects": 6}, "vid": 3}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A mouse is pushing a bowl and making a frightened expression. A cat falling down, causing the bowl to fly up. A mouse holding a key is covered under the bowl. The cat grabbing the bowl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 4, "extra_info": {"duration": 4.05, "events": ["A mouse pushes a bowl.", "A mouse makes a frightened expression.", "A cat falls down.", "The bowl flies up.", "A mouse holds a key.", "A mouse is covered under the bowl.", "The cat grabs the bowl."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 4}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The black cat blows out a candle. Then the scene turns to this black cat holding a bowl when suddenly a plate hits its head.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 5, "extra_info": {"duration": 4.05, "events": ["The black cat blows out a candle.", "The black cat holds a bowl.", "A plate hits the black cat's head."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 5}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two animated birds fly towards the door lock first and have a brief conversation. Then they fly out of a stone window into the sunlight.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 6, "extra_info": {"duration": 6.13, "events": ["Two animated birds fly towards the door lock.", "Animated birds have a brief conversation.", "Animated birds fly out of the stone window.", "Animated birds enter the sunlight."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 6}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two birds hover over a dog and pull the dog's ears upwards. A white horse appears in the doorway of the cottage, observing the interaction between the birds and the dog.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 7, "extra_info": {"duration": 8.22, "events": ["Two birds hover over a dog", "The birds pull the dog's ears upwards", "A white horse appears in the doorway", "The horse observes the interaction"], "n_events": 4, "n_shots": 1, "n_subjects": 4}, "vid": 7}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman moves around a man and drags him into a door. A blue cat-like observer is watching aside.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 8, "extra_info": {"duration": 9.54, "events": ["A woman moves around a man.", "A woman drags the man into a door.", "A blue cat-like observer watches."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 8}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Camera pans right in an old industrial workshop to a snowy landscape, displaying group of penguins.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 9, "extra_info": {"duration": 13.84, "events": ["Camera pans right in an old industrial workshop", "Camera displays a snowy landscape", "Group of penguins appears"], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 9}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The character picks up a knife and attempts to cut their arm.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 10, "extra_info": {"duration": 20.77, "events": ["The character picks up a knife.", "The character attempts to cut their arm."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 10}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A knight in blue and gold armor duels with an opponent who leaps for an aerial attack.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 11, "extra_info": {"duration": 1.29, "events": ["A knight in blue and gold armor duels.", "Opponent leaps for an aerial attack."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 11}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A knight on horseback engages in a sword fight with a soldier, falls off the horse, and other knights and soldiers fight in the background.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 12, "extra_info": {"duration": 2.17, "events": ["A knight on horseback engages in a sword fight with a soldier.", "The knight falls off the horse.", "Other knights and soldiers fight in the background."], "n_events": 3, "n_shots": 1, "n_subjects": 4}, "vid": 12}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men close a window behind them, then a woman appears behind the window and smashes it with a hammer, causing the men to look at the shattered window and then turn to continue talking to each other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 13, "extra_info": {"duration": 5.67, "events": ["Two men close a window behind themselves.", "A woman appears behind the window.", "The woman smashes the window with a hammer.", "The window shatters.", "The two men look at the shattered window.", "The two men turn to continue talking."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 13}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Numerous small creatures crawl upward from the ground to the top of a car's exterior, causing two characters inside the car to react.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 14, "extra_info": {"duration": 1.71, "events": ["Small creatures crawl upward from the ground.", "Creatures reach the top of the car's exterior.", "Two characters inside the car react."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 14}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl whistles as she and the boy look upward together. The camera angle switches from their front to their back, and a large bird flies toward them. The large bird grabs the boy and flies away, while the girl turns to watch the direction in which the large bird is flying. The bird ascends into the sky, where other birds and burning objects are present.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 15, "extra_info": {"duration": 5.38, "events": ["The girl whistles.", "The girl and the boy look upward together.", "A large bird flies toward the girl and the boy.", "The large bird grabs the boy.", "The large bird flies away with the boy.", "The girl turns to watch the large bird.", "The large bird ascends into the sky.", "Other birds are present in the sky.", "Burning objects are present in the sky."], "n_events": 9, "n_shots": 3, "n_subjects": 4}, "vid": 15}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man jumps into mid-air, collides with the surface of the ice, the ice cracks, and the man slowly slides down to the ground, with two characters standing behind him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 16, "extra_info": {"duration": 5.12, "events": ["A man jumps into mid-air.", "The man collides with the ice surface.", "The ice cracks.", "The man slides down to the ground.", "Two characters stand behind the man."], "n_events": 5, "n_shots": 2, "n_subjects": 1}, "vid": 16}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The elephant was sitting on a person and a gorilla, and turned its head to look at them. The person and the gorilla stood up with effort, and the elephant leaned backward. Then the person ran forward, and the gorilla lifted the elephant and looked in the direction the person had gone.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 17, "extra_info": {"duration": 3.75, "events": ["The elephant sat on a person and a gorilla.", "The elephant turned its head to look at them.", "The person and the gorilla stood up with effort.", "The elephant leaned backward.", "The person ran forward.", "The gorilla lifted the elephant.", "The gorilla looked in the direction the person had gone."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 17}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman in green clothes opens her mouth wide and places her hands on her hips, while the woman in purple clothes clenches her fist. The camera switches to a palace where a man and the woman in purple clothes are dancing hand in hand, with the woman's gaze following the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 18, "extra_info": {"duration": 2.62, "events": ["The woman in green clothes opens her mouth wide.", "The woman in green clothes places her hands on her hips.", "The woman in purple clothes clenches her fist.", "The man and the woman in purple clothes dance hand in hand.", "The woman in purple clothes follows the man with her gaze."], "n_events": 5, "n_shots": 2, "n_subjects": 4}, "vid": 18}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character is suspended in midair, making gestures and speaking as the camera pans down to a group of characters gathered in front of a door, among them one wielding a flaming sword. Then, the crowd begins to fight, with the one holding the flaming sword swinging it around and knocking a person to the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 19, "extra_info": {"duration": 8.62, "events": ["A character is suspended in midair.", "The character makes gestures.", "The character speaks.", "The camera pans down to a group of characters.", "A character wields a flaming sword.", "The crowd begins to fight.", "The character with the flaming sword swings it around.", "The character with the flaming sword knocks a person to the ground."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 19}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A creature bites into a bottle of drink to crush it, then its eyes widen; the creature and the boy start to dance, with the creature lying on the ground looking up at the boy.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 20, "extra_info": {"duration": 6.88, "events": ["Creature bites into a bottle of drink", "Creature crushes the bottle", "Creature's eyes widen", "Creature and boy start to dance", "Creature lies on the ground", "Boy looks at creature"], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 20}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera takes a bird's-eye view, a red-haired monster looks up, its mouth moves slightly, the camera pans upwards, and the monsters are arranged into a heart shape.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 21, "extra_info": {"duration": 7.0, "events": ["Camera takes a bird's-eye view", "Red-haired monster looks up", "Red-haired monster's mouth moves slightly", "Camera pans upwards", "Monsters arrange into a heart shape"], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 21}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy and the baby turned to face the mirror, and at the same time, sunglasses slide down from above their heads and are put on. Then the two begin to dance. The camera zooms in on the car's rear windshield with a sign that reads \"Baby Inside\" as the car moves forward and passes a sign for a dog company. The camera pans up past the sign to reveal a large building belonging to the dog company, which is emitting thick smoke.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 22, "extra_info": {"duration": 9.5, "events": ["The boy and the baby turned to face the mirror", "Sunglasses slide down from above onto their heads", "The boy and the baby began to dance", "The camera zooms in on the car's rear windshield", "The car moves forward", "The car passes a sign for a dog company", "A sign for the dog company reads 'Baby Inside'", "The camera pans up past the dog company sign", "The large building belonging to the dog company is revealed", "Thick smoke emits from the large building"], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 22}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A blue car was being chased by a police car on a winding road at night. At an intersection, another police car came and joined the pursuit. The newly joined police car collided with another police car behind it, and a hubcap flew towards the camera. The other police cars continued to chase the blue car. The camera then cuts to the interior of the car, showing the passenger side where a cow, resting its hand on the backrest, turns its head to look at and speak to the camera, with the rearview mirror reflecting the police cars.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 23, "extra_info": {"duration": 5.0, "events": ["A blue car was chased by a police car on a winding road.", "At an intersection, another police car joined the pursuit.", "The newly joined police car collided with another police car.", "A hubcap flew towards the camera.", "The other police cars continued the chase.", "A cow rested its hand on the backrest.", "The cow turned its head to look at the camera.", "The cow spoke to the camera.", "The rearview mirror reflected the police cars."], "n_events": 9, "n_shots": 2, "n_subjects": 1}, "vid": 23}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man falls from the cliff on the ice surface, dropping the manuscript he was holding off the cliff. He grabs onto the ice with his hands, causing scratches to appear on the surface. In the end, a creature falls down, and the scene concludes with it being suspended by a rope.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 24, "extra_info": {"duration": 3.5, "events": ["A man falls from the cliff.", "The man drops the manuscript.", "The manuscript falls off the cliff.", "The man grabs onto the ice.", "Scratches appear on the ice surface.", "A creature falls down.", "The creature is suspended by a rope."], "n_events": 7, "n_shots": 4, "n_subjects": 2}, "vid": 24}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The door opened, two dogs were thrown into the trash bin, a girl came out from the door and yelled, then she closed the door.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 25, "extra_info": {"duration": 3.38, "events": ["The door opened", "Two dogs were thrown into the trash bin", "A girl came out from the door", "The girl yelled", "The girl closed the door"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 25}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A beaver is speaking and then bends down to pick up an object and pushes it towards the fox's face. The scene switches to a creature taking off its shoes, kicking them forward, standing on the ground with feet apart, and the camera pans up to show the creature opening its mouth.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 26, "extra_info": {"duration": 9.5, "events": ["A beaver speaks.", "A beaver bends down.", "A beaver picks up an object.", "A beaver pushes the object towards the fox's face.", "A creature takes off its shoes.", "A creature kicks its shoes forward.", "A creature stands on the ground with feet apart.", "A camera pans up.", "A creature opens its mouth."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 26}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The blue character wearing a hat holds a fork and reacts as he looks ahead, the camera then cuts to another blue character who is hiding behind a wooden curtain observing, and then closes the curtain.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 27, "extra_info": {"duration": 2.88, "events": ["The blue character holds a fork.", "The blue character reacts.", "Another blue character hides behind a curtain.", "The wooden curtain closes."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 27}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The dog is standing on a drum, wagging its tail to DJ, while the green creature is playing the piano.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 28, "extra_info": {"duration": 3.75, "events": ["The dog stands on a drum.", "The dog wags its tail.", "The DJ plays music.", "The green creature plays the piano."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 28}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man picks up a baby dressed in a puppy costume, the camera switches back and forth between the man and the baby. A boy lies on the ground watching them as the baby sticks out its tongue and licks towards the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 29, "extra_info": {"duration": 6.0, "events": ["A man picks up a baby.", "The baby, dressed in a puppy costume, licks towards the man.", "A boy lies on the ground.", "The baby sticks out its tongue."], "n_events": 4, "n_shots": 5, "n_subjects": 3}, "vid": 29}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A dog with a red scarf is sitting on an object, flying forward. The camera switches to a moving train with white smoke emerging above. An airplane is flying, turning a corner. A dog with a purple scarf stands on the airplane, looking backward. The dog with the red scarf makes a move, accelerating through the air, approaching the airplane. Both dogs reach out their paws toward each other. Their scarves flutter in the wind. The ears of the dog with the purple scarf are also blown by the wind. The dog with the purple scarf clutches the tail wing of the airplane, turns its head, as the plane flies away. The purple scarf drifts into view, covering the camera, and the screen fades to black.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 30, "extra_info": {"duration": 8.75, "events": ["A dog sits on a flying object.", "The train emits white smoke.", "An airplane turns a corner.", "A dog stands on the airplane.", "The dog with red scarf accelerates through the air.", "Both dogs reach out their paws.", "The dog with purple scarf clutches the tail wing.", "The purple scarf flutters in the wind.", "Ears of the dog with purple scarf are blown by wind.", "The purple scarf covers the camera."], "n_events": 10, "n_shots": 6, "n_subjects": 2}, "vid": 30}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two dogs on the grass look forward and wag their tails, behind them a flock of sheep. The camera then cuts to a city park, where a black wolf enters the frame from the left, baring its teeth and looking ahead before lowering its mouth to the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 31, "extra_info": {"duration": 5.75, "events": ["Two dogs look forward.", "Two dogs wag their tails.", "A flock of sheep stands behind the two dogs.", "A black wolf enters the frame from the left.", "The black wolf bares its teeth.", "The black wolf looks ahead.", "The black wolf lowers its mouth to the ground."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 31}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man wearing the yellow jacket stepped forward, crossed his arms over his chest and made a gesture of thinking. He looked at the woman beside him who was holding a child. The woman blinked, and then the man spoke to her, waving his hands in front of his chest.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 32, "extra_info": {"duration": 6.88, "events": ["The man stepped forward", "The man crossed his arms", "The man made a thinking gesture", "The woman holding a child blinked", "The man spoke to the woman", "The man waved his hands"], "n_events": 6, "n_shots": 3, "n_subjects": 3}, "vid": 32}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The robot took a step forward, holding out a toy in the direction of the child, then threw it to the ground. After that, it took out a backpack, and the child approached and quickly took the backpack. Then, a hand reached into the backpack and turned to look at the robot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 33, "extra_info": {"duration": 5.0, "events": ["The robot took a step forward.", "The robot held out a toy.", "The robot threw the toy to the ground.", "The robot took out a backpack.", "The child approached the robot.", "The child quickly took the backpack.", "A hand reached into the backpack.", "The hand turned to look at the robot."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 33}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A stone fell into the pond, splashing water, and the dinosaur by the riverbank turned its head to look back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 34, "extra_info": {"duration": 3.88, "events": ["A stone fell into the pond.", "Water splashed.", "The dinosaur turned its head.", "The dinosaur looked back."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 34}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A large and fluffy dog was scared and backed away by a black dog, revealing a little dog hiding beneath. The little dog hung its head and walked forward, as two black dogs from behind approached, encircling the little dog with the black dog in front. The black dogs lowered their heads, watching the little dog, which became frightened.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 35, "extra_info": {"duration": 4.75, "events": ["A large and fluffy dog backed away.", "A black dog scared the large and fluffy dog.", "A little dog was revealed hiding beneath the large and fluffy dog.", "The little dog hung its head.", "The little dog walked forward.", "Two black dogs approached from behind.", "The black dogs encircled the little dog.", "The black dogs lowered their heads.", "The black dogs watched the little dog.", "The little dog became frightened."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 35}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man looks up in surprise, gazes upward, then turns and runs away. The wall behind him collapses, and the man leaps to avoid it but is then struck down to the ground by the debris.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 36, "extra_info": {"duration": 7.0, "events": ["Man looks up in surprise.", "Man gazes upward.", "Man turns.", "Man runs away.", "Wall collapses.", "Man leaps.", "Debris strikes man to the ground."], "n_events": 7, "n_shots": 2, "n_subjects": 1}, "vid": 36}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The light shines on a boy and a robot, the boy stops his moving body, then turns around, the robot turns its head, a car with its headlights on approaches, and then stops.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 37, "extra_info": {"duration": 3.12, "events": ["Light shines on a boy and a robot", "The boy stops moving", "The boy turns around", "The robot turns its head", "A car approaches with headlights on", "The car stops"], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 37}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The characters turned their heads to look up, and the woman in the middle spoke. The elder at a higher position tore off his clothes and lifted his spear, showing off his abs and said something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 38, "extra_info": {"duration": 4.12, "events": ["Characters turned their heads upwards.", "The woman in the middle spoke.", "The elder tore off his clothes.", "The elder lifted his spear.", "The elder showed off his abs.", "The elder said something."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 38}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The green creature looked in the direction the mermaid was pointing. Following that, the mermaid snatched the key from the creature's neck, took a glance behind, and then swam away. The creature turned around and chased after her. Holding the key, the mermaid swam towards a passage and bent down to hand over the key.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 39, "extra_info": {"duration": 4.0, "events": ["The green creature looked in the direction the mermaid pointed.", "The mermaid snatched the key from the green creature's neck.", "The mermaid took a glance behind.", "The mermaid swam away.", "The green creature turned around.", "The green creature chased after the mermaid.", "The mermaid swam towards a passage.", "The mermaid bent down.", "The mermaid handed over the key."], "n_events": 9, "n_shots": 2, "n_subjects": 2}, "vid": 39}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of dogs are barking at an elderly person and a child. They take a few steps backward, with a rope extending upwards behind both the elderly person and the child. The elderly person holds something up to block, while the child clings to the elder's arm, ducking backward. The camera then switches to a black dog that has jumped up from below, staring ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 40, "extra_info": {"duration": 5.5, "events": ["A group of dogs bark at an elderly person and a child.", "The elderly person and the child take a few steps backward.", "A rope extends upwards behind the elderly person and the child.", "The elderly person holds something up to block.", "The child clings to the elderly person's arm.", "The child ducks backward.", "A black dog jumps up from below.", "The black dog stares ahead."], "n_events": 8, "n_shots": 3, "n_subjects": 5}, "vid": 40}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A flock of birds flew out from the bushes, startling a woman who was facing away from the camera. The woman turned to look at the creature behind her as the camera cut to a large bear emerging from the underbrush, with a cub following close behind, its head the only part visible amidst the grass. The shot then switched to a single-log bridge made from a dead tree. The woman walked out from the right side of the frame, followed by the bear. Both the woman and the bear headed towards the log bridge, with the ground covered in snow.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 41, "extra_info": {"duration": 7.5, "events": ["A flock of birds flew out from the bushes.", "A woman turned to look behind her.", "A large bear emerged from the underbrush.", "A cub followed the large bear.", "The woman walked out from the right side of the frame.", "The large bear followed the woman.", "The woman headed towards the log bridge.", "The large bear headed towards the log bridge.", "The ground was covered in snow."], "n_events": 9, "n_shots": 3, "n_subjects": 4}, "vid": 41}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A massive robot and a person run toward the elevator with other robots chasing after them. The person presses the elevator button, but the elevator doors won't close because a dog is crouching in the middle of the doorway. The dog turns its head to look at the person.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 42, "extra_info": {"duration": 6.38, "events": ["A massive robot and a person run toward the elevator.", "Other robots chase after the massive robot and the person.", "The person presses the elevator button.", "The elevator doors fail to close.", "A dog crouches in the elevator doorway.", "The dog turns its head to look at the person."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 42}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera moves to the right, the blue fish turns its head to observe its surroundings, then turns around to look at the sea anemone which is in motion. The scene shifts to the inside of the sea anemone where two clownfish are sleeping. The blue fish makes a facial expression and speaks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 43, "extra_info": {"duration": 5.25, "events": ["The blue fish turns its head.", "The blue fish turns around.", "The sea anemone is in motion.", "Two clownfish are sleeping inside the sea anemone.", "The blue fish makes a facial expression.", "The blue fish speaks."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 43}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A rabbit carrying a group of Smurfs runs forward, and the camera pans to the right to show the hills and the woods.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 44, "extra_info": {"duration": 5.38, "events": ["A rabbit carries a group of Smurfs.", "The rabbit runs forward."], "n_events": 2, "n_shots": 1, "n_subjects": 3}, "vid": 44}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera zooms in on a group photo and a hand turning off a switch. The woman closes the door, and glowing stars shine through the darkness of the room. The camera pans around the room, and the girl takes out a glowing object from under the blanket.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 45, "extra_info": {"duration": 13.25, "events": ["Camera zooms in on a group photo", "Hand turns off a switch", "Woman closes the door", "Glowing stars shine through darkness", "Camera pans around the room", "Girl takes out glowing object from under blanket"], "n_events": 6, "n_shots": 5, "n_subjects": 2}, "vid": 45}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man claps and speaks as the table with a face opens its eyes, floating and flying around the room. A character turns their head in reaction to this, and the table with a face stops beside the other tables.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 46, "extra_info": {"duration": 8.5, "events": ["The man claps.", "The man speaks.", "The table with a face opens its eyes.", "The table with a face floats.", "The table with a face flies around the room.", "A character turns their head.", "The table with a face stops flying.", "The table with a face positions itself beside other tables."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 46}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The pirate captain faces the camera and expresses an emotion, then rotates his eyes and tilts his head to look behind him. The crew members behind him are conversing. The captain adjusts his hat and then looks towards the camera, revealing a smile and making a gesture.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 47, "extra_info": {"duration": 10.25, "events": ["Pirate captain expresses an emotion.", "Pirate captain rotates his eyes.", "Pirate captain tilts his head.", "Crew members converse.", "Pirate captain adjusts his hat.", "Pirate captain reveals a smile.", "Pirate captain looks towards the camera.", "Pirate captain makes a gesture."], "n_events": 8, "n_shots": 3, "n_subjects": 2}, "vid": 47}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy looked at the skeleton, then turned and ran to the water's edge. He then raised his hands high, kicked the water, and splashed around, while the skeleton watched from the side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 48, "extra_info": {"duration": 3.88, "events": ["The boy looked at the skeleton.", "The boy turned.", "The boy ran to the water's edge.", "The boy raised his hands high.", "The boy kicked the water.", "The boy splashed around.", "The skeleton watched from the side."], "n_events": 7, "n_shots": 2, "n_subjects": 2}, "vid": 48}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a classroom, a female character looks on in surprise at a device in front of her that generates a large amount of electric current, which then surges towards a frog.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 49, "extra_info": {"duration": 5.62, "events": ["A female character looks on in surprise.", "A device generates a large amount of electric current.", "Electric current surges towards a frog."], "n_events": 3, "n_shots": 4, "n_subjects": 3}, "vid": 49}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A letter moves forward, a man in the snow walks ahead. Then the envelope falls down in front of the man. The scene shifts to a close-up of the man turning his head to look to the side, the camera zooms out as the man looks to the other side, then he bends down preparing to pick up the letter on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 50, "extra_info": {"duration": 20.75, "events": ["A letter moves forward", "A man walks in the snow", "The envelope falls to the ground", "The man turns his head", "The man looks to the side", "The camera zooms out", "The man prepares to pick up the letter"], "n_events": 7, "n_shots": 3, "n_subjects": 1}, "vid": 50}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lens with a reticle and a complex interface scans over the crowd as people dance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 51, "extra_info": {"duration": 3.25, "events": ["A lens with a reticle scans over the crowd.", "People dance."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 51}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wearing a mask walks forward, passing by a little white rabbit carrying a bucket. The camera angle switches to film from under a tree up towards a treehouse, from which the heads of two characters subsequently peek out.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 52, "extra_info": {"duration": 3.88, "events": ["A character walks forward.", "A little white rabbit carries a bucket.", "Two characters peek out from a treehouse."], "n_events": 3, "n_shots": 2, "n_subjects": 4}, "vid": 52}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A child character is talking to an adult character. The adult character raises their right hand and places a finger to their lips. The child character walks forward, comes to a box filled with Halloween items, and reveals a surprised expression.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 53, "extra_info": {"duration": 5.38, "events": ["Child character talks to adult character.", "Adult character raises their right hand.", "Adult character places a finger to their lips.", "Child character walks forward.", "Child character comes to a box.", "Box is filled with Halloween items.", "Child character reveals a surprised expression."], "n_events": 7, "n_shots": 2, "n_subjects": 2}, "vid": 53}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character with blonde hair opened the door, and a little girl and an adult in green clothing stood at the entrance. The little girl was holding a hat with a green brim, with its opening facing upwards. The camera then switched to the interior, where many characters dressed in Halloween decorations were inside the house, eating and chatting.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 54, "extra_info": {"duration": 5.38, "events": ["Blonde-haired character opened the door.", "Little girl stood at the entrance.", "Adult in green clothing stood at the entrance.", "Little girl held a hat with green brim.", "Characters dressed in Halloween decorations were inside the house.", "Characters ate.", "Characters chatted."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 54}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A female character turns off the room's light and then closes the door; the camera switches to the outside, where the lady, along with a child character dressed in green, walks toward the courtyard entrance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 55, "extra_info": {"duration": 5.62, "events": ["Female character turns off light", "Female character closes the door", "Female character walks toward courtyard", "Child character follows the female character"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 55}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three children characters huddle their heads together. Then two characters walk off to the left side of the screen, and the character with the blue cape walks into a door and then closes it behind them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 56, "extra_info": {"duration": 5.38, "events": ["Three children characters huddle their heads together.", "Two characters walk off to the left side.", "Character with blue cape enters a door.", "Character with blue cape closes the door behind them."], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 56}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room, the wall facing us has white letters that read \"This is my tree house\". The camera zooms in on the white text, and then cuts to a character dressed in grey clothing with glasses showing a surprised expression. He blinks once, and then the scene refreshes, the character's clothing changes to white, and the window next to him turns into a wooden wall.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 57, "extra_info": {"duration": 4.5, "events": ["Camera zooms in on the white text.", "Character shows a surprised expression.", "Character blinks once.", "Scene refreshes.", "Character's clothing changes to white.", "Window turns into a wooden wall."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 57}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "There are already two adults, a man and a woman, sitting on the sofa. A little girl comes to the sofa and sits down. The female character hands the little girl a neatly folded green blanket, which the girl then unfolds and covers her legs with.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 58, "extra_info": {"duration": 7.75, "events": ["Two adults sit on the sofa.", "A little girl comes to the sofa.", "The little girl sits down on the sofa.", "The female adult hands the little girl a green blanket.", "The little girl unfolds the green blanket.", "The little girl covers her legs with the green blanket."], "n_events": 6, "n_shots": 3, "n_subjects": 3}, "vid": 58}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little girl is talking to a kitten, then she lifts the kitten onto her lap, and then picks up the round tag on the kitten's neck to look at the words on it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 59, "extra_info": {"duration": 5.88, "events": ["A little girl talks to a kitten.", "The little girl lifts the kitten onto her lap.", "The little girl picks up the round tag on the kitten's neck.", "The little girl looks at the words on the round tag."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 59}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At an intersection, a child and an adult watch as a group of panicked people run from left to right. Someone has lost their hat in the rush. The camera then shifts to the direction from which the people are running, revealing a monster with its arms wide open, swaying as it moves forward, kicking aside a wooden barrel that blocks its path.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 60, "extra_info": {"duration": 5.5, "events": ["A child and an adult watch a group run.", "People run from left to right.", "A person loses their hat.", "A group of people exhibit panic.", "A monster sways forward.", "The monster extends its arms wide.", "The monster kicks a wooden barrel aside.", "Panicked people rush at an intersection."], "n_events": 8, "n_shots": 2, "n_subjects": 4}, "vid": 60}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wearing short sleeves reaches out and leads a character in a red cape out of the room. The camera then cuts to a television screen where subtitles are scrolling upwards. The camera switches again to show seven characters watching TV in the room.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 61, "extra_info": {"duration": 9.88, "events": ["A character reaches out.", "A character leads another character.", "Subtitles scroll upwards on a television screen.", "Seven characters watch television."], "n_events": 4, "n_shots": 3, "n_subjects": 3}, "vid": 61}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man dressed in a blue suit looks to the right; the camera then cuts to him pushing open a door to look inside the room. The shot switches to the interior of the room, where a large number of gift boxes are arranged on and under the table.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 62, "extra_info": {"duration": 6.75, "events": ["A man looks to the right.", "A man pushes open a door.", "A large number of gift boxes are arranged on a table.", "Gift boxes are arranged under a table."], "n_events": 4, "n_shots": 3, "n_subjects": 1}, "vid": 62}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wearing a hat moves their head downward and then lifts it up, followed by using both hands to scoop water from below to wash their face. Then, the character cups their hands together to draw a handful of water to sip into their mouth. Afterward, the camera switches to a full view, where the character with the hat is kneeling in front of the toilet, hands placed on the toilet seat. Subsequently, they press a button on the side of the toilet, and a stream of water shoots upward from the toilet, to which the hatted character leans forward to drink.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 63, "extra_info": {"duration": 9.25, "events": ["Character wearing a hat moves head downward.", "Character lifts head up.", "Character uses hands to scoop water.", "Character washes face with water.", "Character cups hands to draw water.", "Character sips water into mouth.", "Character kneels in front of the toilet.", "Character places hands on toilet seat.", "Character presses button on the toilet.", "Water shoots upward from the toilet."], "n_events": 10, "n_shots": 2, "n_subjects": 1}, "vid": 63}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wearing a suit and glasses is looking at and speaking to the screen ahead. The shot switches to the screen, where another character takes a cellphone away from their ear and quickly shrinks to disappear into the screen. On the screen, three charts are displayed showing downward trends. The reflection in the screen shows the face of the character with glasses. The shot then switches back to the front of the character, who takes off his glasses and places them on the table, leans back in his chair, and then raises his hand to touch his forehead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 64, "extra_info": {"duration": 7.12, "events": ["Character wearing suit speaks to the screen.", "Another character takes cellphone away from their ear.", "Another character shrinks to disappear into the screen.", "Three charts showing downward trends are displayed.", "The reflection reveals the face of the character with glasses.", "Character wearing suit takes off his glasses.", "Glasses are placed on the table.", "Character wearing suit leans back in chair.", "Character wearing suit raises hand to touch forehead."], "n_events": 9, "n_shots": 3, "n_subjects": 2}, "vid": 64}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A security guard used his hand to block the intense light coming from ahead as he looked forward. A tall character approached the security guard, lifted his foot, and kicked the guard, sending him flying back into a wall. The security guard then slid to the ground as two security guards inside a window on the wall showed expressions of shock and horror. Subsequently, one of the guards quickly ran away, pointing outside the window, while the other guard, after taking a sip of water, hurriedly ran off as well.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 65, "extra_info": {"duration": 5.12, "events": ["Security guard blocked light with hand.", "Tall character approached security guard.", "Tall character kicked security guard.", "Security guard sent flying into wall.", "Security guard slid to ground.", "Two security guards showed shock and horror.", "One security guard pointed outside window.", "One security guard took water sip.", "Same security guard ran off."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 65}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character with black hair got one of his feet stuck in the door and was lying on the ground. He propped himself up to stand, his foot still stuck. He glanced at the object in his hand, then turned his head to look behind him. Someone was watching him. Frightened, he hurriedly closed the door and leaned against it with a scared expression on his face.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 66, "extra_info": {"duration": 5.25, "events": ["Character's foot got stuck in the door.", "Character lay on the ground.", "Character propped himself up.", "Character's foot remained stuck.", "Character glanced at the object in hand.", "Character turned head to look behind.", "Character noticed someone watching.", "Character, frightened, hurriedly closed the door.", "Character leaned against the door.", "Character displayed a scared expression."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 66}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wearing shorts gets their foot stuck in a door. Another character, dressed in black, throws a punch at the shorts-wearing character. The person in shorts twists their body against the door and simultaneously opens it. The character in black charges through the door and falls to the ground. As the door closes, the shorts-wearing character also falls to the floor, dropping something from their hand. A short person rushes over to snatch the item, but the individual in shorts quickly grabs it and stands up, holding the item high with one hand and steadying themselves on the ground with one foot. The short person jumps several times but is unable to reach it. The character in shorts throws a punch and sends the short person flying.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 67, "extra_info": {"duration": 6.5, "events": ["Character's foot gets stuck in door.", "Character in black throws punch.", "Character in shorts opens door twisting body.", "Character in black charges through door.", "Character in black falls to ground.", "Door closes, character in shorts falls.", "Character in shorts drops item from hand.", "Short person rushes to snatch item.", "Character in shorts grabs item and stands up.", "Character in shorts punches, sends short person flying."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 67}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is holding a microphone and speaking. Next to her, a man is watching her. The woman extends the microphone forward, and the scene switches to six girls on the opposite side, where a character with green hair raises her hand to speak.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 68, "extra_info": {"duration": 2.62, "events": ["A woman holds a microphone.", "A woman speaks.", "A man watches the woman.", "The woman extends the microphone forward.", "A character with green hair raises her hand."], "n_events": 5, "n_shots": 2, "n_subjects": 4}, "vid": 68}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of animated girl characters and a puppy are in a room, looking towards the door. Then one of the girls shows a frightened expression. The camera cuts to the door as a shadow kicks it open and staggers in. As the scene brightens, it reveals that the shadow is a girl holding several bath towels, with a cowboy hat on one of them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 69, "extra_info": {"duration": 5.88, "events": ["A group of animated girl characters and a puppy look towards the door", "One of the girls shows a frightened expression", "A shadow kicks the door open", "The shadow staggers in", "The scene brightens", "It reveals the girl holding several bath towels", "A cowboy hat is on one of the towels"], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 69}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl with pink hair is saying something as she walks backwards, the girl with black hair blinks as she looks ahead. The camera cuts to the top of a bookshelf, a hand reaches up, not able to reach anything, then two hands reach up, still unable to reach anything.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 70, "extra_info": {"duration": 4.12, "events": ["The girl with pink hair walks backwards", "The girl with black hair blinks", "A hand reaches up to the top of the bookshelf", "Two hands reach up towards the top of the bookshelf"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 70}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of animated characters each holds a gemstone in their hands, with the gemstones emitting a dazzling light. The camera cuts to a pair of feet that also shine brightly, which are then replaced by a sparkling pair of shoes. The camera switches again to a hand that glows dazzlingly, and then a brown glove is put on. Afterwards, the character, wearing the glove, clenches the fist tightly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 71, "extra_info": {"duration": 3.62, "events": ["Animated characters hold gemstones.", "Gemstones emit dazzling light.", "Feet shine brightly.", "Sparkling shoes replace feet.", "Hand glows dazzlingly.", "Character puts on brown glove.", "Character clenches fist tightly."], "n_events": 7, "n_shots": 3, "n_subjects": 2}, "vid": 71}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A young boy is walking forward, holding the hand of a girl in blue clothing. Behind them, a girl with red and yellow hair stands with her arms crossed, looking at them angrily. Where they have just left, colorful specks of light are scattered. The girl with red and yellow hair's expression turns to astonishment. The camera then switches to show the back of the young boy and the girl in blue. The colorful specks of light fall from the pockets of the boy's shorts. The girl with yellow and red hair, along with a small dog, watches them as they continue to walk ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 72, "extra_info": {"duration": 7.38, "events": ["A young boy walks forward holding a girl's hand.", "A girl in blue clothing accompanies the young boy.", "A girl with red and yellow hair stands with arms crossed.", "A girl with red and yellow hair looks angrily.", "Colorful specks of light scatter from a location.", "The girl with red and yellow hair's expression changes to astonishment.", "Colorful specks of light fall from the boy's pockets.", "A small dog watches the young boy and the girl.", "The young boy and the girl in blue continue walking."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 72}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man in a white short-sleeved shirt rides an electric scooter on the road, raising one hand and waving to the right, then weaving through traffic. The camera cuts to a scene in front of a billboard advertisement as the man rides the electric scooter down some steps and turns to the left, then leans back to greet a lady hanging clothes. The lady opens her eyes wide in surprise.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 73, "extra_info": {"duration": 7.12, "events": ["A man rides an electric scooter on the road.", "Man raises hand and waves to the right.", "Man weaves through traffic.", "Man rides the electric scooter down some steps.", "Man turns to the left.", "Man leans back to greet a lady.", "Lady hanging clothes opens her eyes wide in surprise."], "n_events": 7, "n_shots": 2, "n_subjects": 3}, "vid": 73}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little boy in a white short-sleeved shirt is showing off something to a little girl in green clothes. The little girl makes an unusually angry face and gesture at the boy, then the boy quickly runs away, looking back and laughing at the girl as he does. The little girl chases after the boy, and a plump auntie watches them from behind. They are chasing each other down a hallway, with the girl trying to kick the boy, but he runs away.\n\nThe scene shifts to the front of a chicken's gravestone, and the camera pans back from the gravestone as a pair of hands scatter some chicken feathers into a hole in front of it and then bury it with earth. The camera then switches to the opposite side of the gravestone, where the boy and girl are crying in front of it. Shortly after, the boy puts one hand on the girl's shoulder, and then both children take out a chicken drumstick with their left hands and start eating.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 74, "extra_info": {"duration": 11.0, "events": ["A boy shows something to a girl.", "The girl makes an angry face and gesture at the boy.", "The boy runs away, laughing.", "The girl chases after the boy.", "The boy evades the girl's attempted kick.", "A plump auntie watches the children from behind.", "A pair of hands scatters chicken feathers into a hole.", "A pair of hands buries the hole with earth.", "The boy and girl cry in front of the gravestone.", "The boy and girl start eating chicken drumsticks."], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 74}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man wearing a hat is speaking frantically to someone in front of him. The camera switches to a view of the man's back, at his buttocks, where a red tail appears. The tail touches the butt of a woman in front of him. The camera then cuts to their faces, where the woman gives the man a coquettish and annoyed look, and then closes her eyes and moves to kiss him. As they kiss, the man's mouth transforms into a red animal-like snout. The man's eyes widen with shock. After kissing the woman, the man covers his mouth and quickly runs off. The woman looks in the direction of the man as he runs away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 75, "extra_info": {"duration": 8.25, "events": ["A man speaks frantically to someone.", "A red tail appears from the man's back.", "The tail touches the woman's butt.", "The woman gives the man a coquettish and annoyed look.", "The woman closes her eyes.", "The woman moves to kiss the man.", "The man's mouth transforms into an animal-like snout.", "The man's eyes widen with shock.", "The man covers his mouth.", "The man runs off."], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 75}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A chubby woman is about to serve some rice, standing next to her is a boy, and then they both turn their heads to look in the direction of the camera. The shot changes to the woman opening the front door, where a man wearing glasses and a long-haired girl are standing. The camera then cuts back to the chubby woman, who says something to the man and girl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 76, "extra_info": {"duration": 5.75, "events": ["A woman prepares to serve rice.", "A boy stands next to the woman.", "The woman and boy turn heads towards the camera.", "The woman opens the front door.", "A man wearing glasses stands at the door.", "A long-haired girl stands next to the man.", "The woman speaks to the man and girl."], "n_events": 7, "n_shots": 3, "n_subjects": 4}, "vid": 76}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A boy in a blue suit turns around and crouches in front of a pile of gift boxes. There is space built with gift boxes in front, leaving a small window. The boy looks through the window to see a girl with long hair crying inside.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 77, "extra_info": {"duration": 5.75, "events": ["A boy turns around.", "A boy crouches.", "A boy looks through a small window.", "A girl cries inside a space."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 77}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the dead of night, a car sways as it approaches from a distance towards the camera. There are two people inside the vehicle. From high above in the distance, a large iron ball is suspended by a chain, swinging towards the car. Atop the iron ball stands a man in white clothing. The man in white jumps off the iron ball, opens his arms, and scrambles to the back of the car. Through the rear car window, the man can be seen hanging on with both arms, looking inside the vehicle, before he ultimately closes his eyes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 78, "extra_info": {"duration": 7.5, "events": ["A car sways approaching the camera.", "Two people sit inside the vehicle.", "A large iron ball swings towards the car.", "A man in white stands atop the iron ball.", "The man in white jumps off the iron ball.", "The man in white opens his arms.", "The man in white scrambles to the back of the car.", "The man in white hangs on with both arms.", "The man in white looks inside the vehicle.", "The man in white closes his eyes."], "n_events": 10, "n_shots": 4, "n_subjects": 3}, "vid": 78}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a scene underwater, where a person releases two grab handles with both hands. The camera then cuts to the face of a boy and subsequently switches to the boy's back as a statue of a Chinese dragon sinks into the depths of the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 79, "extra_info": {"duration": 5.5, "events": ["A person releases two grab handles underwater.", "The statue of a Chinese dragon sinks into the water."], "n_events": 2, "n_shots": 3, "n_subjects": 1}, "vid": 79}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A boy and a girl were on the rooftop of a building, both holding a kite as they ran to the edge of the roof and threw the kite out. The kite plunged straight down, and the two of them looked at the kite string in their hands with disappointment. Then suddenly, their eyes widened; the camera cuts to behind them, where the kite rose from below and spread all its arms.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 80, "extra_info": {"duration": 5.38, "events": ["A boy and a girl were holding a kite on a rooftop.", "They ran to the edge of the roof.", "The boy and the girl threw the kite out.", "The kite plunged straight down.", "The boy and the girl looked at the kite string.", "The boy and the girl showed disappointment.", "Their eyes widened.", "The kite rose from below.", "The kite spread all its arms."], "n_events": 9, "n_shots": 3, "n_subjects": 2}, "vid": 80}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera looks forward from within a crowd, where a group of people are playing drums, with many spectators watching alongside. Then, the drummers and drums part to either side, clearing a path down the middle, through which a lion dancer emerges. The scene switches to a view within the crowd, where a bald man and a man with a crew cut are angrily staring ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 81, "extra_info": {"duration": 3.12, "events": ["The camera looks forward from within a crowd.", "People are playing drums.", "Spectators are watching the performance.", "The drummers and drums part to either side.", "A path down the middle clears.", "A lion dancer emerges through the path.", "Bald man stares angrily ahead.", "Man with a crew cut stares angrily ahead."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 81}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A gentleman in a suit handed a piece of blue clothing to the bespectacled man to his right, who in turn passed the clothing to the short-haired woman beside him. The short-haired woman pressed the clothing with an iron and then handed it to the short-bearded man on her right. The short-bearded man spritzed some perfume on the clothing and passed it to a hand beside him, which then took the clothing inside the house.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 82, "extra_info": {"duration": 4.12, "events": ["The gentleman handed a piece of blue clothing.", "The bespectacled man passed the clothing to the woman.", "The short-haired woman pressed the clothing with an iron.", "The woman handed the clothing to the short-bearded man.", "The short-bearded man spritzed perfume on the clothing.", "A hand took the clothing inside the house."], "n_events": 6, "n_shots": 1, "n_subjects": 5}, "vid": 82}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A boy with a backpack runs while watering the flowers in the corridor with a watering can. He puts down the can and runs up the stairs. At the stair corner, two men are playing chess while a small dog watches on. The boy runs past and tosses the fish in his hand to the dog, then continues upwards. The dog catches the fish and settles back down, while the two chess players remain in the same position but with their eyes widened.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 83, "extra_info": {"duration": 4.12, "events": ["A boy runs while watering flowers.", "The boy puts down the watering can.", "The boy runs up the stairs.", "Two men play chess at the stair corner.", "A small dog watches the two men.", "The boy tosses a fish to the dog.", "The dog catches the fish.", "The chess players widen their eyes."], "n_events": 8, "n_shots": 2, "n_subjects": 4}, "vid": 83}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy sits on the bed, leaning against the wall, his eyes gazing diagonally at the wall behind him. On the other side of the wall, several people press their ears against it, listening to the sounds. Suddenly, they widen their eyes and quickly leave, all except for one big-headed kid who reacts a bit more slowly. The camera pans to the left, revealing the entire room of the boy. The boy turns his eyes forward and sits up, supporting himself with his hands on the bed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 84, "extra_info": {"duration": 6.38, "events": ["The boy gazes diagonally at the wall.", "People press their ears against the wall.", "People's eyes widen.", "People quickly leave the room.", "One big-headed kid reacts slowly.", "The boy sits up.", "The boy supports himself with his hands on the bed."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 84}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the stage, a person is performing martial arts as the lights shine down. The performer is a boy wearing a blue suit. The camera switches to the audience, where a girl in the middle watches without an expression, while the others are smiling and clapping. The camera switches back to the stage, where, after performing a gesture, the boy kicks upward into a split stance and then holds still in place.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 85, "extra_info": {"duration": 5.62, "events": ["A person performs martial arts on stage.", "Lights shine down on stage.", "The boy wears a blue suit.", "The audience watches the performance.", "A girl watches without expression.", "Others smile and clap.", "The boy performs a gesture.", "The boy kicks upward into a split stance.", "The boy holds still in place."], "n_events": 9, "n_shots": 3, "n_subjects": 3}, "vid": 85}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A red dragon flies into the sky, a firework explodes, bursting into a screen full of golden light. The red dragon passes through the golden glow, with fireworks continuing to bloom behind it. The red dragon starts to fall downwards; underneath its head, there is a man and a woman being swung side to side. The fireworks behind them extinguish into black smoke as the red dragon wobbles downward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 86, "extra_info": {"duration": 6.12, "events": ["A red dragon flies into the sky.", "A firework explodes in the sky.", "The red dragon passes through the golden glow.", "Fireworks continue to bloom behind the red dragon.", "The red dragon starts to fall downwards.", "A man and a woman are swung side to side.", "Fireworks extinguish into black smoke.", "The red dragon wobbles downward."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 86}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a hallway, four animated characters and a cat are bounced into the air by a spring on the floor. They open their eyes wide and their mouths in excitement, with light emanating from behind them. The scene then cuts to a different angle, looking up from below as they land on the floor above. A male adult turns around, making a victorious gesture, while the others walk away to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 87, "extra_info": {"duration": 7.25, "events": ["Four animated characters and a cat are bounced into the air", "Characters and cat open their eyes wide", "Characters and cat open their mouths in excitement", "Light emanates from behind the characters and cat", "Characters and cat land on the floor above", "A male adult turns around", "The male adult makes a victorious gesture", "Other characters walk away to the right"], "n_events": 8, "n_shots": 3, "n_subjects": 2}, "vid": 87}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room filled with books and sundries, a blue cat walks from right to left, the camera moves from bottom to top, revealing multiple hammocks, with a red creature on the top layer swinging looking out through a round window.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 88, "extra_info": {"duration": 6.12, "events": ["A blue cat walks from right to left.", "The camera moves from bottom to top.", "A red creature swings in the top hammock.", "The red creature looks out through a round window."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 88}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a black screen, and towards the end, a blue cat enters the screen from the bottom right side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 89, "extra_info": {"duration": 5.12, "events": ["A black screen appears.", "A blue cat enters from the bottom right."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 89}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A soldier in blue attire with a large beard and another soldier with a green helmet hold their guns and look forward. Opposite them, two soldiers wearing gas masks also hold their guns aimed at them. The camera shifts back to the soldier in blue with the large beard; he tilts his head slightly. Then the camera switches to a low-angle shot looking up at the two soldiers with gas masks as they gaze at a dog on the other side. Both of them crouch down, put down their guns, and prepare to surrender.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 90, "extra_info": {"duration": 9.12, "events": ["A soldier in blue attire holds a gun.", "Another soldier with a green helmet holds a gun.", "Two soldiers wearing gas masks aim their guns.", "Soldier in blue attire tilts his head.", "Two soldiers with gas masks gaze at the dog.", "Two soldiers with gas masks crouch down.", "Two soldiers with gas masks put down their guns.", "Two soldiers with gas masks prepare to surrender."], "n_events": 8, "n_shots": 4, "n_subjects": 3}, "vid": 90}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The two large doors were open, and a soldier wearing a helmet charged in on horseback. Following him, another soldier dressed in blue also rode in through the doors. After entering, the blue-clad soldier immediately dismounted and turned to close the large doors.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 91, "extra_info": {"duration": 5.25, "events": ["The doors opened", "A soldier charged in on horseback", "Another soldier rode in", "The blue-clad soldier dismounted", "The blue-clad soldier closed the doors"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 91}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A soldier is running forward holding a dog, while another soldier is sitting on the ground, watching them run past, panting heavily.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 92, "extra_info": {"duration": 4.5, "events": ["A soldier runs forward.", "The soldier holds a dog.", "Another soldier sits on the ground.", "The sitting soldier watches them run past.", "The sitting soldier pants heavily."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 92}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two soldiers are holding guns aimed forward, with a dog by their side. Opposite them, three surrendering soldiers raise their hands high, slowly walking through the intersection of two trenches. The two soldiers slowly turn, continuously aiming their guns at the three surrendering soldiers.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 93, "extra_info": {"duration": 3.75, "events": ["Two soldiers aim guns forward", "A dog stands by the soldiers' side", "Three soldiers raise hands high", "Surrendering soldiers walk through the intersection", "Two soldiers turn slowly", "Two soldiers continuously aim at the surrendering soldiers"], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 93}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the map, a red dashed line snakes from left to right and stops at a marker. A vehicle enters from the left side of the map and gradually covers most of the dashed line.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 94, "extra_info": {"duration": 4.75, "events": ["A red dashed line snakes across the map.", "The line stops at a marker.", "A vehicle enters from the left side of the map.", "The vehicle covers most of the dashed line."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 94}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An officer wearing a hat reached out and patted the head of the puppy in front of him, and the puppy picked up an object in front and sat on the ground looking at the officer as the officer reached out to take the object from the dog's mouth. Behind the officer stood a person at attention, motionless.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 95, "extra_info": {"duration": 4.12, "events": ["The officer patted the puppy's head.", "The puppy picked up an object.", "The puppy sat on the ground.", "The puppy looked at the officer.", "The officer reached out to the puppy.", "The officer took the object from the puppy's mouth.", "A person stood at attention behind the officer."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 95}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera tilts upward to capture a deep red sky, a hand emerges from a distant trench and throws a grenade. In the trench nearby, visible in the frame, a soldier crouches with hands over his head at the edge of the trench. The scene switches to two soldiers running along the trench when an explosion occurs at the edge, the blast wave knocking them down. One soldier lies on the ground, gasping for air.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 96, "extra_info": {"duration": 4.38, "events": ["A hand throws a grenade from a trench.", "A soldier crouches with hands over his head.", "Two soldiers run along the trench.", "An explosion occurs at the trench edge.", "The blast wave knocks two soldiers down.", "One soldier lies on the ground gasping for air."], "n_events": 6, "n_shots": 3, "n_subjects": 3}, "vid": 96}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A soldier in a blue helmet turned his head to nod towards the back, and the camera switched to a soldier in green who glanced at the blue-helmeted soldier, then looked forward in confusion. The blue-helmeted soldier had already stood up and charged into the dense fog ahead. After a while, the camera switched back to the man in green, who was accompanied by a dog. Both climbed out of the trench and rushed forward following the direction of the blue-helmeted soldier.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 97, "extra_info": {"duration": 11.12, "events": ["A soldier nodded towards the back.", "A soldier glanced at another soldier.", "A soldier stood up.", "A soldier charged into dense fog.", "A man climbed out of the trench.", "A dog climbed out of the trench.", "Man and dog rushed forward."], "n_events": 7, "n_shots": 5, "n_subjects": 3}, "vid": 97}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the trench, the soldier in green uniform charged towards the pile of wood ahead. He took off his helmet and moved one plank after another, revealing a dog lying inside.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 98, "extra_info": {"duration": 4.12, "events": ["Soldier charged towards the pile of wood", "Soldier took off his helmet", "Soldier moved one plank", "Soldier revealed a dog lying inside"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 98}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the open ground in front of a building, a soldier in green clothes and a dog are playing. The dog runs towards the distant buildings, leaps up, then turns and runs back to the soldier. Both the person and the dog run around in circles.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 99, "extra_info": {"duration": 6.62, "events": ["A soldier plays with a dog.", "The dog runs towards buildings.", "The dog leaps up.", "The dog turns.", "The dog runs back to the soldier.", "The soldier and the dog run in circles."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 99}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three soldiers look toward the camera in surprise. Then, the soldier in the middle walks forward, out of the frame. The soldier wearing glasses on the left follows him, whereas the soldier on the right remains in place, watching the direction the first two soldiers left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 100, "extra_info": {"duration": 3.75, "events": ["Three soldiers look toward the camera in surprise.", "The soldier in the middle walks forward.", "The soldier in the middle exits the frame.", "The soldier wearing glasses follows the soldier in the middle.", "The soldier on the right stays in place."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 100}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A soldier propped himself up on one arm in bed, looked at the dog in front of him, then extinguished the kerosene lamp and lay down on his side to sleep.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 101, "extra_info": {"duration": 3.12, "events": ["Soldier propped up on one arm in bed.", "Soldier looked at the dog.", "Soldier extinguished the kerosene lamp.", "Soldier lay down on his side.", "Soldier went to sleep."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 101}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A soldier opened his eyes, with a dog standing on him. The dog jumped off him, and the soldier got out of bed and stood up. Inside the tent, two other soldiers also sat up from their beds.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 102, "extra_info": {"duration": 4.0, "events": ["A soldier opened his eyes.", "A dog stood on the soldier.", "The dog jumped off the soldier.", "The soldier got out of bed.", "The soldier stood up.", "Inside the tent, two other soldiers sat up.", "Two other soldiers got out of their beds."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 102}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a bomb crater, a soldier with a blue helmet takes out a hand grenade and hands it to a soldier next to him in a green uniform. The soldier in green is aiming his gun forward and immediately takes the grenade and tosses it ahead. Then the blue helmet soldier lies down and takes aim forward with his gun, while the soldier in green covers his head with both hands and ducks down. Many soldiers are charging forward behind them. The scene switches to the opposing position, which is shrouded in smoke and deserted.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 103, "extra_info": {"duration": 4.25, "events": ["Soldier in a bomb crater takes out a hand grenade.", "Soldier with blue helmet hands grenade to soldier in green uniform.", "Soldier in green uniform tosses hand grenade ahead.", "Soldier with blue helmet lies down, takes aim with gun.", "Soldier in green uniform covers head, ducks down.", "Many soldiers charge forward."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 103}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three figures walked through the corridor from the end of the passage, the camera switched to the back view of the three figures, with the last person carrying a backpack, from which the head of a dog poked out.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 104, "extra_info": {"duration": 5.0, "events": ["Three figures walked through the corridor.", "The last person carried a backpack.", "A dog's head poked out from the backpack."], "n_events": 3, "n_shots": 2, "n_subjects": 4}, "vid": 104}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the trench, a soldier wearing glasses looked ahead. To his left, another soldier stood up, holding a dog and then also looked forward. To the left of the soldier holding the dog, another soldier stood up and faced forward. The camera then switched to what they were looking at: the sun was rising on the horizon, and a bird perched on the antlers of a deer nearby.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 105, "extra_info": {"duration": 5.38, "events": ["A soldier in the trench looked ahead.", "Another soldier stood up while holding a dog.", "A third soldier stood up.", "All soldiers faced forward.", "The sun rose on the horizon.", "A bird perched on the deer's antlers."], "n_events": 6, "n_shots": 2, "n_subjects": 4}, "vid": 105}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the other side of the camera lens, a character peeks out from beside a door. A character wearing green clothes turns to look at the person peeking. The person who was peeking moves from beside the door to the center of the doorway, standing there uncertainly for a moment. Then, they crouch down and slowly shift to the other side of the door before quickly vanishing. Two other characters in the room are looking towards the doorway.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 106, "extra_info": {"duration": 3.75, "events": ["A character peeks out from beside a door.", "A character wearing green clothes turns to look.", "The first character moves to the center of the doorway.", "The first character stands uncertainly for a moment.", "The first character crouches down.", "The first character shifts to the other side.", "The first character quickly vanishes from the doorway.", "Two characters look towards the doorway."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 106}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little boy, frightened, stands upright and then starts running forward. On both sides, there are three little girls watching him. The camera switches to the hallway, where the boy rushes down the stairs with the three girls following closely behind. The little boy stops at the stairway entrance, glances left and right, and then throws a blue ball from the left side of the screen, followed by the appearance of two little birds.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 107, "extra_info": {"duration": 4.38, "events": ["Little boy stands upright.", "Little boy starts running forward.", "Three little girls watch the boy.", "Little boy rushes down the stairs.", "Three little girls follow the boy.", "Little boy stops at the stairway entrance.", "Little boy glances left and right.", "Little boy throws a blue ball.", "Two little birds appear."], "n_events": 9, "n_shots": 2, "n_subjects": 3}, "vid": 107}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman embraced the man and spun around once. A snowflake fluttered down from the sky, which the man caught with the tip of his index finger, and then both the man and woman kissed towards the snowflake.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 108, "extra_info": {"duration": 4.5, "events": ["The woman embraced the man.", "They spun around once.", "A snowflake fluttered down from the sky.", "The man caught the snowflake with his index finger.", "The man and woman kissed towards the snowflake."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 108}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman are arguing face to face, getting closer as they become more heated in their dispute, ultimately reaching the point where their noses are pressed against each other as they continue to push forward while arguing. Several children nearby watch them, frightened, until the two almost have their faces touching, at which point they both suddenly realize in surprise.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 109, "extra_info": {"duration": 4.88, "events": ["A man and a woman start arguing.", "The man and the woman get closer during the argument.", "The man and the woman's noses press against each other.", "The man and the woman continue to push forward while arguing.", "Several children nearby begin to watch the argument.", "The children appear frightened by the argument.", "The man and the woman's faces almost touch.", "The man and the woman suddenly realize in surprise."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 109}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of the fireplace, a person wearing a hat is showing a cellphone to a little girl sitting on the left. The little girl remains still, reading her book, paying no attention to the person with the hat. The person with the hat then turns to show the cellphone to two little girls on the right, but they too remain still, reading their books, ignoring him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 110, "extra_info": {"duration": 3.25, "events": ["Person shows cellphone to little girl.", "Little girl reads her book.", "Person turns to two little girls.", "Two little girls read their books.", "Person shows cellphone to two little girls."], "n_events": 5, "n_shots": 1, "n_subjects": 4}, "vid": 110}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man wearing a hat and three red-haired women looked through the window to the outside. The man said something, and then all four of them ducked down out of sight beneath the window.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 111, "extra_info": {"duration": 2.5, "events": ["A man wearing a hat looked through the window.", "Three red-haired women looked through the window.", "The man said something.", "All four individuals ducked down beneath the window."], "n_events": 4, "n_shots": 1, "n_subjects": 4}, "vid": 111}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the bus, a plump woman sitting in her seat looked towards the front seat where a little girl with red hair turned around and spoke to the plump woman. The plump woman sighed, and to their left, a man wearing a hat glanced at the packaged food in his hand and then reached out to give it to the red-haired girl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 112, "extra_info": {"duration": 9.75, "events": ["Plump woman looked towards front seat", "Little girl turned around", "Little girl spoke to plump woman", "Plump woman sighed", "Man glanced at packaged food", "Man reached out to little girl", "Man offered packaged food to little girl"], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 112}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of a fireplace ablaze with fire, a man and a woman—two adults—look toward an object on the table in front of them. The woman is knitting a sweater. From the object emerges a girl's head, and then a hand stretches out from each side to reach for the wine glasses on the table. The camera switches to the opposite side, where a little boy is looking at them in surprise.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 113, "extra_info": {"duration": 3.5, "events": ["Fireplace ablaze with fire", "Man and woman look toward object", "Woman knits a sweater", "Girl's head emerges from object", "Girl's hand reaches for wine glass", "Camera switches sides", "Little boy looks in surprise"], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 113}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the ground, there was a pile of tangled wires. Several tentacles dropped down from above and a fuse was burning. A red-haired girl ran over, lifted a baby from the tangled wires, and a small explosion occurred within the wires. The baby was laughing. The scene then cut to the doorway where the little girl, looking terrified, was holding the baby, and two other red-haired girls stood by the door watching them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 114, "extra_info": {"duration": 3.88, "events": ["Tentacles dropped from above.", "A fuse was burning.", "Red-haired girl ran over.", "Girl lifted baby from wires.", "Small explosion occurred within wires.", "Baby laughed.", "Girl held baby looking terrified.", "Two red-haired girls stood by door.", "Red-haired girls watched other girl and baby."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 114}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room filled with burning candles, three lit candles slowly ascend diagonally upwards. Suddenly, the candles on the wall are all blown out. The camera switches to four children, with a slightly older boy in the middle holding three candles. The candles on the distant wall are also blown out. Lastly, the three candles held by the boy are extinguished, and in the darkness, their eyes look upwards. Suddenly, a burst of red smoke explodes behind them, and then a tall monster appears within the smoke. They turn around to look at the monster.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 115, "extra_info": {"duration": 6.25, "events": ["Three candles ascend diagonally upwards.", "All candles on the wall are blown out.", "Four children stand with an older boy holding candles.", "Candles on distant wall are blown out.", "Three candles held by the older boy are extinguished.", "Children look upwards in the darkness.", "Red smoke bursts behind the children.", "A tall monster appears in the smoke.", "Children turn around to face the monster."], "n_events": 9, "n_shots": 2, "n_subjects": 3}, "vid": 115}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A blonde woman stands at the entrance of a house, holding a green \"For Sale\" sign in her hand. A few silhouettes pass by her. The woman walks forward a few steps with a smile, then glances at the house. Turning back, she sees that the silhouettes have moved on. Her expression changes to one of anger, and she puts down the sign and stamps her foot forcefully.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 116, "extra_info": {"duration": 3.62, "events": ["Blonde woman stands at house entrance.", "Woman holds green 'For Sale' sign.", "Silhouettes pass by blonde woman.", "Woman walks forward a few steps.", "Woman smiles.", "Woman glances at house.", "Woman turns back.", "Silhouettes have moved on.", "Woman's expression changes to anger.", "Woman puts down sign and stamps foot."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 116}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a round button with red and white stripes. A hand appears and presses the button several times. The camera then cuts to a red-haired girl standing in front of a house. As the door opens, a small-headed, large-bodied officer emerges from a door just big enough for him to pass through, with a small baby hanging on his right chest.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 117, "extra_info": {"duration": 6.5, "events": ["A hand presses the round button multiple times.", "The red-haired girl stands in front of a house.", "The door opens.", "A small-headed officer emerges from the door.", "A small baby hangs on the officer's right chest."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 117}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A fish and a mermaid were floating in the water. The fish swam upwards, only to find something covering the surface above. After swimming in a circle and coming back down, the fish spoke to the mermaid.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 118, "extra_info": {"duration": 5.62, "events": ["A fish floated in water.", "A mermaid floated in water.", "The fish swam upwards.", "The fish found something covering the surface.", "The fish swam in a circle.", "The fish returned downwards.", "The fish spoke to the mermaid."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 118}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A mermaid looks downward, then swims downward, the camera cuts to her catching up with a male mermaid, swimming forward by his side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 119, "extra_info": {"duration": 4.25, "events": ["A mermaid looks downward.", "A mermaid swims downward.", "A mermaid catches up with a male mermaid.", "A mermaid swims by the male mermaid's side."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 119}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A male merman extended his hand to a female merman in an inviting gesture, and the female merman placed her hand on the male merman’s. The male merman then led the female merman away, swimming upwards to the left, as a seahorse and a dolphin looked on. The two animals exchanged glances. Around them, many other merpeople were present. The scene then shifts to show the male and female merman starting to dance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 120, "extra_info": {"duration": 4.88, "events": ["A male merman extended his hand to a female merman", "The female merman placed her hand on the male merman's hand", "The male merman led the female merman upwards", "The male merman and the female merman swam to the left", "A seahorse and a dolphin looked on", "The seahorse and the dolphin exchanged glances", "Merpeople surrounded the two mermen", "The male merman and the female merman started to dance"], "n_events": 8, "n_shots": 2, "n_subjects": 4}, "vid": 120}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A purple-haired female ghost is speaking to the camera; she then flies upwards, followed by six other female ghosts. The camera angle shifts to show them flying past a row of buildings, heading towards the darkness in the distance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 121, "extra_info": {"duration": 8.62, "events": ["A purple-haired female ghost speaks to the camera.", "The purple-haired female ghost flies upwards.", "Six other female ghosts follow the purple-haired ghost.", "The group of female ghosts flies past a row of buildings.", "Female ghosts head towards the darkness."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 121}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera tilts upward from below, capturing a sphere of light above. A woman is floating and spinning around the light sphere. The shot switches to an even lower angle as several female characters peek out from beneath the floor one after another, looking up at the woman above. The camera cuts again, a beam of light shoots down from the light sphere, and the woman enters the beam. After a flash of bright light, the woman disappears, and the camera quickly pans up to film the light sphere.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 122, "extra_info": {"duration": 8.38, "events": ["Camera tilts upward capturing a sphere of light", "A woman floats and spins around the light sphere", "Several female characters peek out from beneath the floor", "Female characters look up at the woman above", "A beam of light shoots down from the light sphere", "The woman enters the beam of light", "The woman disappears after a flash of bright light", "Camera quickly pans up to film the light sphere"], "n_events": 8, "n_shots": 4, "n_subjects": 2}, "vid": 122}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A police officer, with arms crossed, speaks to two people in suits ahead of him. The two in suits turn around, and the person on the right takes a photo and tucks it into the breast pocket of the jacket, then both leave. Behind them, another officer is working at a computer.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 123, "extra_info": {"duration": 2.5, "events": ["A police officer speaks to two people in suits.", "The person on the right takes a photo.", "The person tucks the photo into the breast pocket.", "Both people in suits leave.", "Another officer works at a computer."], "n_events": 5, "n_shots": 1, "n_subjects": 4}, "vid": 123}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the store, a brown-haired man pushes a shopping cart forward, followed by a blond-haired man. The blond man takes a piece of clothing off a mannequin. Behind them, there are two women. The two men walk straight out of the exit, where a red light is flashing. As they leave the store, the brown-haired man pushes the shopping cart forward, and the cart rolls through a bed of flowers and onto the grass. There are also some people around the store.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 124, "extra_info": {"duration": 6.38, "events": ["Brown-haired man pushes shopping cart", "Blond-haired man removes clothing from mannequin", "Two men walk towards exit", "Red light flashes at exit", "Men leave the store", "Shopping cart rolls through flower bed", "Cart moves onto the grass", "People are present around the store"], "n_events": 8, "n_shots": 3, "n_subjects": 3}, "vid": 124}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people approach a food store, with the brown-haired man in front leading while holding a black object in his hand. They open the door and enter the store. The camera then cuts to the store door being kicked open by the brown-haired man, who comes out carrying a large stack of food; the blond-haired man follows him, carrying a slightly smaller amount of food. As they both exit, some of the food falls to the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 125, "extra_info": {"duration": 7.0, "events": ["Two people approach a food store.", "Brown-haired man leads holding a black object.", "They open the store door.", "Brown-haired man enters the store.", "Brown-haired man kicks the store door open.", "Brown-haired man exits carrying a large stack of food.", "Blond-haired man exits carrying food.", "Food falls to the ground."], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 125}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a spaceship, a female astronaut floats and activates the switch of a door in front of her. Once the door opens, the female astronaut's feet touch down onto the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 126, "extra_info": {"duration": 3.25, "events": ["A female astronaut floats in a spaceship.", "The female astronaut activates a switch.", "A door opens in front of the female astronaut.", "The female astronaut's feet touch down onto the ground."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 126}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald little boy walks out of a house with a red door, the scene cuts to a bus stop where a boy with thinning hair speaks to the approaching bald boy.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 127, "extra_info": {"duration": 6.12, "events": ["Bald little boy walks out of a house.", "Boy with thinning hair speaks to the bald boy."], "n_events": 2, "n_shots": 2, "n_subjects": 2}, "vid": 127}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald boy is sitting on a platform, with his right hand propping up his cheek and his left hand resting on his knee, talking to Snoopy in front of him. Snoopy walks forward two steps, hops onto the platform, sits down, and mimics the boy's pose with his right hand on his cheek and his left hand supporting his knee.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 128, "extra_info": {"duration": 4.75, "events": ["A bald boy is sitting on a platform.", "The bald boy props up his cheek with right hand.", "The bald boy rests his left hand on his knee.", "Snoopy walks forward two steps.", "Snoopy hops onto the platform.", "Snoopy sits down.", "Snoopy mimics the boy's pose.", "Snoopy props up his cheek with right hand.", "Snoopy supports his knee with left hand."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 128}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Snow falls from the sky as a little boy dressed in red looks ahead with a smile. He moves forward, and the camera angle switches to show the opposite view, where four children stand on each side, wearing ice skates and holding hands on the ice. The little boy in red skates into the midst of them and reaches out to hold the hand of a nearby child.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 129, "extra_info": {"duration": 5.38, "events": ["Snow falls from the sky.", "A little boy smiles.", "The little boy moves forward.", "Four children stand on the ice.", "The children hold hands.", "The little boy in red skates into the group.", "The little boy reaches out.", "The little boy holds the hand of another child."], "n_events": 8, "n_shots": 3, "n_subjects": 2}, "vid": 129}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera showcases a series of comic images in front. It moves down to the right, passing a simple sketch, then moves downwards and fixes on Snoopy lying on top of the red house. After a while, Snoopy opens his eyes and sits up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 130, "extra_info": {"duration": 9.38, "events": ["Camera showcases a series of comic images", "Camera moves down to the right", "Camera passes a simple sketch", "Camera moves downwards", "Camera fixes on Snoopy", "Snoopy lies on top of the red house", "Snoopy opens eyes", "Snoopy sits up"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 130}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald little boy speaks to the girl next to him, then turns and walks away. The little girl, smiling, takes two steps forward, brings her hands to her chin, and speaks to the blond-haired boy, who turns his head to look in the direction where the bald little boy left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 131, "extra_info": {"duration": 4.25, "events": ["Bald little boy speaks to girl.", "Bald little boy turns away.", "Bald little boy walks away.", "Girl takes two steps forward.", "Girl brings hands to chin.", "Girl speaks to blond-haired boy.", "Blond-haired boy turns head."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 131}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Snoopy opened his mouth wide, jumped down to the ground from the top of the red house, and then danced around with joy. The bald little boy came from the left carrying a bowl of food and placed it on the ground. Snoopy took a look and then started eating.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 132, "extra_info": {"duration": 6.5, "events": ["Snoopy opened his mouth wide.", "Snoopy jumped down from the top of the red house.", "Snoopy danced around with joy.", "The bald little boy carried a bowl of food.", "The bald little boy placed the bowl on the ground.", "Snoopy took a look at the bowl.", "Snoopy started eating from the bowl."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 132}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald little boy sits on a platform, his right hand propping his cheek and his left hand resting on his knee. Snoopy, holding a food bowl, looks up at the sky, then throws the bowl away and runs off to the right. The camera cuts to a red house where Snoopy runs up, jumps onto the roof, bares his teeth for a moment, then sits down and puts on a flying cap, a red scarf, and goggles. Next, he extends his hands forward as if driving. Two plumes of black smoke emerge from behind the red house, which then moves forward and flies up out of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 133, "extra_info": {"duration": 9.5, "events": ["A bald little boy props his cheek with his right hand.", "A bald little boy rests his left hand on his knee.", "Snoopy looks up at the sky.", "Snoopy throws the food bowl away.", "Snoopy runs off to the right.", "Snoopy jumps onto the red house roof.", "Snoopy bares his teeth.", "Snoopy sits down on the roof.", "Snoopy puts on a flying cap, a red scarf, and goggles.", "The red house moves forward and flies up out of the frame."], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 133}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The bald little boy sat on the mat while the little girl stood with her hands behind her back, talking to the boy. The little boy walked away to the left, then the screen faded to dark. The little boy came back and continued to sit on the mat. The little girl smiled, turned around, and turned on the projector in front of her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 134, "extra_info": {"duration": 8.38, "events": ["The bald little boy sat on the mat.", "The little girl talked to the bald little boy.", "The bald little boy walked away to the left.", "The screen faded to dark.", "The bald little boy came back.", "The bald little boy continued to sit on the mat.", "The little girl smiled.", "The little girl turned around.", "The little girl turned on the projector."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 134}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The little boy held a ball in his right hand, tapped the glove on his left hand, and then threw it forward. The ball quickly bounced back, knocking the child over, and the child's hat, gloves, shoes, socks, and clothes all flew off.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 135, "extra_info": {"duration": 4.0, "events": ["The little boy held a ball.", "The little boy tapped the glove.", "The little boy threw the ball forward.", "The ball bounced back.", "The ball knocked the child over.", "The child's hat flew off.", "The child's gloves flew off.", "The child's shoes flew off.", "The child's socks flew off.", "The child's clothes flew off."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 135}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Against the blue drawing background, the names of the production staff switch on the screen multiple times.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 136, "extra_info": {"duration": 5.25, "events": ["Production staff names switch on the screen."], "n_events": 1, "n_shots": 1, "n_subjects": 1}, "vid": 136}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four children are playing baseball. A boy in a yellow shirt and a girl with brown hair are standing on a mound of dirt. On the right, a black-skinned boy points at the boy in the yellow shirt while speaking. The boy in the yellow shirt initially looks at the black-skinned boy, then turns his head towards the direction of the camera, revealing a puzzled expression. The girl first looks towards the direction of the camera, and then turns her gaze towards the boy in the yellow shirt. The frame freezes and the camera pans slightly to the right, revealing the edge of the picture on the right side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 137, "extra_info": {"duration": 4.38, "events": ["Four children play baseball.", "A boy in a yellow shirt stands on a mound of dirt.", "A girl with brown hair stands on a mound of dirt.", "A black-skinned boy points at the boy in the yellow shirt.", "The boy in the yellow shirt turns his head towards the camera.", "The boy in the yellow shirt reveals a puzzled expression.", "The girl with brown hair looks towards the direction of the camera.", "The girl with brown hair turns her gaze towards the boy in the yellow shirt.", "The camera frame freezes.", "The camera pans slightly to the right."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 137}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera sequentially switches between three black and white drawings. The first drawing shows a little boy walking in front of a new village, the second depicts a letter being thrown out of a mailbox slot, with the boy turning his head back to look, and the third shows the little boy standing under a tree, holding a heart-shaped object with both hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 138, "extra_info": {"duration": 8.12, "events": ["A little boy walks in front of a village", "A letter is thrown out of a mailbox slot", "The little boy turns his head back to look", "The little boy stands under a tree", "The little boy holds a heart-shaped object"], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 138}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans from a painting of Snoopy standing on a red house to the right, moving to an image of a little boy writing on paper. The boy is biting the end of his pen, with one hand propping up his cheek. Then the image transforms into a live shot, the child's supporting hand and index finger shift slightly, a gust of wind blows, and the papers on the boy's study desk are swept away. The little boy looks on in confusion, and then turns his head to look to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 139, "extra_info": {"duration": 6.62, "events": ["The camera pans from a painting of Snoopy to an image of a boy.", "The boy bites the end of his pen.", "The boy props up his cheek with one hand.", "The image transforms into a live shot.", "The boy's supporting hand and index finger shift slightly.", "A gust of wind blows.", "Papers on the boy's desk are swept away.", "The boy looks on in confusion.", "The boy turns his head to the left."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 139}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald little boy looks to the right, then turns his head towards the camera and smiles. The camera cuts to the back of the bald boy, who is sitting on the grass next to a boy in orange clothing, both watching the stars in front of them. The bald boy stands up and faces the boy in orange, who turns his head to look at the bald boy.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 140, "extra_info": {"duration": 5.12, "events": ["The bald little boy looks to the right", "The bald little boy turns his head towards the camera", "The bald little boy smiles", "The bald little boy sits on the grass", "The bald little boy watches the stars", "The bald little boy stands up", "The boy in orange turns his head to look at the bald boy"], "n_events": 7, "n_shots": 2, "n_subjects": 2}, "vid": 140}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald boy yelled something to the right, then turned and ran to the left. A boy with messy hair was playing in the mud. The bald boy ran past the mud-playing boy, who then turned to look in the direction the bald boy had run. The camera moved to the left, revealing the bald boy hiding behind a tree, with the tree blocking the middle of him, leaving the sides of his face and body exposed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 141, "extra_info": {"duration": 4.62, "events": ["A bald boy yelled to the right.", "The bald boy turned and ran to the left.", "A boy with messy hair played in the mud.", "The bald boy ran past the mud-playing boy.", "The boy with messy hair turned to look.", "The bald boy hid behind a tree."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 141}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people are swaying left and right with their hands raised high, while only a little boy stands, looking around at the others in confusion. The camera zooms in on the boy's upper body; he takes a bite of the biscuit in his hand, and then excitedly raises his arms and starts to sway left and right too. The camera then switches to show the boy and many people next to him swaying together. Once again, the camera angle changes to capture the back of a woman swaying, as well as the sea of people in front of her moving in unison.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 142, "extra_info": {"duration": 9.62, "events": ["A group of people sway left and right.", "A little boy stands looking around in confusion.", "The little boy takes a bite of a biscuit.", "The little boy raises his arms.", "The little boy starts to sway.", "People next to the boy sway together.", "A woman sways to the rhythm.", "A sea of people moves in unison."], "n_events": 8, "n_shots": 4, "n_subjects": 3}, "vid": 142}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man in a red outfit is posed with one hand behind him and the other over his chest, leans forward, and then straightens his body with one hand behind and the other placed in front of his stomach. A blonde woman comes over and supports the man's arm. A short woman walks past the camera toward the man in the red outfit and the blonde woman, and both the man in red and the blonde woman look towards the short woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 143, "extra_info": {"duration": 4.12, "events": ["Man in red outfit leans forward.", "Man in red outfit straightens his body.", "Blonde woman supports man's arm.", "Short woman walks towards the man and blonde woman.", "Man in red and blonde woman look towards short woman."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 143}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A short woman hands a piece of paper to a man in red clothes, who leans forward to take it and looks at it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 144, "extra_info": {"duration": 4.25, "events": ["A short woman hands a piece of paper.", "Man in red clothes leans forward.", "Man in red clothes takes the paper.", "Man in red clothes looks at the paper."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 144}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the camera's view, there is a towering tree. Someone falls from above and hits a branch, which sends them bouncing off in a different direction, then strikes another branch. The branch once again deflects the falling person. As the shot changes to show the person being launched by a branch, it becomes apparent there are two people, each being propelled in different directions. A girl in blue clothing hits a branch while lying prone and continues to fall. A blonde boy, head first, hits another branch and keeps tumbling down. The girl's head strikes yet another branch, and with her eyes wide open, she slides down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 145, "extra_info": {"duration": 7.75, "events": ["Someone falls from the tree.", "The person hits a branch.", "The branch sends the person in a different direction.", "The person strikes another branch.", "The shot reveals two people.", "A girl in blue hits a branch while prone.", "The girl continues to fall.", "A blonde boy hits a branch head first.", "The boy keeps tumbling down.", "The girl's head strikes another branch."], "n_events": 10, "n_shots": 5, "n_subjects": 2}, "vid": 145}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At night, a treehouse dweller stands opposite a girl and a boy, makes a move towards the two children, the camera then cuts to behind the treehouse where the treehouse dweller picks up a bucket. The camera switches to the side of the treehouse, as the treehouse dweller turns slightly to the left. The angle changes to a top-down shot from the top of the treehouse, where the treehouse dweller makes a slight movement. Then the camera cuts to the two children; the little girl remains composed, while the little boy and a dog behind him show expressions of surprise.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 146, "extra_info": {"duration": 8.62, "events": ["Treehouse dweller stands opposite a girl and a boy", "Treehouse dweller makes a move towards the children", "Treehouse dweller picks up a bucket", "Treehouse dweller turns slightly to the left", "Treehouse dweller makes a slight movement", "Little girl remains composed", "Little boy shows expressions of surprise", "Dog shows expressions of surprise"], "n_events": 8, "n_shots": 5, "n_subjects": 4}, "vid": 146}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A phoenix is soaring in the sky. A woman in blue clothes watches as the phoenix flies by, shedding red specks of light. The camera follows the woman's perspective as it rotates. The woman turns her head to look at a phoenix feather falling from the sky. She reaches out her hand to catch it, and a female sprite emerges from the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 147, "extra_info": {"duration": 7.62, "events": ["A phoenix soars in the sky.", "The phoenix sheds red specks of light.", "The woman watches the phoenix.", "The woman turns her head.", "A phoenix feather falls from the sky.", "The woman reaches out her hand.", "The woman catches the phoenix feather.", "A female sprite emerges from the water."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 147}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little girl and a dog look forward in surprise from behind a wall. The camera switches to the opposite side, where a woman in white clothes takes a crown from a yellow-clad servant. She looks at the crown greedily and then raises it to put it on herself.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 148, "extra_info": {"duration": 5.0, "events": ["A little girl and a dog look in surprise.", "A woman takes a crown from a servant.", "The woman looks at the crown greedily.", "The woman raises the crown.", "The woman puts the crown on herself."], "n_events": 5, "n_shots": 2, "n_subjects": 4}, "vid": 148}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the room, an old woman turns around to face the camera and scatters a row of gems forward. On the right side, a blonde woman, holding her shoulders, looks on. The camera zooms out to reveal the trail of gems extending to the feet of a man in red. The shot switches to the feet of the old woman, who steps on the first gem. The gem emits a glow, and then the gems ahead light up one by one. The radiance forms a beam of light that gradually extends towards the man in red.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 149, "extra_info": {"duration": 5.12, "events": ["Old woman turns around to face the camera.", "Old woman scatters a row of gems forward.", "Blonde woman looks on, holding her shoulders.", "Camera reveals trail of gems extending to man's feet.", "Old woman steps on the first gem.", "First gem emits a glow.", "Gems light up one by one.", "Beam of light forms.", "Radiance extends towards the man in red."], "n_events": 9, "n_shots": 2, "n_subjects": 3}, "vid": 149}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man dressed in red sits on the edge of the stage holding a treasure chest. He opens the chest, looks at the gold coins inside, then grabs a handful and throws them back in, bursting into laughter.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 150, "extra_info": {"duration": 5.0, "events": ["A man sits on the stage edge.", "The man opens the treasure chest.", "The man looks at the gold coins.", "The man grabs a handful of coins.", "The man throws the coins into the chest.", "The man bursts into laughter."], "n_events": 6, "n_shots": 2, "n_subjects": 1}, "vid": 150}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A boy with a head ornament clenched his fists, said something, and then stood up. The camera cuts to a room where a pumpkin-headed figure stood up. He swayed from side to side, and explosions occurred beneath him. The camera then switches to a frontal shot of an old witch; explosions happen behind her, emitting purple smoke, as the pumpkin head is set ablaze, and the old woman looks back in horror.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 151, "extra_info": {"duration": 5.0, "events": ["A boy clenched his fists.", "A boy said something.", "A boy stood up.", "A pumpkin-headed figure stood up.", "The pumpkin-headed figure swayed from side to side.", "Explosions occurred beneath the pumpkin-headed figure.", "An old witch was shown from the front.", "Explosions emitted purple smoke behind the old witch.", "The pumpkin head was set ablaze.", "The old witch looked back in horror."], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 151}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera tilts down for an overhead shot, where an old man looks upward, pushed in the chest with a stick. He falls downward, and as the camera angle shifts to a low angle shot, a woman is seen holding the stick at the mouth of a well. The old man falls down into the well, and the camera then switches to the well's opening, where the woman gives a smile. The scene cuts to the ground level, where the woman takes a deep red hat and replaces the black hat on the head of a short man standing before her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 152, "extra_info": {"duration": 6.5, "events": ["Camera tilts down for an overhead shot.", "Old man looks upward.", "Woman pushes old man in chest with stick.", "Old man falls into the well.", "Camera angle shifts to a low angle shot.", "Woman holds the stick at the well's mouth.", "Woman smiles at the well's opening.", "The camera switches to ground level.", "Woman takes a deep red hat.", "Woman replaces black hat on short man's head."], "n_events": 10, "n_shots": 4, "n_subjects": 3}, "vid": 152}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman in white clothes is embracing a man in green clothes, as a priest holding a book says something to the woman in white. A server watches them, and the woman in white throws the man in green forward. The camera cuts to the front, where the man in green is thrown down, a stool overturned on the ground, while a little girl and a small dog from the other side of the wall are watching what is happening.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 153, "extra_info": {"duration": 2.5, "events": ["A woman in white clothes embraces a man in green clothes.", "A priest holding a book says something to the woman in white.", "A server watches the woman and man.", "The woman in white throws the man in green forward.", "The man in green is thrown down.", "A stool overturns on the ground.", "A little girl watches the event.", "A small dog watches the event."], "n_events": 8, "n_shots": 2, "n_subjects": 5}, "vid": 153}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bald man fires his gun ahead, the surrounding scenery quickly recedes, the camera angle switches to the front, a car sways from side to side, bullets strike the area around the car, the camera angle switches to the front of the car, inside there are two people, a brown-haired man is driving the car, and looks outside along with the other person in the car.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 154, "extra_info": {"duration": 6.12, "events": ["A bald man fires his gun ahead.", "The surrounding scenery quickly recedes.", "A car sways from side to side.", "Bullets strike the area around the car.", "A brown-haired man is driving the car.", "The brown-haired man looks outside.", "The other person in the car looks outside."], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 154}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At night, a green car smashed through the wooden fence ahead, then rolled forward and crashed into a house. Many people inside the house, holding guns, moved from other rooms to the one that was hit by the car and aimed their guns at the vehicle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 155, "extra_info": {"duration": 5.62, "events": ["Green car smashed through the wooden fence.", "Car rolled forward after breaking the fence.", "Vehicle crashed into a house.", "People inside the house moved to the impacted room.", "Occupants aimed their guns at the vehicle."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 155}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows many people dining in a restaurant. The camera cuts to a table where a brown-haired man takes a hamburger passed to him by a server and then moves forward to pick up a plate of vegetables. Behind him, a blond-haired man turns around and frantically starts eating fries from the plate of a woman behind him. Then he snatches the plate out of the woman's hands, and she looks at him in shock.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 156, "extra_info": {"duration": 5.75, "events": ["People dine in a restaurant.", "A brown-haired man receives a hamburger from a server.", "The brown-haired man picks up a plate of vegetables.", "A blond-haired man turns around.", "The blond-haired man frantically eats fries.", "The blond-haired man snatches a plate from a woman.", "The woman looks at the blond-haired man in shock."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 156}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people are holding guns, pointing towards the broken vehicles in front, speaking to the front, subtitles appear at the bottom. A brown-haired man and a blond man fall out of the car, with the brown-haired man crawling forward a couple of times.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 157, "extra_info": {"duration": 7.5, "events": ["People are holding guns.", "People are pointing guns towards vehicles.", "Subtitles appear at the bottom.", "The brown-haired man falls out of the car.", "The blond man falls out of the car.", "The brown-haired man crawls forward."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 157}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At night, a mouse was scurrying across a road when a bus with its headlights on approached from the distance. The headlights shone on the mouse, which ran across the road in panic. As the bus passed by the camera, the shot switched to the inside of the bus. Inside, the passengers were sleeping. The camera panned from left to right inside the bus, and finally focused on two front-row seats on the right, where a brown-haired man was sleeping and a blond-haired man was awake, looking out of the window. Then he turned his head forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 158, "extra_info": {"duration": 11.0, "events": ["A mouse scurried across a road at night.", "A bus approached with headlights on.", "Headlights shone on the mouse.", "The mouse ran across the road in panic.", "The bus passed by the camera.", "Passengers inside the bus were sleeping.", "Camera panned inside the bus.", "A brown-haired man was sleeping in a front-row seat.", "A blond-haired man looked out of the window.", "The blond-haired man turned his head forward."], "n_events": 10, "n_shots": 2, "n_subjects": 4}, "vid": 158}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures several individuals wearing prison uniforms sitting inside a cell. A bald, burly man is propping himself up with his right hand on a stool. He then stands up, hands in his pockets. The blond man to his left glances at the blue bag on the stool, exchanges a look with a brown-haired man further to the left, and then turns his head to pick up the blue bag from the stool.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 159, "extra_info": {"duration": 3.88, "events": ["Several individuals sit inside a cell.", "A bald man propps himself up with his right hand.", "The bald man stands up.", "The bald man places hands in pockets.", "A blond man glances at the blue bag.", "The blond man exchanges a look with the brown-haired man.", "The blond man turns his head.", "The blond man picks up the blue bag."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 159}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a dirty and messy room, a person is holding a device and scanning the leftover food in a bowl. The camera then switches to another direction, where a bald man picks up a white towel from the sofa. Behind him, an agent is taking notes with a pen. The bald man turns the towel to face the camera, revealing a face pattern on the towel.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 160, "extra_info": {"duration": 5.12, "events": ["A person scans leftover food in a bowl.", "A bald man picks up a white towel from the sofa.", "An agent takes notes with a pen.", "The bald man turns the towel to face the camera.", "A face pattern is revealed on the towel."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 160}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A female astronaut grabs a control stick in front of her. The camera switches to the exterior of the spacecraft, where a robotic arm extends. After swaying side to side for a few times, the arm lifts high and flings the object at the end of the robotic arm into space.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 161, "extra_info": {"duration": 9.88, "events": ["A female astronaut grabs a control stick.", "A robotic arm extends from the spacecraft.", "The robotic arm sways side to side.", "The robotic arm lifts high.", "The robotic arm flings the object into space."], "n_events": 5, "n_shots": 2, "n_subjects": 1}, "vid": 161}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Rain is falling from the sky when suddenly a red car starts up and dashes forward. The camera switches to a blond man walking on the sidewalk. A black car pulls up beside the blond man, the door opens, and a person in black appears. They grab the man and pull him towards the car. The camera then switches to a view from inside the car looking out at the door as the blond man struggles, his hands gripping the edge of the door. The person in black grabs the blond man's shoulders. The blond man looks left and right before finally being pulled into the car by the person in black.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 162, "extra_info": {"duration": 4.88, "events": ["Rain falls from the sky.", "Red car starts up.", "Red car dashes forward.", "Blond man walks on sidewalk.", "Black car pulls up beside blond man.", "Car door opens.", "Person in black appears.", "Person in black grabs blond man.", "Blond man struggles at car door.", "Person in black pulls blond man into car."], "n_events": 10, "n_shots": 3, "n_subjects": 2}, "vid": 162}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two odd-looking individuals fell to the ground and struggled to sit up. A car coming from the left side hit them again, knocking them down. The person on the right side managed to sit up once more.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 163, "extra_info": {"duration": 7.88, "events": ["Two individuals fell to the ground.", "The individuals struggled to sit up.", "A car hit the individuals again.", "The car was coming from the left side.", "One individual managed to sit up once more."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 163}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A blond-haired man runs towards the makeshift toilet ahead, while the brown-haired man watches him. After the blond-haired man enters the toilet and closes the door, the camera cuts to the inside of the toilet. The blond-haired man sits on the toilet and picks up his cellphone to make a call.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 164, "extra_info": {"duration": 5.0, "events": ["Blond-haired man runs towards the makeshift toilet.", "Blond-haired man enters the makeshift toilet.", "Blond-haired man closes the toilet door.", "Blond-haired man sits on the toilet.", "Blond-haired man picks up his cellphone.", "Blond-haired man makes a call."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 164}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "During the night, in a town, a car quickly moved forward after turning at an intersection. A second car was reversing and followed the first car's direction after turning at the same intersection, swaying from side to side as it traveled on the road. The reversing car first knocked over items on the right side of the road and then crashed into vehicles parked on the left. The camera then cuts to the road where the car in front sped past with silhouettes visible inside, followed by the reversing car, swaying as it chased after the leading vehicle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 165, "extra_info": {"duration": 5.38, "events": ["A car turned at an intersection.", "The car quickly moved forward.", "A second car reversed direction.", "The second car followed the first car.", "The second car swayed while traveling.", "The second car knocked over roadside items.", "The second car crashed into parked vehicles.", "The leading car sped past."], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 165}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A koala gestures in various ways and speaks towards the camera, when suddenly some petals block its mouth. It falls to the ground, eyes closed. A yellow scorpion looks at the koala, takes a step back, blinks, and glances to the upper right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 166, "extra_info": {"duration": 6.25, "events": ["A koala gestures towards the camera.", "A koala speaks.", "Petals block the koala's mouth.", "The koala falls to the ground.", "The koala's eyes close.", "A yellow scorpion looks at the koala.", "The yellow scorpion takes a step back.", "The yellow scorpion blinks.", "The yellow scorpion glances to the upper right."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 166}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hand slaps a green button and the camera cuts to a construction elevator. A koala falls onto the elevator, which is also carrying other creatures. Then the elevator rapidly descends. The camera angle switches to a top-down view from the roof, showing the elevator descending. The camera switches again to a side view of the elevator to continue displaying the rapid descent. Once more, the camera switches, showing the koala holding a small snake, with the snake and other animals widening their eyes in fear. The camera then cuts to a purple spider, whose six legs are suspended in air. Finally, the camera shifts to focus on the face of a grey lizard.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 167, "extra_info": {"duration": 5.75, "events": ["A hand slaps a green button", "A koala falls onto the elevator", "The elevator rapidly descends", "The koala holds a small snake", "The snake and other animals widen their eyes", "A purple spider's six legs are suspended in air", "The camera focuses on the face of a grey lizard"], "n_events": 7, "n_shots": 7, "n_subjects": 8}, "vid": 167}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures a flagpole, with a national flag on the left descending. On the right side, a snake clamps onto a line attached to the flagpole with its mouth, rapidly ascending. Lined up on the snake’s back from head to tail are a lizard, a scorpion, and a spider. The snake’s body swings, causing the scorpion and spider below to sway as well. As they reach the top of the flagpole, the four animals fly into the air following their momentum. The camera then switches to an aerial view, showing the four animals spreading all their limbs in mid-air. The camera angle changes again to a ground-level view. The snake, lizard, and scorpion land first, followed by the spider, which lands last, covering the other three animals with its legs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 168, "extra_info": {"duration": 6.5, "events": ["National flag descends on the left.", "Snake clamps onto line with mouth.", "Snake ascends flagpole rapidly.", "Lizard lines up on snake's back.", "Scorpion lines up on snake's back.", "Spider lines up on snake's back.", "Snake's body swings with animals swaying.", "Animals fly into the air from flagpole.", "Snake, lizard, and scorpion land first.", "Spider lands, covering other animals with legs."], "n_events": 10, "n_shots": 4, "n_subjects": 4}, "vid": 168}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of a fish tank, a snake slowly lifts its head, looking left and right. It gazes at the picture of mountains illuminated by the light ahead, then buries its head back into its body.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 169, "extra_info": {"duration": 5.88, "events": ["Snake lifts its head.", "Snake looks left.", "Snake looks right.", "Snake gazes at the picture.", "Light illuminates the mountains.", "Snake buries its head into its body."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 169}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A sightseeing boat full of tourists sailed from the left to the right side of the river. Afterwards, several animals successively popped their heads out of the water and then they swam towards the left side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 170, "extra_info": {"duration": 6.5, "events": ["A sightseeing boat sailed from left to right side of river.", "Several animals popped their heads out of the water.", "Animals swam towards the left side."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 170}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man adjusted his crooked beard with his hand, and the scene switched to him parking the vehicle on the edge of a cliff. The camera zoomed out to show a panoramic view of the vehicle at the cliff's edge, then cut to the man turning around to look back. It then switched to the man climbing out of the window and reaching out to grab the ladder on the roof of the vehicle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 171, "extra_info": {"duration": 7.25, "events": ["The man adjusted his crooked beard", "The man parked the vehicle on the cliff's edge", "The man turned around to look back", "The man climbed out of the vehicle's window", "The man reached out to grab the ladder on the roof"], "n_events": 5, "n_shots": 4, "n_subjects": 1}, "vid": 171}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The creature blinked and then let out a shout. The camera zoomed in, showing the creature's tongue wiggling. The shot then switched to glass shattering, with the two creatures beside it standing up to look ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 172, "extra_info": {"duration": 4.25, "events": ["The creature blinked.", "The creature let out a shout.", "The creature's tongue wiggled.", "Glass shattered.", "Two creatures stood up.", "Two creatures looked ahead."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 172}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man grasps with both hands, the camera cuts to the man getting into the car, then cuts to him sitting in the passenger seat, the car moves forward, it cuts again to the man picking up some items to look at them, and then he turns around holding the items, while the boy lifts his head to look at the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 173, "extra_info": {"duration": 7.25, "events": ["The man grasps with both hands.", "The man gets into the car.", "The man sits in the passenger seat.", "The car moves forward.", "The man picks up some items.", "The man looks at the items.", "The man turns around holding the items.", "The boy lifts his head."], "n_events": 8, "n_shots": 4, "n_subjects": 2}, "vid": 173}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl bent down to open the small door at the rear of the bus, the camera cut to a close-up of the door being opened, and four reptilian creatures walked forward. The girl watched them, as three of the creatures leaped forward, and the snake looked back in the girl's direction.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 174, "extra_info": {"duration": 4.38, "events": ["The girl bent down to open the small door.", "Four reptilian creatures walked forward.", "Three of the creatures leaped forward.", "The snake looked back in the girl's direction."], "n_events": 4, "n_shots": 4, "n_subjects": 3}, "vid": 174}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The creature pushed off the lid of the barrel, shook the banana peel off its head, looked at another creature crawling out of the box, and then looked to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 175, "extra_info": {"duration": 5.62, "events": ["The creature pushed off the lid of the barrel", "The creature shook the banana peel off its head", "The creature looked at another creature", "Another creature crawled out of the box", "The creature looked to the right"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 175}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The snake moves forward; the camera switches to three creatures sitting in a vehicle driving off into the distance, then switches back to the snake quickly advancing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 176, "extra_info": {"duration": 5.38, "events": ["The snake moves forward.", "Three creatures sit in a vehicle.", "The vehicle drives off into the distance.", "The snake advances quickly."], "n_events": 4, "n_shots": 3, "n_subjects": 4}, "vid": 176}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl opens her mouth wide, turns around to look ahead, the camera cuts to the girl climbing up, cuts to the girl stepping on the boy, cuts to the girl lying down, cuts to the boy looking down at the approaching creature, and then he squats down, picks up the creature, and jumps to toss it upwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 177, "extra_info": {"duration": 6.0, "events": ["The girl opens her mouth wide.", "The girl turns around to look ahead.", "The girl climbs up.", "The girl steps on the boy.", "The girl lies down.", "The boy looks down at the approaching creature.", "The boy squats down.", "The boy picks up the creature.", "The boy jumps.", "The boy tosses the creature upwards."], "n_events": 10, "n_shots": 4, "n_subjects": 3}, "vid": 177}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person reaches out to take a cookie; the camera cuts to this person eating the cookie. It then cuts to a group of people taking cookies one by one, before cutting to a woman looking around, with a crowd swaying behind her. The scene switches to a girl jumping up in cheer, a boy turns his head to look at her, and the girl lifts the boy into an embrace.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 178, "extra_info": {"duration": 7.75, "events": ["One person takes a cookie.", "Person eats the cookie.", "Group of people take cookies.", "Woman looks around.", "Crowd sways.", "Girl jumps up in cheer.", "Boy turns his head.", "Girl embraces the boy."], "n_events": 8, "n_shots": 5, "n_subjects": 4}, "vid": 178}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man moves forward quickly, then turns a corner. The camera switches to show him rapidly circling around two people. It then cuts to a woman reaching up to touch her temples and slightly shaking her head, followed by a scene where the woman extends a leg and trips the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 179, "extra_info": {"duration": 5.5, "events": ["The man moves forward quickly.", "The man turns a corner.", "The man rapidly circles around two people.", "The woman reaches up to touch her temples.", "The woman slightly shakes her head.", "The woman extends a leg.", "The woman trips the man."], "n_events": 7, "n_shots": 4, "n_subjects": 3}, "vid": 179}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl turns her head to look at the yellow creature behind her. The camera pans left to reveal a small dog biting the creature. The shot switches to the dog biting down hard, then to the creature shaking its body in pain, and finally to the girl watching the camera, her brow slightly furrowed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 180, "extra_info": {"duration": 7.5, "events": ["The girl turns her head.", "The girl looks at the yellow creature.", "A small dog bites the yellow creature.", "The dog bites down hard.", "The yellow creature shakes its body.", "The yellow creature expresses pain.", "The girl watches the camera.", "The girl furrows her brow."], "n_events": 8, "n_shots": 4, "n_subjects": 3}, "vid": 180}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The octopus grabbed the boy and girl with its tentacles, and one tentacle took out the girl's bag. The camera then cuts to the octopus emptying the contents of the bag, before retracting its tentacle, looking at the bag in surprise, and continuing to pour things out, switching to the boy looking up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 181, "extra_info": {"duration": 9.25, "events": ["The octopus grabbed the boy with its tentacles.", "The octopus grabbed the girl with its tentacles.", "A tentacle took out the girl's bag.", "The octopus emptied the contents of the girl's bag.", "The octopus's tentacle retracted.", "The octopus looked at the girl's bag in surprise.", "The octopus continued to pour things out of the girl's bag.", "The boy looked up."], "n_events": 8, "n_shots": 3, "n_subjects": 3}, "vid": 181}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girl approached the stone wall and then disappeared; the boy leant back, the camera switched to show the boy getting up and running forward, then switched again to show the boy crashing into the stone wall and falling down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 182, "extra_info": {"duration": 5.12, "events": ["The girl approached the stone wall.", "The girl disappeared.", "The boy leant back.", "The boy got up.", "The boy ran forward.", "The boy crashed into the stone wall.", "The boy fell down."], "n_events": 7, "n_shots": 3, "n_subjects": 2}, "vid": 182}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The soldier takes a deep breath and runs forward. The camera switches to a woman assuming a combat stance. It then cuts to the soldier flying past the woman, who opens her mouth in surprise. The camera switches and zooms in, focusing on the woman's eyes widening and her brows slightly furrowing. It then switches to an object flying in and sticking into a wooden stake.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 183, "extra_info": {"duration": 7.12, "events": ["The soldier takes a deep breath.", "The soldier runs forward.", "The woman assumes a combat stance.", "The soldier flies past the woman.", "The woman opens her mouth in surprise.", "The camera focuses on the woman's eyes.", "The woman's brows furrow slightly.", "An object flies in.", "The object sticks into a wooden stake."], "n_events": 9, "n_shots": 5, "n_subjects": 2}, "vid": 183}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The octopus carries two children forward, as the camera switches to three mermaids holding tridents.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 184, "extra_info": {"duration": 5.38, "events": ["The octopus carries two children forward.", "Three mermaids hold tridents."], "n_events": 2, "n_shots": 2, "n_subjects": 3}, "vid": 184}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The orange creature shuffled the basket, the camera cut to the contents spilling out of the plate, the creature fell forward and lay on the ground. The pink creature ran over holding the basket, then pointed at the yellow creature and laughed loudly before falling backward, losing balance and falling forward onto the ground. The orange creature turned around, pointed at the pink creature and laughed heartily. The pink creature sat up, picked up an object and threw it at the orange creature, then pointed at it and laughed out loud. The orange creature also picked up an item and threw it at the pink creature.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 185, "extra_info": {"duration": 11.0, "events": ["The orange creature shuffled the basket.", "Contents spilled out of the plate.", "The orange creature fell forward onto the ground.", "The pink creature ran over holding the basket.", "The pink creature pointed at the yellow creature and laughed.", "The pink creature lost balance and fell onto the ground.", "The orange creature turned and laughed at the pink creature.", "The pink creature sat up.", "The pink creature threw an object at the orange creature.", "The orange creature threw an item at the pink creature."], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 185}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person reaches out and opens the bottle cap, the camera cuts to a girl pouring the contents out of the bottle, it then cuts to the liquid from the bottle diffusing and rising up around the body of a dragon statue, with the camera following the movement to capture the scene.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 186, "extra_info": {"duration": 7.12, "events": ["A person reaches out and opens the bottle cap.", "A girl pours the contents out of the bottle.", "The liquid diffuses around the dragon statue.", "The liquid rises up around the dragon statue."], "n_events": 4, "n_shots": 3, "n_subjects": 1}, "vid": 186}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob is running in circles, while the jellyfish sway their bodies up and down. The camera zooms in and switches to the scene of Squidward taking a bath, and then it switches again to Squidward talking while scrubbing himself.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 187, "extra_info": {"duration": 17.38, "events": ["SpongeBob runs in circles", "Jellyfish sway bodies up and down", "Squidward takes a bath", "Squidward talks", "Squidward scrubs himself"], "n_events": 5, "n_shots": 3, "n_subjects": 3}, "vid": 187}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob reached out and said something, ran forward, the camera switched to SpongeBob opening the door and saying something, then he closed the door, took a step forward, and Patrick Star widened his eyes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 188, "extra_info": {"duration": 4.62, "events": ["SpongeBob reached out", "SpongeBob said something", "SpongeBob ran forward", "SpongeBob opened the door", "SpongeBob said something", "SpongeBob closed the door", "SpongeBob took a step forward", "Patrick Star widened his eyes"], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 188}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob says something to Patrick, leaps forward, and runs away. Patrick turns his head to see a skeleton body thrown to the right. The camera then cuts to SpongeBob running forward, switches to Patrick sitting and blinking, propping himself up with one hand while leaning back, and looking down to his left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 189, "extra_info": {"duration": 8.0, "events": ["SpongeBob speaks to Patrick.", "SpongeBob leaps forward.", "SpongeBob runs away.", "Patrick turns his head.", "Skeleton body is thrown to the right.", "Patrick sits and blinks.", "Patrick props himself up with one hand.", "Patrick leans back.", "Patrick looks down to his left."], "n_events": 9, "n_shots": 3, "n_subjects": 2}, "vid": 189}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The ruffian boss opened his mouth wide, sweating profusely, looking ahead. The camera zooms out to reveal a group of creatures raising their hands to pass Karen to the right. The camera then cuts to Karen's smiling expression.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 190, "extra_info": {"duration": 5.12, "events": ["The ruffian boss opened his mouth wide.", "The ruffian boss sweated profusely.", "A group of creatures raised their hands.", "The creatures passed Karen to the right.", "Karen smiled."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 190}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man leaned against the wall observing his surroundings and took deep breaths, then relaxed and looked down, when suddenly a pair of hands grabbed and choked the man, pulling him up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 191, "extra_info": {"duration": 4.38, "events": ["The man leaned against the wall.", "The man observed his surroundings.", "The man took deep breaths.", "The man relaxed.", "The man looked down.", "A pair of hands grabbed the man.", "The hands choked the man.", "The hands pulled the man up."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 191}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man said something holding the microphone, then handed the microphone to the approaching woman, who reached out to take the microphone and said something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 192, "extra_info": {"duration": 4.0, "events": ["The man spoke into the microphone.", "The man extended the microphone.", "The woman approached.", "The woman reached out.", "The woman took the microphone.", "The woman spoke."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 192}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person's fingers are placed on the mobile phone screen. The camera switches to show this individual looking at the phone screen, frowning slightly. The camera switches again, capturing from the side as the person puts the phone down, lies on their side, and rests both hands by the pillow.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 193, "extra_info": {"duration": 14.5, "events": ["Person places fingers on mobile phone screen.", "Individual frowns while looking at phone screen.", "Person puts phone down.", "Individual lies on side.", "Individual rests hands by pillow."], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 193}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob reached out to grab a bag and opened it; the air flow from inside the bag surged upwards, deforming SpongeBob’s face as he got blown by it. The camera zooms out, and a creature reaches out to pinch the bag closed and says something to SpongeBob, while Patrick Star turns his head to look to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 194, "extra_info": {"duration": 4.62, "events": ["SpongeBob reached out to grab a bag", "SpongeBob opened the bag", "Air from inside the bag surged upwards", "SpongeBob's face got deformed by the air flow", "A creature reached out to pinch the bag closed", "The creature said something to SpongeBob", "Patrick Star turned his head to look left"], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 194}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boss with a ruffian demeanor walked forward, reached out to open the drawer, which was filled with documents, and he climbed onto the edge of the drawer and said something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 195, "extra_info": {"duration": 3.38, "events": ["The boss walked forward.", "The boss reached out.", "The boss opened the drawer.", "The drawer was filled with documents.", "The boss climbed onto the drawer edge.", "The boss said something."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 195}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person in a diving suit raises both hands, moves their body, turns around to pick up a book, and walks toward the left side of the frame. The camera then cuts to him walking forward, turning around to close the door.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 196, "extra_info": {"duration": 9.12, "events": ["Person raises both hands", "Person moves their body", "Person turns around", "Person picks up a book", "Person walks toward the left side", "Person walks forward", "Person turns around to close the door"], "n_events": 7, "n_shots": 2, "n_subjects": 1}, "vid": 196}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "What the ruffian boss said, after turning around and pressing the red button, the camera moves up, Brother Octopus looks up at the moving lines within the machine, while the other creatures behind him are eating, their bodies slightly moving.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 197, "extra_info": {"duration": 4.88, "events": ["Ruffian boss turns around.", "Ruffian boss presses the red button.", "Brother Octopus looks up.", "Machine lines move.", "Creatures behind Brother Octopus eat.", "Creatures' bodies move slightly."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 197}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob moves forward but suddenly stops. Patrick Star bends forward, and another creature says something and turns to leave. The camera switches to Patrick sitting upright, saying something, and SpongeBob runs over and says something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 198, "extra_info": {"duration": 5.38, "events": ["SpongeBob moves forward.", "SpongeBob suddenly stops.", "Patrick Star bends forward.", "Another creature speaks.", "Another creature turns to leave.", "Patrick Star sits upright.", "Patrick Star says something.", "SpongeBob runs over.", "SpongeBob says something."], "n_events": 9, "n_shots": 2, "n_subjects": 3}, "vid": 198}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera zooms in, the figures of SpongeBob and other creatures gradually blur, the shot switches to a pipe opening, the camera pulls back, SpongeBob stretches out his hand to press the switch button, and raises his hands joyfully laughing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 199, "extra_info": {"duration": 7.38, "events": ["The camera zooms in.", "Figures gradually blur.", "The shot switches to a pipe opening.", "The camera pulls back.", "SpongeBob stretches out his hand.", "SpongeBob presses the switch button.", "SpongeBob raises his hands.", "SpongeBob laughs joyfully."], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 199}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "SpongeBob and Patrick Star lowered their heads and walked forward. Suddenly, SpongeBob stretched out his hand, pointed forward and said something, then together with Patrick Star, they raised their heads to look ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_animation", "task": "video/caption", "idx": 200, "extra_info": {"duration": 4.0, "events": ["SpongeBob and Patrick Star lowered their heads.", "SpongeBob and Patrick Star walked forward.", "SpongeBob stretched out his hand.", "SpongeBob pointed forward.", "SpongeBob and Patrick Star raised their heads.", "SpongeBob said something.", "SpongeBob and Patrick Star looked ahead."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 200}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman and a man were initially talking, and then the woman turned around and raised her hand to call a taxi.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 201, "extra_info": {"duration": 3.63, "events": ["A woman and a man were talking.", "The woman turned around.", "The woman raised her hand.", "The woman called a taxi."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 201}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A young male speaks in an outdoor snowy setting at night. The scene then cuts to two other individuals sitting by a fire. There are no significant movements or actions besides the speaking and facial expressions.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 202, "extra_info": {"duration": 9.47, "events": ["A young male speaks.", "The scene cuts to two individuals.", "Two individuals sit by a fire."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 202}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A boy runs down a set of stairs and chases after another boy down a cobblestone street. A man in a brown suit and hat runs down the stairs and chases after the boys.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 203, "extra_info": {"duration": 9.67, "events": ["A boy runs down a set of stairs.", "A boy chases after another boy.", "A man runs down the stairs.", "A man chases after the boys."], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 203}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A silver car drives on a city road, makes a left turn at an intersection, and proceeds along a driveway towards a red brick building, as seen through a wrought iron fence.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 204, "extra_info": {"duration": 19.68, "events": ["A silver car drives on a city road.", "The silver car makes a left turn at an intersection.", "The silver car proceeds along a driveway.", "The silver car approaches a red brick building."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 204}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men sit in the chair. A boy and a girl enter the scene. A man beckons the boy over and talks to him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 205, "extra_info": {"duration": 8.22, "events": ["Two men sit in chairs.", "A boy and a girl enter the scene.", "A man beckons the boy over.", "The man talks to the boy."], "n_events": 4, "n_shots": 2, "n_subjects": 4}, "vid": 205}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is initially seen reading a magazine, then lowers it to reveal their face, tosses it aside, sits up, and stands to approach a barred window. Another individual passes by the cell in the background.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 206, "extra_info": {"duration": 11.97, "events": ["Person reads a magazine.", "Person lowers the magazine.", "Person reveals their face.", "Person tosses the magazine aside.", "Person sits up.", "Person stands up.", "Person approaches a barred window.", "Another individual passes by the cell."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 206}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman converse, and then the woman closes her eyes and prepares to kiss the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 207, "extra_info": {"duration": 4.42, "events": ["A man and a woman converse.", "The woman closes her eyes.", "The woman prepares to kiss the man."], "n_events": 3, "n_shots": 2, "n_subjects": 3}, "vid": 207}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A male actor runs from the theater aisle toward the camera, stretches out his arms, tips his hat to bow to the audience, and then throws his hat toward the back of the stage.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 208, "extra_info": {"duration": 14.22, "events": ["Male actor runs from the theater aisle.", "Male actor stretches out his arms.", "Male actor tips his hat.", "Male actor bows to the audience.", "Male actor throws his hat toward the back of the stage."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 208}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Ahead, a person walks forward with a gun raised, the camera cuts to three soldiers standing back to back, holding their guns and looking around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 209, "extra_info": {"duration": 5.88, "events": ["A person walks forward with a gun raised.", "Three soldiers stand back to back.", "Soldiers hold their guns.", "Soldiers look around."], "n_events": 4, "n_shots": 4, "n_subjects": 3}, "vid": 209}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man was peeking from behind a tree, another man carrying firewood walked by, the man quickly hid behind the tree, the man carrying the firewood continued to walk forward, the man behind the tree raised a handgun with both hands, aiming at the man carrying the firewood, who slowly turned around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 210, "extra_info": {"duration": 10.5, "events": ["A man peeked from behind a tree.", "Another man walked by carrying firewood.", "The first man quickly hid behind the tree.", "The man with firewood continued to walk forward.", "The first man raised a handgun with both hands.", "The man with firewood turned around slowly."], "n_events": 6, "n_shots": 5, "n_subjects": 2}, "vid": 210}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The light is twinkling; the girl turns her head, and the boy turns his head too. The scene shifts to the green perspective of an object, with plants swaying. The girl lowers her head to pick up the object, turns around, and hands it to the boy, then turns and leaves to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 211, "extra_info": {"duration": 7.25, "events": ["The light twinkles", "The girl turns her head", "The boy turns his head", "Plants sway", "The girl lowers her head", "The girl picks up the object", "The girl turns around", "The girl hands the object to the boy", "The girl turns", "The girl leaves to the left"], "n_events": 10, "n_shots": 3, "n_subjects": 2}, "vid": 211}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman looks at the object in front of her, her body slightly moves, the camera cuts to the forest canopy, then moves down, circling around the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 212, "extra_info": {"duration": 7.12, "events": ["The woman looks at the object.", "The woman's body slightly moves.", "The camera circles around the man."], "n_events": 3, "n_shots": 2, "n_subjects": 3}, "vid": 212}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman standing on a ladder hands a feather to a little elephant, the rope gets burned and then snaps, the woman falls from the ladder, and a person below quickly reacts.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 213, "extra_info": {"duration": 5.12, "events": ["A woman stands on a ladder.", "The woman hands a feather to a little elephant.", "A rope gets burned.", "The rope snaps.", "The woman falls from the ladder.", "A person below quickly reacts."], "n_events": 6, "n_shots": 4, "n_subjects": 3}, "vid": 213}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman, holding an axe, ran to one side and leaned against the wall. Taking advantage of the light, she observed the surroundings of the corridor.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 214, "extra_info": {"duration": 9.88, "events": ["The woman grasped an axe.", "The woman ran to one side.", "The woman leaned against the wall.", "The woman used the light to observe.", "The woman surveyed the corridor's surroundings."], "n_events": 5, "n_shots": 4, "n_subjects": 1}, "vid": 214}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the night, a thick fog descends in front of a door. A woman dressed in theatrical costume pushes past two guards and flings one of them into the water, causing splashes. The woman then turns and walks to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 215, "extra_info": {"duration": 6.38, "events": ["A thick fog descends in front of a door.", "A woman pushes past two guards.", "The woman flings a guard into the water.", "Water splashes upon the guard's impact.", "The woman turns.", "The woman walks to the right."], "n_events": 6, "n_shots": 4, "n_subjects": 3}, "vid": 215}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man ran past the tent, then slowly stopped in front of the gate, while electric sparks emitted from the top of the tent.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 216, "extra_info": {"duration": 4.25, "events": ["The man ran past the tent.", "The man slowly stopped.", "Electric sparks emitted from the tent top."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 216}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The doctor wearing glasses nodded, then took off his mask to speak, while the camera switched to another man glaring ahead with a woman standing with her mouth open.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 217, "extra_info": {"duration": 4.12, "events": ["The doctor nodded.", "The doctor took off his mask.", "Another man glared ahead.", "A woman stood with mouth open."], "n_events": 4, "n_shots": 2, "n_subjects": 3}, "vid": 217}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The character knocked the woman to the ground, then grabbed her and threw her against the wall, before raising an axe and leaping towards her. The woman dodged to the side and spun downwards, grabbing onto the wall to hang there, with a sword in her other hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 218, "extra_info": {"duration": 8.12, "events": ["Character knocked woman to ground", "Character grabbed woman", "Character threw woman against wall", "Character raised axe", "Character leaped towards woman", "Woman dodged to the side", "Woman spun downwards", "Woman grabbed onto wall", "Woman hung on wall", "Woman held sword in hand"], "n_events": 10, "n_shots": 5, "n_subjects": 3}, "vid": 218}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The screen keeps switching angles, the man scoops up metal shavings with his right hand, places them into a bottle in his left hand, and then sets the bottle on the table. His hands then start to fiddle with another object.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 219, "extra_info": {"duration": 5.38, "events": ["The screen switches angles.", "The man scoops up metal shavings with his right hand.", "The man places the metal shavings into a bottle.", "The man sets the bottle on the table.", "The man starts to fiddle with another object."], "n_events": 5, "n_shots": 4, "n_subjects": 1}, "vid": 219}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The mechanical arm reaches into the water, attempting to retrieve glasses that emit a yellow glow. The camera cuts to a different angle from a camera, then switches back to the mechanical arm. In the water with the yellow glow, the mechanical arm crushes the goggles and slowly withdraws from the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 220, "extra_info": {"duration": 9.75, "events": ["The mechanical arm reaches into the water.", "The mechanical arm attempts to retrieve the glasses.", "The glasses emit a yellow glow.", "The mechanical arm crushes the glasses.", "The mechanical arm withdraws from the water."], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 220}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man in the driver's seat looked to the right, the camera angle switched to inside, the woman used her ID card to unlock the door, turned around and opened the door, then walked forward. The door was about to close when the man in the suit inside came over to open it. The man in the suit turned his head to look around and then walked forward with the blonde woman, as the door slowly closed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 221, "extra_info": {"duration": 13.75, "events": ["Man in driver's seat looked right", "Woman used ID card to unlock door", "Woman turned around", "Woman opened the door", "Woman walked forward", "Door was about to close", "Man in suit came to open it", "Man in suit looked around", "Man in suit walked forward with woman", "Door closed slowly"], "n_events": 10, "n_shots": 2, "n_subjects": 4}, "vid": 221}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man looked back, and a woman walked up and opened the door. She turned her head to look at the person coming out. After that, a person in black and another in a trench coat came out. The one in the trench coat turned around and walked forward, speaking to the woman while pointing at her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 222, "extra_info": {"duration": 6.38, "events": ["The man looked back", "A woman walked up", "The woman opened the door", "The woman turned her head", "A person in black came out", "Another in a trench coat came out", "The one in the trench coat turned around", "The one in the trench coat walked forward", "The one in the trench coat spoke to the woman", "The one in the trench coat pointed at the woman"], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 222}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The blond man is baring his teeth and looking ahead. The blond man is pushed away by the person in front of him, and there is someone behind being hung up by a rope. Then another man pushes the black person forwards. The blond man at the back is hitting the person who is hung up, and then there is a close-up of the black person turning his face around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 223, "extra_info": {"duration": 7.62, "events": ["The blond man bares his teeth.", "Person in front pushes the blond man away.", "Someone is hung up by a rope behind.", "Another man pushes the black person forwards.", "The blond man at the back hits the person hung up.", "The black person turns his face around."], "n_events": 6, "n_shots": 2, "n_subjects": 4}, "vid": 223}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans left, the man stands up and steps back, a woman holding flowers walks forward, and the girl is sitting on the bed, holding something in her hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 224, "extra_info": {"duration": 4.88, "events": ["The camera pans left", "The man stands up", "The man steps back", "A woman holding flowers walks forward", "The girl is sitting on the bed", "The girl holds something in her hand"], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 224}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The blue robot turned around and looked down, a spherical object was rolling in the midst of a ruined city.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 225, "extra_info": {"duration": 4.5, "events": ["The blue robot turned around.", "The blue robot looked down.", "A spherical object was rolling.", "The spherical object moved amid ruins."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 225}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four people look ahead and clap, among them the woman at the front turns her head to look at the man to her right, after making eye contact the two shake hands and slightly nod, then look forward again. The man wearing glasses walks over to them and shakes hands with each in turn, below the scene text appears as an introduction.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 226, "extra_info": {"duration": 9.0, "events": ["Four people clap while looking ahead.", "The woman at the front turns her head.", "The woman and the man to her right make eye contact.", "The woman and the man to her right shake hands.", "The woman and the man slightly nod at each other.", "They look forward again.", "The man wearing glasses walks over to the group.", "The man wearing glasses shakes hands with the first person.", "The man wearing glasses shakes hands with the second person.", "Text appears below the scene as an introduction."], "n_events": 10, "n_shots": 2, "n_subjects": 3}, "vid": 226}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The red car is being chased by a police car. The driver inside the vehicle is turning the steering wheel, navigating the car through a roundabout intersection, with the police closely following behind. Several cars are shuttling back and forth on the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 227, "extra_info": {"duration": 8.25, "events": ["The red car is being chased by a police car.", "The driver navigates the red car through a roundabout.", "Cars are shuttling on the road.", "Police car closely follows the red car."], "n_events": 4, "n_shots": 6, "n_subjects": 2}, "vid": 227}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is leaning forward at a table, with a group of people watching her and making various movements and expressions. The woman licks her fingers and chews while looking to the right. Two men are looking at her, the man on the right is extending a hand, while the man on the left is supporting himself on a pillar.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 228, "extra_info": {"duration": 3.38, "events": ["A woman leans forward at a table.", "A group of people watch the woman.", "People make various movements and expressions.", "The woman licks her fingers.", "The woman chews.", "The woman looks to the right.", "A man on the right extends his hand.", "A man on the left supports himself on a pillar."], "n_events": 8, "n_shots": 4, "n_subjects": 5}, "vid": 228}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Underneath the elephant's body, it uses its trunk to blow the feathers away. The camera cuts to a boy behind two railings speaking as he looks ahead, and the girl turns her head to look forward as well.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 229, "extra_info": {"duration": 6.12, "events": ["The elephant uses its trunk to blow away feathers.", "A boy speaks behind two railings.", "A girl turns her head to look forward."], "n_events": 3, "n_shots": 3, "n_subjects": 3}, "vid": 229}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman looks across, where another person with their back to the camera uses a stick to ignite a mixture. The camera follows the movement as the woman blows out the stick, causing sparks to fly from the bowl in front of her. A blue liquid is drawn into a syringe, and a pair of hands places the tray holding the syringe on a table. One hand picks up the syringe, the camera pans up, and there's a cherry on the tray, which reflects the shadow of the woman's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 230, "extra_info": {"duration": 12.12, "events": ["A woman looks across the room", "Person uses a stick to ignite a mixture", "Woman blows out the stick", "Sparks fly from the bowl", "Blue liquid is drawn into a syringe", "Hands place the tray with syringe on a table", "One hand picks up the syringe", "The camera pans up", "A cherry is on the tray", "The cherry reflects the shadow of the woman's movements"], "n_events": 10, "n_shots": 5, "n_subjects": 2}, "vid": 230}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the tunnel, a woman turns around and runs, stopping by a cage, opens the cage, and looks at a fire extinguisher. The camera then cuts to a man running from inside the tunnel.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 231, "extra_info": {"duration": 13.12, "events": ["A woman turns around in the tunnel.", "The woman runs in the tunnel.", "The woman stops by a cage.", "The woman opens the cage.", "The woman looks at a fire extinguisher.", "A man runs from inside the tunnel."], "n_events": 6, "n_shots": 4, "n_subjects": 2}, "vid": 231}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man looked at the elephant in front of him, then the elephant walked forward, and the man's gaze followed the movement of the elephant. The camera switched to the backside of the elephant, where a person wearing a red hat was pushing the elephant forward, with several other people standing nearby, turning their heads to look at the elephant. The camera then switched again, showing a few people pushing against a barrier in front of them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 232, "extra_info": {"duration": 8.5, "events": ["The man looked at the elephant.", "The elephant walked forward.", "The man's gaze followed the elephant.", "A person wearing a red hat pushed the elephant.", "Several people turned their heads to look at the elephant.", "A few people pushed against a barrier."], "n_events": 6, "n_shots": 4, "n_subjects": 5}, "vid": 232}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A character wrapped in bandages and a group of children run forward. They are looking to the left as a person dressed as a rabbit greets them, who then runs ahead to them. The character wrapped in bandages and the group of children stand in place, looking towards the direction of the camera, while two other characters enter the frame from the right, performing some actions.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 233, "extra_info": {"duration": 4.88, "events": ["A character wrapped in bandages runs forward.", "A group of children runs forward.", "The group looks to the left.", "A person dressed as a rabbit greets the group.", "The person dressed as a rabbit runs ahead to the group.", "The character wrapped in bandages stands in place.", "The group of children stands in place.", "The group looks towards the camera's direction.", "Two other characters enter the frame from the right.", "Two characters perform some actions."], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 233}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is riding a horse; a man comes driving a carriage; the woman dismounts from the horse; a man in front of the crowd moves and watches her; the woman runs towards the back of the carriage.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 234, "extra_info": {"duration": 10.62, "events": ["A woman rides a horse.", "A man drives a carriage.", "The woman dismounts from the horse.", "A man moves in front of the crowd.", "The man watches the woman.", "The woman runs towards the carriage's back."], "n_events": 6, "n_shots": 7, "n_subjects": 6}, "vid": 234}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The albino orangutan showed aggression, then picked up a concrete object from the side and smashed it towards the iron mesh gate, while a bald man shouted at it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 235, "extra_info": {"duration": 8.88, "events": ["The albino orangutan showed aggression.", "The albino orangutan picked up a concrete object.", "The albino orangutan smashed the concrete object towards the gate.", "A bald man shouted at the albino orangutan."], "n_events": 4, "n_shots": 9, "n_subjects": 4}, "vid": 235}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy runs in the woods, shaking his head, looking around. The camera follows the movement, then cuts to a blonde woman speaking beside a tree, turning and swaying her body.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 236, "extra_info": {"duration": 4.0, "events": ["The boy runs in the woods.", "The boy shakes his head.", "The boy looks around.", "The blonde woman speaks beside a tree.", "The blonde woman turns.", "The blonde woman sways her body."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 236}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans from a distance to close-up on a building where a man is tied up by an object. The shot switches to two other men: one is climbing upward to the rooftop, while the other man hangs off the edge of the building, glancing down below in panic as debris falls around him. The camera cuts back to the man, showing his surroundings from various angles. He leisurely rotates his wrist and then looks off in one direction.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 237, "extra_info": {"duration": 18.25, "events": ["The camera pans on a building.", "A man is tied up by an object.", "One man climbs upward to the rooftop.", "Another man hangs off the building's edge.", "Debris falls around the hanging man.", "The tied man rotates his wrist.", "The tied man looks off in a direction."], "n_events": 7, "n_shots": 5, "n_subjects": 3}, "vid": 237}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man dressed in brown clothes sat at the edge of the table, and someone was walking around in a room behind him. Then the man turned his head, and around the tables sat others wearing the same clothes. Another man came up beside him and reached out to grab an apple from the table. The man dressed in brown clothes grabbed this man's hand, while a bald man sitting at the next table turned his head to watch everything.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 238, "extra_info": {"duration": 7.5, "events": ["Man sat at the table's edge.", "Someone walked around in the room.", "Man turned his head.", "Others sat around the tables.", "Another man approached the table.", "Man reached out for an apple.", "Man in brown clothes grabbed the approaching man's hand.", "Bald man turned his head to watch."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 238}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man displays an object, takes a few steps forward, and speaks, then the camera shifts to the interior where two people are cleaning the floor. The man wearing a hat moves to the left, walking towards another man, and speaks to him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 239, "extra_info": {"duration": 4.75, "events": ["A man displays an object.", "The man takes a few steps forward.", "The man speaks.", "Two people clean the floor.", "The man wearing a hat moves to the left.", "The man wearing a hat walks towards another man.", "The man wearing a hat speaks to another man."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 239}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The SUV reversed and unraveled the barbed wire. Its bumper fell off, startling the soldiers behind who then stepped back. The SUV drove forward as the soldiers got into the Humvee. The SUV sped away, leaving the spiked bumper on the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 240, "extra_info": {"duration": 9.62, "events": ["The SUV reversed.", "The SUV unraveled the barbed wire.", "The SUV's bumper fell off.", "The soldiers stepped back.", "The SUV drove forward.", "The soldiers got into the Humvee.", "The SUV sped away.", "The spiked bumper remained on the road."], "n_events": 8, "n_shots": 5, "n_subjects": 2}, "vid": 240}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two children fell from the sky towards the ground, flailing their limbs in the air, passing through the treetops, and landing in the water near a waterfall, splashing water around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 241, "extra_info": {"duration": 6.75, "events": ["Two children fell from the sky.", "Children flailed their limbs in the air.", "They passed through the treetops.", "Children landed in the water.", "They splashed water around near a waterfall."], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 241}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is lying on the bed, stroking her pet, then the scene switches to another girl who takes out a small booklet from the bedside table, casually closes the drawer, and tilts her head to look at the booklet.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 242, "extra_info": {"duration": 5.88, "events": ["A woman lies on the bed.", "A woman strokes her pet.", "Another girl takes out a small booklet.", "Another girl closes the bedside table drawer.", "Another girl tilts her head to look at the booklet."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 242}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the field, a group of people are playing. A little girl runs towards the direction of the camera. The shot switches to the interior, where a man is sitting with his arms crossed over his chest. The woman behind him watches the man and walks towards him. The shot switches back to the little girl, following her as she moves, stopping in the frame and smiling forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 243, "extra_info": {"duration": 9.75, "events": ["Group of people play in the field", "Little girl runs towards the camera", "Man sits with arms crossed over chest", "Woman behind man watches the man", "Woman walks towards the man", "Little girl stops and smiles"], "n_events": 6, "n_shots": 3, "n_subjects": 4}, "vid": 243}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman turns her head to look to the right, the camera follows the movement, she walks over and stares at a photo on the mantelpiece, her hands resting on the mantelpiece. The camera switches to the woman's front, as she looks captivated at what's in front of her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 244, "extra_info": {"duration": 6.25, "events": ["The woman turns her head to look right.", "The woman walks over to the mantelpiece.", "The woman stares at a photo on the mantelpiece.", "The woman rests her hands on the mantelpiece."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 244}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man lies on the ground, then props himself up with one hand and turns his head, following this by kneeling on both legs, and lifting his upper body. The camera then cuts to an opening, and the man slowly stands up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 245, "extra_info": {"duration": 9.25, "events": ["The man lies on the ground.", "The man props himself up with one hand.", "The man turns his head.", "The man kneels on both legs.", "The man lifts his upper body.", "The man slowly stands up."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 245}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman opens her mouth and looks forward as her hair flutters and blood flows from her nose. The camera cuts to her hands, from which blue flames are emitted and fly towards a raging fire in front of her. The flames gradually swell, and the camera then switches to a long shot of the fire. Another character stands in front of the fire, looking up at it, as the camera pans upward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 246, "extra_info": {"duration": 7.0, "events": ["The woman opens her mouth.", "The woman's hair flutters.", "Blood flows from the woman's nose.", "The woman's hands emit blue flames.", "Blue flames fly towards the fire.", "The fire swells.", "Another character stands in front of the fire.", "Another character looks up at the fire.", "The camera pans upward."], "n_events": 9, "n_shots": 3, "n_subjects": 2}, "vid": 246}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man wearing a yellow robe is holding an item with both hands, turning his head to look at the person behind him. The person behind reacts, afterward the camera moves downward, and a woman picks up the item from the water and places it into a plate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 247, "extra_info": {"duration": 6.0, "events": ["Man in yellow robe holds item with both hands.", "Man turns head toward person behind.", "Person behind reacts to man turning head.", "Woman retrieves item from water.", "Woman places item into plate."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 247}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pulls away from a sign to reveal a house. The text on the right side slowly fades away as the camera switches to a poster about a hero's disappearance. The camera moves upward towards the top of the poster, and text on the left side of the frame appears, indicating that someone inside the house is shopping.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 248, "extra_info": {"duration": 12.12, "events": ["Camera pulls away from a sign", "Text fades away", "Camera reveals a house", "Camera switches to a poster", "Hero's disappearance is featured on the poster", "Camera moves upward", "Text appears indicating shopping"], "n_events": 7, "n_shots": 2, "n_subjects": 2}, "vid": 248}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman sat on the bed, turned her head to look at the boy lying on the bed, then she turned around, got off the bed and left. The boy moved his head a bit and watched the direction in which the woman left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 249, "extra_info": {"duration": 5.88, "events": ["The woman sat on the bed.", "The woman turned her head to look at the boy.", "The woman turned around.", "The woman got off the bed.", "The woman left.", "The boy moved his head a bit.", "The boy watched the direction the woman left."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 249}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman looks ahead and speaks, while the man with his head lowered turns to look to the side. The camera then cuts to the man moving quickly around the room, placing clothes into a suitcase, as the woman leans against the door watching him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 250, "extra_info": {"duration": 9.88, "events": ["The woman speaks.", "The man turns his head.", "The man places clothes into a suitcase.", "The woman leans against the door."], "n_events": 4, "n_shots": 8, "n_subjects": 2}, "vid": 250}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of hands opens a handbag and pulls out a power cord. The camera cuts to a woman crawling forward, reaching a wall outlet to plug in the power and charge her phone.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 251, "extra_info": {"duration": 10.0, "events": ["Hands open a handbag", "Hands pull out a power cord", "Woman crawls forward", "Woman reaches a wall outlet", "Woman plugs in the power cord", "Woman charges her phone"], "n_events": 6, "n_shots": 2, "n_subjects": 1}, "vid": 251}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little girl holding a pink umbrella angrily yells in the direction of the camera, then turns around and walks away. The camera switches to the opposite side, and a man walks away helplessly, with two other people passing by beside him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 252, "extra_info": {"duration": 4.5, "events": ["A little girl yells in the direction of the camera.", "A little girl turns around.", "A little girl walks away.", "A man walks away helplessly.", "Two people pass by the man."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 252}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man lies in bed, covering his ears with a pillow. The camera cuts to an old man walking into the room from the doorway, holding a stick. The camera cuts again as the old man pokes the man's buttocks with the stick, causing the man to jump up from the bed in pain.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 253, "extra_info": {"duration": 6.62, "events": ["A man lies in bed.", "The man covers ears with pillow.", "An old man walks into the room.", "The old man holds a stick.", "The old man pokes the man's buttocks with stick.", "The man jumps up from bed in pain."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 253}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside the restaurant, a man looks across, and the camera cuts to a woman walking away angrily. The camera switches back to the man, then to a wide shot. The man watches the woman leave, then glances back once more before dejectedly leaning back. Behind him, a man and a woman have been watching them all along, and there are other people dining in the restaurant.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 254, "extra_info": {"duration": 9.75, "events": ["A man looks across the restaurant.", "A woman walks away angrily.", "The man watches the woman leave.", "The man glances back.", "The man leans back dejectedly.", "A man and a woman watch the scene.", "Other people dine in the restaurant."], "n_events": 7, "n_shots": 5, "n_subjects": 3}, "vid": 254}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a monitor with an elderly person speaking to the camera. An old man and a woman are seated behind a table facing the camera. The old man slides an envelope forward, and the woman watches. The camera then cuts to a younger person across from them, who takes the envelope and looks at its cover, before the camera switches to a view of the envelope's cover.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 255, "extra_info": {"duration": 6.12, "events": ["Elderly person speaks to the camera.", "Old man slides an envelope forward.", "Woman watches the old man.", "Younger person takes the envelope.", "Camera shows the envelope's cover."], "n_events": 5, "n_shots": 4, "n_subjects": 4}, "vid": 255}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men and a girl are in the park, the man on the left flips a coin, and the man on the right bends over to catch it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 256, "extra_info": {"duration": 4.75, "events": ["Two men and a girl are in the park", "The man on the left flips a coin", "The man on the right bends over", "The man on the right catches the coin"], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 256}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man takes a drink from a glass, the camera cuts to him gesturing with his glass forward, the camera cuts to an overweight man opposite him also raising his glass, the camera cuts back to the first man as he continues to drink. Beside him, a woman smiles as she watches, the camera then cuts to the overweight man holding his glass and smiling at him, while many people behind the overweight man are also smiling as they watch them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 257, "extra_info": {"duration": 4.25, "events": ["A man takes a drink from a glass.", "A man gestures with his glass forward.", "An overweight man opposite him raises his glass.", "The first man continues to drink.", "A woman beside the first man smiles.", "The overweight man holds his glass and smiles.", "Many people behind the overweight man smile."], "n_events": 7, "n_shots": 5, "n_subjects": 4}, "vid": 257}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man lies on a hospital bed. A bearded man walks from the corridor to the entrance of the ward, glances at the man on the bed, then looks forward. A female police officer, with her arm in a sling, approaches from the opposite direction. The bearded man picks up the item in his hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 258, "extra_info": {"duration": 6.5, "events": ["A man lies on a hospital bed", "A bearded man walks from the corridor", "The bearded man glances at the man on the bed", "A female police officer approaches", "The bearded man picks up the item"], "n_events": 5, "n_shots": 3, "n_subjects": 3}, "vid": 258}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man in the jacket watched the man opposite him, who was wearing floral clothing, throw the Molotov cocktail in his hand into a nearby car and then run away. The man in the jacket watched as the fire started, and then he also ran off.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 259, "extra_info": {"duration": 5.25, "events": ["Man in floral clothing threw Molotov cocktail into car", "Man in floral clothing ran away", "Fire started in the car", "Man in jacket ran off"], "n_events": 4, "n_shots": 3, "n_subjects": 2}, "vid": 259}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a bridge, a motorcyclist dressed in blue waves toward the front. The camera switches to the opposite side, where a man with a selfie stick is focusing intently on taking a selfie, with many people walking by his side. The camera then switches to behind the selfie-taking man as a motorcyclist wearing red comes speeding through and collides with him. The camera switches again, showing the motorcyclist in red racing down some stairs, with the motorcyclist in blue closely following behind, as other people on the stairs swiftly move aside to avoid them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 260, "extra_info": {"duration": 6.0, "events": ["Motorcyclist waves toward the front.", "Man uses selfie stick to take a selfie.", "People walk by the selfie-taking man.", "Motorcyclist in red collides with the selfie-taking man.", "Motorcyclist in red races down the stairs.", "Motorcyclist in blue follows the motorcyclist in red.", "People move aside on the stairs."], "n_events": 7, "n_shots": 4, "n_subjects": 4}, "vid": 260}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man drives a car, drifting around a corner, while a white car chases after him, rushing towards the roadside. A man in a yellow hat quickly jumps into the bed of a pickup truck, and then the white car crashes into it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 261, "extra_info": {"duration": 7.5, "events": ["A man drives a car.", "The car drifts around a corner.", "A white car chases the first car.", "The white car rushes towards the roadside.", "A man in a yellow hat jumps into a pickup truck.", "The white car crashes into the pickup truck."], "n_events": 6, "n_shots": 7, "n_subjects": 4}, "vid": 261}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man puts down the phone from his ear and then places the phone on the table, looking at the man in the hospital bed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 262, "extra_info": {"duration": 7.88, "events": ["The man puts down the phone from his ear.", "The man places the phone on the table.", "The man looks at the man in the hospital bed."], "n_events": 3, "n_shots": 2, "n_subjects": 2}, "vid": 262}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two motorcycles sped past on the road, among other vehicles. The camera then switches to a white-bearded rider who pulls out a gun and fires a shot ahead, hitting a tree in front of him. The rider in front dodges and continues riding forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 263, "extra_info": {"duration": 3.88, "events": ["Two motorcycles sped past on the road.", "Other vehicles were present on the road.", "A white-bearded rider pulled out a gun.", "The white-bearded rider fired a shot.", "The shot hit a tree in front.", "The rider in front dodged the shot.", "The dodging rider continued riding forward."], "n_events": 7, "n_shots": 3, "n_subjects": 5}, "vid": 263}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hand reached out to touch the leg of the person in front, followed by a fist swinging towards someone's face. The camera then cuts to a standing man punching another person sitting on the ground, knocking him out. The camera switches again to show the person on the ground knocked unconscious, while bystanders look on.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 264, "extra_info": {"duration": 3.12, "events": ["A hand touches another person's leg.", "A fist swings towards a face.", "A standing man punches a sitting person.", "The punched person falls to the ground.", "Bystanders witness the unconscious person on the ground."], "n_events": 5, "n_shots": 4, "n_subjects": 3}, "vid": 264}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside the helicopter, two individuals look ahead, while the camera captures an aerial view of the road below where two motorcycles speed along, accompanied by other vehicles on the road. The camera then switches again to capture the profile of a person riding a motorcycle at high speed on the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 265, "extra_info": {"duration": 4.38, "events": ["Two individuals look ahead inside the helicopter.", "The helicopter captures an aerial view of the road.", "Two motorcycles speed along the road.", "Other vehicles accompany the motorcycles on the road.", "The camera captures the profile of a speeding motorcyclist."], "n_events": 5, "n_shots": 3, "n_subjects": 4}, "vid": 265}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is riding a horse forward, the camera cuts to the back of the rider. Another person is running on the road, the camera cuts to the front of the person running on the road. Then it switches to the front of the rider, who raises a whip. The camera cuts again, and a group of people are running on the road. The rider swings the whip and knocks down the people in front, and there are also people falling to the ground as they run behind.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 266, "extra_info": {"duration": 13.12, "events": ["A person rides a horse forward.", "Another person runs on the road.", "The rider raises a whip.", "A group of people run on the road.", "The rider swings the whip.", "The whip knocks down people in front.", "People fall to the ground while running."], "n_events": 7, "n_shots": 8, "n_subjects": 3}, "vid": 266}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of navy soldiers and a bartender are in the tavern. One soldier is dancing on the table in the center; the bartender pulls the tablecloth off of the middle table. Four other soldiers jump onto the four tables at the side, while another five soldiers watch them from behind.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 267, "extra_info": {"duration": 4.62, "events": ["A group of navy soldiers is in the tavern.", "A navy soldier dances on the table.", "The bartender pulls off the tablecloth.", "Four navy soldiers jump onto the side tables.", "Five navy soldiers watch from behind."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 267}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person opens the back door of a car and takes out a bundle of rope. Another person at the front door watches him. He turns around and walks towards the back of the vehicle, fiddling with the rope as he goes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 268, "extra_info": {"duration": 5.75, "events": ["A person opens the back door of a car.", "A person takes out a bundle of rope.", "Another person watches from the front door.", "A person turns around.", "A person walks towards the back of the vehicle.", "A person fiddles with the rope."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 268}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is making a phone call, looking at the call list on the paper, the pen moving upwards, all the way to the top, where he draws a circle around the name of the topmost person.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 269, "extra_info": {"duration": 12.62, "events": ["A person makes a phone call.", "The person looks at the call list on the paper.", "The person's pen moves upwards.", "The person draws a circle around a name."], "n_events": 4, "n_shots": 7, "n_subjects": 1}, "vid": 269}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person grabs the wire mesh with one hand, then picks up a box from the ground, and walks away with the child next to them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 270, "extra_info": {"duration": 5.75, "events": ["Person grabs the wire mesh.", "Person picks up a box from the ground.", "Person walks away.", "Child walks next to the person."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 270}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "As the subway doors were closing, a person dashed into the train carriage, holding a suitcase which got stuck outside the doors. Inside the subway, he forcefully pulled at the suitcase handle, while an elderly person next to him offered support. The camera then cuts to the subway slowly starting to move forward, with him still holding onto the suitcase without letting go.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 271, "extra_info": {"duration": 8.12, "events": ["Person dashed into the subway carriage.", "Suitcase got stuck outside the subway doors.", "Person pulled at the suitcase handle forcefully.", "Elderly person offered support to the person.", "Subway started to move forward."], "n_events": 5, "n_shots": 3, "n_subjects": 3}, "vid": 271}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is swinging an axe down, repeatedly striking with force. The camera angle shifts from the man's upper body to slowly pan down to the ground, where the man is using the axe to chop at a wooden stake in front of him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 272, "extra_info": {"duration": 8.38, "events": ["A person swings an axe down.", "The axe repeatedly strikes with force.", "The man chops at a wooden stake."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 272}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman are in a room, looking at each other. The camera cuts to the woman as she cries in distress and then lowers her head. The man watches her and then takes something out of the refrigerator next to him and leaves the room, with the woman covering her face with her hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 273, "extra_info": {"duration": 14.0, "events": ["A man and a woman look at each other.", "The woman cries in distress.", "The woman lowers her head.", "The man takes something out of the refrigerator.", "The man leaves the room.", "The woman covers her face with her hands."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 273}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are drinking at the bar, with people moving behind them. The blond man takes a sip of his drink, and the black-haired man takes a puff of a cigarette. A woman appears from the left of the frame and sits next to the blond man. The three glance at each other, and the blond man looks at the woman before picking up his glass to take a drink.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 274, "extra_info": {"duration": 11.62, "events": ["Two men drink at the bar.", "People move behind the two men.", "The blond man sips his drink.", "The black-haired man puffs a cigarette.", "A woman sits next to the blond man.", "The three glance at each other.", "The blond man looks at the woman.", "The blond man picks up his glass.", "The blond man takes a drink."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 274}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man in a suit and another man in a vest are sitting on the stairs in front of the door, while a man in a leather jacket stands in front of the door. The two seated men are smoking. The man in the leather jacket takes a step forward, the man in the vest hands over the cigarette in his hand, and the man in the leather jacket takes it and lights it, then walks over and leans against the pillar on the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 275, "extra_info": {"duration": 14.0, "events": ["Two men sit on the stairs.", "The two men smoke.", "A man in a leather jacket stands.", "The man in a leather jacket steps forward.", "The man in a vest hands over a cigarette.", "The man in a leather jacket takes the cigarette.", "The man in a leather jacket lights the cigarette.", "The man in a leather jacket walks over.", "The man in a leather jacket leans against the pillar."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 275}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are looking ahead, the camera cuts to the scene where both are wrestling on the ground. The man in black throws the man in red to the ground. The camera cuts back to the two men from before, showing excited expressions on their faces.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 276, "extra_info": {"duration": 5.62, "events": ["Two men look ahead", "Both men start wrestling", "Man in black throws man in red to the ground", "Men show excited expressions"], "n_events": 4, "n_shots": 3, "n_subjects": 4}, "vid": 276}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Someone struck the drum set with their hands, a man watched the person playing the drums, the person playing spoke something, a woman outside walked to the door, the camera switched to the upper body of the woman, and she looked at them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 277, "extra_info": {"duration": 4.62, "events": ["Someone struck the drum set with their hands.", "A man watched the person playing the drums.", "The person playing the drums spoke.", "A woman outside walked to the door.", "The woman looked at them."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 277}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people are wrestling on the mat; one person is forcefully pinning down the other. The referee is lying on the ground, observing them. The camera switches to a man in blue, who is intently watching the action. The camera switches back to the competition area where the person underneath is struggling fiercely. The referee waves his hand, pointing diagonally upwards. Many people in the background are watching the match.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 278, "extra_info": {"duration": 5.25, "events": ["Two people are wrestling on the mat.", "One person is pinning down the other.", "The referee is observing from the ground.", "The man in blue is watching the action.", "The person underneath is struggling fiercely.", "The referee waves his hand.", "People in the background are watching the match."], "n_events": 7, "n_shots": 3, "n_subjects": 5}, "vid": 278}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men looked ahead, the man on the right pulled up the zipper of his jacket, and then both turned around and walked back, as another man approached from behind them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 279, "extra_info": {"duration": 4.5, "events": ["Two men looked ahead.", "The man on the right pulled up his zipper.", "Both men turned around.", "Both men walked back.", "Another man approached from behind."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 279}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angle starts from below and pans up to capture a lady's face, then the lady turns her head upwards, and the camera switches to a wide shot, revealing that the lady is participating in a figure skating competition, with judges and spectators watching in the background.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 280, "extra_info": {"duration": 6.88, "events": ["Camera pans up to capture a lady's face", "Lady turns her head upwards", "Camera reveals a figure skating competition", "Judges watch the performance", "Spectators observe from the background"], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 280}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man leans forward with both hands on a red railing, shouting out towards the front. A lady glances back while gliding forward on the ice rink, then turns her whole body to face the camera, waving in the direction of the camera, and continues to slide forward backwards. The camera then switches back to the man, who is speaking towards the front.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 281, "extra_info": {"duration": 4.38, "events": ["A man leans forward on a red railing.", "The man shouts out towards the front.", "A lady glances back while gliding on ice.", "The lady turns her whole body to face the camera.", "The lady waves in the direction of the camera.", "The lady continues to slide forward backwards."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 281}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The shot pans from a low angle upwards, capturing a woman looking down from a railing. A man runs over and also looks down. The shot then switches to a bird's-eye view, showing a person lying on the ground in the middle of a building. The man pulls the woman away from the railing, and the shot changes to a level perspective as the man is seen leading the woman away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 282, "extra_info": {"duration": 7.0, "events": ["Camera pans upwards", "Woman looks down from railing", "Man runs over to railing", "Shot switches to bird's-eye view", "Person lies on the ground", "Man pulls woman away from railing", "Shot changes to level perspective", "Man leads woman away"], "n_events": 8, "n_shots": 3, "n_subjects": 2}, "vid": 282}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person wearing a hat is driving, slowly moving forward on the road. Through the windshield and side window, one can see a man wearing a duckbill cap and a woman in a green long coat walking in the opposite direction of the car on the right-hand sidewalk. The camera follows the gaze of the person inside the car, initially showing the front of the two pedestrians, then their side profiles, and finally their backs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 283, "extra_info": {"duration": 9.75, "events": ["A person drives forward on the road.", "A man in a duckbill cap walks on the sidewalk.", "A woman in a green long coat walks on the sidewalk.", "The man and woman walk in the opposite direction of the car.", "The camera follows the gaze of the person inside the car.", "The camera shows the front of the pedestrians.", "The camera shows the side profiles of the pedestrians.", "The camera shows the backs of the pedestrians."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 283}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At night, a black car moves from the left side of the frame towards the right. In the shadows by the wall, a man wearing a hat stands with his arms folded, watching as the black car drives into a well-lit area and comes to a stop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 284, "extra_info": {"duration": 10.25, "events": ["A black car moves from left to right.", "A man stands by the wall.", "The man folds his arms.", "The black car drives into a well-lit area.", "The black car comes to a stop."], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 284}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man bent over and, using his thick fingers, pressed and pulled open a drawer. A woman with long hair, holding a bottle, looked at the man, who then stood up straight and faced the woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 285, "extra_info": {"duration": 4.0, "events": ["A man bent over.", "Man pressed open a drawer.", "Man pulled open the drawer.", "Woman holding a bottle looked at the man.", "Man stood up straight.", "Man faced the woman."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 285}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman lies on her side on the sofa making a phone call. The camera then switches to show the woman putting down the phone with her left hand and looking at the notebook in her right hand. The camera switches again to show the woman lying on her stomach on the sofa making a phone call.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 286, "extra_info": {"duration": 5.5, "events": ["A woman lies on her side on the sofa.", "The woman makes a phone call.", "The woman puts down the phone with her left hand.", "The woman looks at the notebook in her right hand.", "The woman lies on her stomach on the sofa.", "The woman makes another phone call."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 286}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a man with a briefcase on his back and a bunch of flowers in his left hand, walking towards a grave while making a phone call, as the camera pans from left to right in the cemetery.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 287, "extra_info": {"duration": 4.88, "events": ["A man walks towards a grave.", "The man carries a briefcase on his back.", "The man holds a bunch of flowers in his left hand.", "The man makes a phone call."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 287}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera revolves around a man and a woman who are talking face to face, panning from one side of the two to the other, with people walking by on the side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 288, "extra_info": {"duration": 3.88, "events": ["The camera revolves around a man and a woman.", "The man and the woman are talking to each other.", "People are walking by the conversation."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 288}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bare-chested man leans forward, glancing at the clothes he is putting on. He then slips his arms into the sleeves with effort, buttons up, and looks to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 289, "extra_info": {"duration": 4.0, "events": ["A bare-chested man leans forward.", "Man glances at the clothes.", "Man slips arms into sleeves.", "Man buttons up the clothes.", "Man looks to the left."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 289}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A middle-aged man and woman walked towards the camera, looking at a woman with a hat in the direction of the camera. The woman quickly turned around, and the camera cut to the ground. A cigarette butt was thrown onto the floor, then the camera switched back to the woman with the hat standing with a man in a green outfit. The woman covered her mouth with her hand, and the camera cut to the ground again, where a foot stepped on the cigarette butt.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 290, "extra_info": {"duration": 3.62, "events": ["Middle-aged man and woman walked towards the camera.", "Woman with hat looked at the camera direction.", "Woman with hat quickly turned around.", "Cigarette butt was thrown onto the floor.", "Woman with hat stood with man in green outfit.", "Woman with hat covered her mouth with her hand.", "Foot stepped on the cigarette butt."], "n_events": 7, "n_shots": 4, "n_subjects": 5}, "vid": 290}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman with long hair glanced at the wall, then turned and walked towards the exit. The man standing in the room, in disbelief, covered his forehead with his hand. The woman walked to the end of the room, turned around, and walked back. When she reached the door, she spoke to the man with a smile, and the man lowered his hand and smiled back at her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 291, "extra_info": {"duration": 9.0, "events": ["A woman glanced at the wall.", "The woman turned towards the exit.", "A man covered his forehead with his hand.", "The woman walked to the end of the room.", "The woman turned around.", "The woman walked back to the door.", "The woman spoke to the man with a smile.", "The man lowered his hand.", "The man smiled back at the woman."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 291}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a street bustling with people, a man in a green shirt handed over his camera to the woman in front of him who was wearing a hat. The woman took a puff of her cigarette with her right hand and then threw it to the ground, exhaling the smoke. She then fiddled with the camera using both hands, and the lens quickly turned to the man in the green shirt. He gestured with his hands as he walked towards the woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 292, "extra_info": {"duration": 6.88, "events": ["Man in green shirt handed over camera to woman.", "Woman with hat took puff of cigarette.", "Woman threw cigarette to ground.", "Woman exhaled smoke.", "Woman fiddled with camera.", "Camera lens turned to man in green shirt.", "Man gestured with hands.", "Man walked towards woman."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 292}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is walking down from the stairs, and a man downstairs is putting on his coat, looking at the woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 293, "extra_info": {"duration": 3.0, "events": ["A woman walks down the stairs.", "A man downstairs puts on his coat.", "The man looks at the woman."], "n_events": 3, "n_shots": 2, "n_subjects": 2}, "vid": 293}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man is lying on his side in bed, a woman walks to the bedside, places down a scarf, reaches out to take off the man's glasses, and then gently grasps the man's hair with her hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 294, "extra_info": {"duration": 10.5, "events": ["A man is lying on his side in bed", "A woman walks to the bedside", "A woman places down a scarf", "A woman reaches out", "A woman takes off the man's glasses", "A woman gently grasps the man's hair"], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 294}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man in white clothes is pressing down on another man, moving towards the house ahead, while the trunk of the car behind them slowly opens.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 295, "extra_info": {"duration": 6.0, "events": ["A man presses down on another man.", "They move towards the house.", "The trunk of the car opens slowly."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 295}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man in white clothes stands leaning against a tree, the camera cuts to a soccer field, where a flock of birds takes flight from the field.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 296, "extra_info": {"duration": 9.12, "events": ["A man stands leaning against a tree.", "Birds take flight from the soccer field."], "n_events": 2, "n_shots": 2, "n_subjects": 1}, "vid": 296}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person lays their head against the car window, looking inside. Then, they stand up, and the window reflects their shadow. They turn their head to look behind, and afterwards, they walk away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 297, "extra_info": {"duration": 5.88, "events": ["A person lays head against the car window", "The person stands up", "Car window reflects the person's shadow", "The person turns head to look behind", "The person walks away"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 297}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At sunset, a person with a cane walks forward, followed by someone behind them. The camera cuts to the person with the cane sitting down, gazing at the setting sun.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 298, "extra_info": {"duration": 6.62, "events": ["Sun sets.", "Person walks forward with a cane.", "Someone follows the person with a cane.", "Person with a cane sits down.", "Person gazes at the setting sun."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 298}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man bends down to look into the mirror, then straightens up, turns to face the camera, looks back at the mirror, and the camera moves forward, pushing all the way to the edge of the mirror near a photo of a woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 299, "extra_info": {"duration": 10.5, "events": ["A man bends down to look into the mirror.", "The man straightens up.", "The man turns to face the camera.", "The man looks back at the mirror.", "The camera moves forward to the edge of the mirror."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 299}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, the silhouette of a man sits motionless atop a wall, the camera angle switches to the man's side profile, and the man lifts his head to look forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 300, "extra_info": {"duration": 7.62, "events": ["The silhouette of a man sits atop a wall.", "The man lifts his head."], "n_events": 2, "n_shots": 2, "n_subjects": 1}, "vid": 300}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A long-haired man was knocked down by another man wearing a white hat. Then, the man with the white hat opened the car door, got inside, and started the vehicle. The man who had been knocked down raised his hand, prompting the man in the white hat to stop the car. He then used the car door to hit the fallen man before driving off. The man lying on the ground got up from the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 301, "extra_info": {"duration": 13.25, "events": ["A man knocked down a long-haired man.", "The man in the white hat opened the car door.", "The man in the white hat got inside the car.", "The man in the white hat started the vehicle.", "The long-haired man raised his hand.", "The man in the white hat stopped the car.", "The man in the white hat hit the long-haired man with the car door.", "The man in the white hat drove off.", "The long-haired man got up from the ground."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 301}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside the first magnificent building, a man pressed the elevator button, then stepped back and waited for the elevator to arrive, before turning his head to look to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 302, "extra_info": {"duration": 5.5, "events": ["A man pressed the elevator button.", "A man stepped back.", "The elevator arrived.", "A man turned his head to the left."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 302}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures a panoramic shot of a bathroom scene where a woman is looking into a mirror, and a man is sitting on the toilet. The shot switches to a close-up of the woman, who adjusts her hair. The man stands up from the toilet, the woman closes the cabinet door in front of the mirror, and the man picks up a towel from the sink to dry his hands. The woman then turns around to speak with the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 303, "extra_info": {"duration": 7.12, "events": ["Woman looks into mirror", "Man sits on toilet", "Woman adjusts her hair", "Man stands up from toilet", "Woman closes cabinet door", "Man picks up towel from sink", "Man dries hands with towel", "Woman turns around", "Woman speaks with man"], "n_events": 9, "n_shots": 4, "n_subjects": 2}, "vid": 303}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A black player leaps into the air to complete a dunk. The camera then cuts to the bench, where the coach raises his fist in excitement, and the substitute players stand up, stirred by the action. Nearby, a reporter hurriedly snaps photos. The camera switches to a frontal view of the substitute players, who, along with the audience, are cheering. The players continue to move around on the court while the camera shifts to an elderly person, panning from right to left across the elder's head. The background of the video is a basketball game.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 304, "extra_info": {"duration": 7.62, "events": ["Black player completes a dunk.", "Coach raises fist in excitement.", "Substitute players stand up.", "Reporter snaps photos.", "Substitute players cheer.", "Audience cheers.", "Players continue to move on court.", "Camera pans across elderly person's head."], "n_events": 8, "n_shots": 4, "n_subjects": 4}, "vid": 304}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person slightly moved their lips towards the camera; the camera angle switched to the side of the man as he opened the door of a storage locker.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 305, "extra_info": {"duration": 5.38, "events": ["A person slightly moved their lips towards the camera.", "The man opened the door of a storage locker."], "n_events": 2, "n_shots": 2, "n_subjects": 1}, "vid": 305}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man first supports the chin of the woman in front of him with his left hand, then switches to his right hand, and then kisses the woman on her forehead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 306, "extra_info": {"duration": 5.62, "events": ["Man supports woman's chin with his left hand.", "Man switches to his right hand.", "Man kisses woman on her forehead."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 306}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a street, a woman walks towards a flower shop ahead, moving forward along the road together with a man carrying an umbrella coming from the front. Many people are walking around. The camera switches to the front of a building where the man and woman walk past it. Vehicles are passing on the road, and someone walks towards the direction of the camera, startling a flock of birds into flight.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 307, "extra_info": {"duration": 8.25, "events": ["A woman walks towards a flower shop.", "A man carrying an umbrella moves forward.", "People walk around the street.", "The man and woman walk past a building.", "Vehicles pass on the road.", "Someone walks towards the camera.", "A flock of birds takes flight."], "n_events": 7, "n_shots": 2, "n_subjects": 3}, "vid": 307}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a scene inside a restaurant, where a man places the newspaper he is holding onto a chair, turns around to face another gentlemen seated across the table. The man across the table takes out a cigarette and puts it in his mouth, while other people in the restaurant are moving about.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 308, "extra_info": {"duration": 4.25, "events": ["A man places the newspaper onto a chair.", "The man turns around to face another seated man.", "The seated man takes out a cigarette.", "The seated man puts the cigarette in his mouth.", "People in the restaurant are moving about."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 308}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows the lush foliage of plants, with a grasshopper climbing up a leaf. The camera angle then switches to a low-angle shot, capturing a large flock of birds flying in the sky.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 309, "extra_info": {"duration": 7.75, "events": ["Plants show lush foliage.", "Grasshopper climbs up a leaf.", "Birds fly in the sky."], "n_events": 3, "n_shots": 2, "n_subjects": 2}, "vid": 309}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera moves from right to left, viewing through the glass windows of the house in front, a woman is seen taking off her coat. The shot then switches to outside of the house, where a man wearing a hat emerges from behind a tree and then turns to walk towards the direction of the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 310, "extra_info": {"duration": 6.75, "events": ["Camera moves from right to left", "Woman takes off her coat", "Man wearing a hat emerges from behind a tree", "Man turns to walk towards the camera's direction"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 310}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man is standing in front of the wooden door of a stone house, looking back towards the direction of a girl. The camera films from behind the girl and then pans around her from left to right, rotating to face the girl front on.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 311, "extra_info": {"duration": 6.0, "events": ["A man stands in front of a wooden door.", "The man looks back towards the girl.", "The camera pans around the girl from left to right.", "The camera rotates to face the girl front on."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 311}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A female police officer with glasses strides toward the camera, swinging a baton. She stops in front of a cell, turns around to look inside, where a girl dressed in white is facing the wall, curled up and lying on the floor.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 312, "extra_info": {"duration": 5.75, "events": ["A female police officer strides toward the camera.", "The female police officer swings a baton.", "The female police officer stops in front of a cell.", "The female police officer turns around.", "A girl lies on the floor facing the wall."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 312}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The judge lifted the hand of the athlete on the right, then let it go, the camera switched to the audience seating, where the audience members clapped and cheered for the athlete.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 313, "extra_info": {"duration": 3.88, "events": ["The judge lifted the athlete's hand.", "The judge let the athlete's hand go.", "The audience members clapped.", "The audience members cheered for the athlete."], "n_events": 4, "n_shots": 2, "n_subjects": 3}, "vid": 313}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman lies in bed, facing away from the camera towards the headboard. After a moment, she turns her head to face the ceiling. The camera angle changes, and a bouquet of artificial flowers is lifted up from the bottom of the screen by a hand. The woman stands up and looks up at the flowers, then she walks from the left side of the flowers to the right side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 314, "extra_info": {"duration": 9.75, "events": ["A woman lies in bed.", "Woman turns her head to face the ceiling.", "Hand lifts a bouquet of artificial flowers.", "Woman stands up.", "Woman looks up at the flowers.", "Woman walks from left to right side of the flowers."], "n_events": 6, "n_shots": 2, "n_subjects": 1}, "vid": 314}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the bed in the bedroom, a man covers a woman with a blanket and wraps it tightly around her, then makes eye contact with her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 315, "extra_info": {"duration": 5.62, "events": ["Man covers woman with a blanket.", "Man wraps blanket tightly around woman.", "Man makes eye contact with woman."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 315}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the room, a woman sits on the floor leaning against the bed, holding a square cup in her hand, while a man lies on the bed, pouring a glass of wine for the woman. Then the man puts the wine bottle on the floor, the woman takes a sip, and afterwards the man props his arm on the bed and lifts his head to look upwards, as the woman tilts her head to look diagonally downward after finishing her drink.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 316, "extra_info": {"duration": 5.0, "events": ["Woman sits on floor leaning against bed.", "Woman holds square cup in hand.", "Man lies on bed pouring wine for woman.", "Man places wine bottle on floor.", "Woman takes a sip of wine.", "Man props arm on bed.", "Man lifts head to look upwards.", "Woman tilts head to look diagonally downward."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 316}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera faces the stream flowing ahead and slowly moves forward. The shot changes to an aerial view of an open space, where three bald monks are seen walking from below the camera. The camera gradually lowers and shifts to a rear view of the three monks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 317, "extra_info": {"duration": 11.0, "events": ["The camera moves forward", "Three bald monks walk", "The camera lowers", "The camera shifts to a rear view"], "n_events": 4, "n_shots": 2, "n_subjects": 3}, "vid": 317}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video was shot on a dark night, with the camera pointed at the pitch-black sea ahead, then quickly panning left to capture a man wearing a red hat. The camera then switches to a seaside platform, where the man with the red hat is standing sideways at the edge. An airplane flies overhead, and the man watches as it passes before turning back to face the camera and walking forward, with the lights on the platform swaying.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 318, "extra_info": {"duration": 11.5, "events": ["Camera captured pitch-black sea", "Camera panned to a man with a red hat", "Man with a red hat stood on a seaside platform", "An airplane flew overhead", "Man with a red hat watched the airplane", "Man with a red hat turned back to face the camera", "Man with a red hat walked forward", "Lights swayed on the platform"], "n_events": 8, "n_shots": 2, "n_subjects": 1}, "vid": 318}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the deck of a ship, a man in blue clothes drew a line in front of several items with chalk, then the man stood up, placed the chalk in his left chest pocket, took a step forward, stood on a piece of equipment, and crossed his arms, looking ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 319, "extra_info": {"duration": 10.75, "events": ["A man drew a line with chalk", "The man stood up", "The man placed the chalk in his pocket", "The man took a step forward", "The man stood on a piece of equipment", "The man crossed his arms", "The man looked ahead"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 319}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man with a red hat takes a fish out of a bucket, walks down a small wooden bridge above the pool, and the camera switches to the man standing on a ladder, holding the fish with one hand outstretched. An orca leaps into the air and eats the fish from the man's hand, then falls back down. The camera then switches again, focusing on a railing in front of him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 320, "extra_info": {"duration": 10.25, "events": ["Man takes fish out of bucket", "Man walks down wooden bridge", "Man stands on ladder", "Man holds fish outstretched", "Orca leaps into the air", "Orca eats fish from man's hand", "Orca falls back down"], "n_events": 7, "n_shots": 3, "n_subjects": 1}, "vid": 320}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman are sitting on the bed in the cabin, the woman looks at the man, the man looks forward, the camera then cuts to the corridor, the woman walks ahead, the man comes down from a distant staircase and looks towards the woman.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 321, "extra_info": {"duration": 4.25, "events": ["A man and a woman sit on the bed.", "The woman looks at the man.", "The man looks forward.", "The woman walks in the corridor.", "The man comes down the staircase.", "The man looks towards the woman."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 321}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a bright room, a formally dressed man is sitting on a sofa speaking to the camera, with several staff members doing their own things behind him. Suddenly, a light bulb bursts and the screen goes dark. After a while, a faint light appears on the left side of the screen, revealing a staff member.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 322, "extra_info": {"duration": 4.5, "events": ["A man is speaking to the camera", "A light bulb bursts", "The screen goes dark", "A faint light appears", "A staff member is revealed"], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 322}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a woman is using clippers to cut her hair. She pushes the clippers to the back of her head, turns to look at the screen, then faces the screen and smiles slightly, before putting the clippers on her head again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 323, "extra_info": {"duration": 6.0, "events": ["A woman uses clippers to cut her hair.", "The woman pushes the clippers to the back of her head.", "The woman turns to look at the screen.", "The woman faces the screen.", "The woman smiles slightly.", "The woman puts the clippers on her head again."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 323}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is combing her hair in front of a mirror, first combing the hair on the left side, then the hair on the right side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 324, "extra_info": {"duration": 6.38, "events": ["A woman is combing her hair.", "The woman combs the left side of her hair.", "The woman combs the right side of her hair."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 324}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angles down from the entrance of the camp towards the inside, where two girls are walking into the camp. Inside, many children are playing. The shot then cuts to a body of water, where a boy runs along a wooden platform by the lakeside before leaping into the water. Nearby, some people are kissing while others are walking forward. The camera then switches to capture a sign with the camp rules, and once again cuts to an archery range, where a girl is drawing a bow, with others beside her also engaged in archery activities.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 325, "extra_info": {"duration": 4.5, "events": ["Two girls walk into the camp.", "Children play inside the camp.", "A boy runs along a wooden platform.", "The boy leaps into the water.", "Some people kiss by the body of water.", "Others walk by the body of water.", "A sign displays the camp rules.", "A girl is drawing a bow at the archery range.", "Others engage in archery activities."], "n_events": 9, "n_shots": 4, "n_subjects": 5}, "vid": 325}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a night with only a faint glow, a woman sat on the ground with her left leg stretched out in front of her, and a person lying in front of her. The woman took a flashlight in her right hand, shining it towards the left side of her left leg, and with her left hand, she picked up a notebook and flipped it open, looking at it while pointing with her finger.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 326, "extra_info": {"duration": 4.5, "events": ["A night presents faint glow.", "A woman sits on the ground.", "Woman's left leg is stretched out.", "A person lies in front of the woman.", "Woman takes flashlight with her right hand.", "Woman shines flashlight towards left side of her leg.", "Woman picks up notebook with her left hand.", "Woman flips notebook open.", "Woman looks at notebook.", "Woman points with her finger at notebook."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 326}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman picks up a cell phone, a child crawls out from the blankets, and the woman, child, and another older girl sit down together. The woman presses on the screen to start taking a selfie.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 327, "extra_info": {"duration": 8.0, "events": ["A woman picks up a cell phone.", "A child crawls out from the blankets.", "The woman sits down.", "An older girl sits down.", "The woman presses on the screen."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 327}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man hurriedly gets out of the car. The camera cuts to the outside of the car, where the man quickly gets out of the taxi, closes the door, and runs forward. As he runs, he looks towards the right front, and the camera switches to show an electronic screen displaying text.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 328, "extra_info": {"duration": 5.38, "events": ["Man hurriedly exits the car", "Man closes the car door", "Man runs forward", "Man looks towards the right front", "Camera shows an electronic screen displaying text"], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 328}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A large group of people are jumping around together, the camera focuses on a man in floral clothing and a woman in pink clothing, then the shot switches to their upper bodies. The woman wraps her arms around the man's neck, and then the two kiss each other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 329, "extra_info": {"duration": 6.38, "events": ["A large group of people jump around together", "The camera focuses on a man in floral clothing", "The camera focuses on a woman in pink clothing", "The woman wraps her arms around the man's neck", "The man and the woman kiss each other"], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 329}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The ATM machine is furiously spewing out cash, the camera switches to a man who yells in reaction to this, followed by the camera switching to the ATM screen which displays two lines of text.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 330, "extra_info": {"duration": 4.5, "events": ["The ATM machine spews out cash.", "A man yells in reaction.", "The ATM screen displays two lines of text."], "n_events": 3, "n_shots": 3, "n_subjects": 2}, "vid": 330}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man enters the driver's seat of a car, picks up his phone, and places it on the mount.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 331, "extra_info": {"duration": 3.88, "events": ["A man enters the driver's seat.", "The man picks up his phone.", "The man places the phone on the mount."], "n_events": 3, "n_shots": 2, "n_subjects": 1}, "vid": 331}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man dressed in grey-blue clothes gestured a thumbs-up toward the front. Behind him, a man in red clothes and another person wearing a hat were squatting down, looking ahead. The camera then cuts to a person in red clothes on the opposite side, who rolls a basketball forward. The basketball rolls along the ground toward the man in grey-blue clothes, while a woman in red clothes walks towards them from the background. The camera switches back to the man in grey-blue, who kicks the basketball into the air with one foot. The man in red and the one with the hat behind him stand up to watch. The man in red clothes on the opposite side spreads his arms to catch the flying basketball and turns his head to look to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 332, "extra_info": {"duration": 4.88, "events": ["A man gives a thumbs-up.", "Two people squat, looking ahead.", "Person in red rolls a basketball.", "Woman in red walks forward.", "Man in grey-blue kicks basketball into air.", "Two people stand up to watch.", "Man in red catches the basketball.", "Man in red turns head to right."], "n_events": 8, "n_shots": 4, "n_subjects": 5}, "vid": 332}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera shoots from inside the car towards the front, through the car window glass we see a man in a white tank top standing in front of the car. The driver shifts gears, the camera then switches to the perspective of the man in the white tank top outside the vehicle, looking at the man and woman in the driver and passenger seats. The camera angle switches again towards the direction of the car but focuses on the woman's side outside the vehicle. The man in the white tank top is leaning on a stick, tilting his head to look inside; he watches for a while, then shows a faint smile. Next, the camera switches back to the perspective of the man in the white tank top, as the woman in the passenger seat pushes the door open, preparing to get out of the car.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 333, "extra_info": {"duration": 7.62, "events": ["Man in white tank top stands in front of the car.", "Driver shifts gears.", "Man in white tank top leans on a stick.", "Man in white tank top tilts his head to look inside.", "Man in white tank top shows a faint smile.", "Woman in passenger seat pushes the door open."], "n_events": 6, "n_shots": 4, "n_subjects": 3}, "vid": 333}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man walked towards a yellow car, followed by a woman. When the man got to the car door, he held something in his left hand and used the car key in his right hand to open the door. He then leaned over and stuffed the item from his left hand under the driver's seat.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 334, "extra_info": {"duration": 10.5, "events": ["A man walked towards a yellow car", "A woman followed the man", "The man reached the car door", "The man held something in his left hand", "The man used the car key to open the door", "The man leaned over", "The man stuffed the item under the driver's seat"], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 334}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The mirror reflected a man lying on the bed and a woman leaning against the headboard. The camera angle then switched to the bed, where the man glanced at the woman several times before sitting up to check the clock on the table, then lay back down again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 335, "extra_info": {"duration": 11.38, "events": ["The mirror reflected a man lying on the bed.", "A woman leaned against the headboard.", "The man glanced at the woman several times.", "The man sat up to check the clock.", "The man lay back down again."], "n_events": 5, "n_shots": 4, "n_subjects": 2}, "vid": 335}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside a car, the person in the passenger seat opened a box, revealing the cake inside. The man in the driver's seat looked towards the passenger seat, and the person in the passenger seat took out a cake knife and cut a piece of the cake.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 336, "extra_info": {"duration": 7.75, "events": ["The person in the passenger seat opened a box", "The box revealed the cake inside", "The man in the driver's seat looked towards the passenger seat", "The person in the passenger seat took out a cake knife", "The person in the passenger seat cut a piece of the cake"], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 336}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man looks ahead with lowered eyes. The woman opposite him waves her hand and slaps the man across the face. The man's head twists to the side from the impact, and then he looks forward again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 337, "extra_info": {"duration": 2.38, "events": ["Man looks ahead with lowered eyes.", "Woman opposite man waves her hand.", "Woman slaps man across the face.", "Man's head twists to the side from impact.", "Man looks forward again."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 337}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the car, a man took a paper bag from the side and pulled out a piece of food from it, and started eating it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 338, "extra_info": {"duration": 5.5, "events": ["Man took paper bag from the side.", "Man pulled out piece of food from bag.", "Man started eating the food."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 338}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of a field, a woman lifts a little girl high up, the camera then cuts to an open space in front of a house, where a boy in green clothes, with his back to the camera, throws an American football to another boy dressed in black. The ball is intercepted by a man wearing a white shirt, who turns around, pats the head of the boy in black, puts his arm around his shoulder, and walks towards the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 339, "extra_info": {"duration": 4.5, "events": ["Woman lifts little girl.", "Boy in green throws football.", "Man intercepts football.", "Man pats boy in black's head.", "Man puts arm around boy in black's shoulder.", "Man and boy in black walk towards camera."], "n_events": 6, "n_shots": 2, "n_subjects": 5}, "vid": 339}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A black cowboy rides a horse, looking solemnly downwards. He takes off the cowboy hat from his head and places it over his chest. Beside him, a woman on horseback also rests her cowboy hat on her chest. Next to the woman is a group of riderless horses.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 340, "extra_info": {"duration": 7.62, "events": ["A black cowboy rides a horse.", "The cowboy looks downward solemnly.", "The cowboy takes off the cowboy hat.", "The cowboy places the cowboy hat over the cowboy's chest.", "A woman on horseback rests her cowboy hat on her chest.", "A group of riderless horses accompany the riders."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 340}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a split screen shot, with the left side featuring two male workers in the foreground and another worker visible in the background. On the right side, a rocket is flying upwards to the right, with a blast of flame propelling it forward from the tail of the rocket. Surrounding the flame is a large trail of smoke and dust. In the left side of the frame, one of the workers looks up, and the right side of the screen moves to the left, covering the entire frame. The rocket continues to fly when suddenly, a thick burst of flame erupts from below the rocket.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 341, "extra_info": {"duration": 7.88, "events": ["Two male workers stand in the foreground.", "Another worker is visible in the background.", "A rocket flies upwards with a blast of flame.", "Flame propels the rocket forward from its tail.", "A large trail of smoke and dust surrounds the flame.", "One worker looks up at the sky.", "The right side screen covers the entire frame.", "The rocket continues to fly.", "A thick burst of flame erupts from below the rocket."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 341}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displayed a photograph featuring a woman, a man, and three children. The camera then switched to a room where two male staff members were helping a male astronaut put on his spacesuit, with another staff member also working nearby.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 342, "extra_info": {"duration": 6.62, "events": ["Photograph displayed a woman, a man, and three children.", "Camera switched to a room.", "Two male staff members helped a male astronaut.", "A male astronaut put on his spacesuit.", "Another staff member worked nearby."], "n_events": 5, "n_shots": 2, "n_subjects": 4}, "vid": 342}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a workroom, a man stands in front of a cabinet with its doors open, taking things out from inside. He is holding the items in his arms, having taken so many that he has to use his chin to keep them in place. Afterwards, he closes the cabinet doors, turns around, and walks over to a metal rack. He picks up a bucket of drinks and walks behind the rack.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 343, "extra_info": {"duration": 7.88, "events": ["A man stands in front of an open cabinet", "The man takes items out of the cabinet", "The man holds items using his chin", "The man closes the cabinet doors", "The man turns around", "The man walks over to a metal rack", "The man picks up a bucket of drinks", "The man walks behind the rack"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 343}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two boxers are competing in the ring. The one in red shorts throws a punch, the one in black shorts ducks to avoid it, then rushes into the embrace of the boxer in red shorts. The camera cuts to a scene where the boxer in black shorts falls to the ground. The referee comes over to separate the two fighters, and the boxer in black shorts stands up and moves forward. Outside the ring, the crowd watches the match with excitement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 344, "extra_info": {"duration": 7.0, "events": ["Boxer in red shorts throws a punch.", "Boxer in black shorts ducks.", "Boxer in black shorts rushes into an embrace.", "Boxer in black shorts falls to the ground.", "Referee separates the two fighters.", "Boxer in black shorts stands up.", "Boxer in black shorts moves forward.", "Crowd watches the match."], "n_events": 8, "n_shots": 2, "n_subjects": 4}, "vid": 344}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a large room, a man is watching a boxing match on a laptop while stroking a dog in front of him. Behind the sofa, another man is bent over inspecting something which emits a few flashes of light. The camera shifts to the man behind the sofa, who is smoking. At a farther distance, a woman is sitting on a bed, sewing something with needle and thread.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 345, "extra_info": {"duration": 6.75, "events": ["A man watches a boxing match on a laptop.", "The man strokes a dog.", "Another man inspects something behind the sofa.", "Something emits flashes of light.", "The man behind the sofa smokes.", "A woman sits on a bed.", "The woman sews something with needle and thread."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 345}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a restaurant, in front of the camera, there are three people sitting on either side of a table, with a man on the left, a man and a woman on the right. The waiter comes over, places a dish on the table, and then walks away. The man on the left turns his head to speak to the waiter. The woman across from him uses her finger to wipe her eyes twice, and the man tilts his head. There are other people dining in the restaurant as well.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 346, "extra_info": {"duration": 6.38, "events": ["Waiter places a dish on the table.", "Waiter walks away from the table.", "Man on the left turns head to speak.", "Woman wipes her eyes with her finger twice.", "Man on the left tilts his head."], "n_events": 5, "n_shots": 1, "n_subjects": 4}, "vid": 346}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The door of a prison cell is open, a male officer is holding the door, and a female officer, holding the keys, looks inside the cell. Inside, there is a female prisoner who stands up and walks out of the cell. The camera angle switches to the back of the female prisoner as the female officer grabs the prisoner's arm and walks to the right, while the male officer closes the cell door. The camera then switches to a corridor where the female prisoner is in the middle, flanked by the two officers, as all three walk forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 347, "extra_info": {"duration": 11.88, "events": ["Prison cell door opens", "Male officer holds the door", "Female officer holds keys", "Female officer inspects the cell", "Female prisoner stands up", "Female prisoner walks out of the cell", "Female officer grabs prisoner's arm", "Officers escort prisoner to the right", "Male officer closes the cell door", "Officers walk forward with prisoner"], "n_events": 10, "n_shots": 4, "n_subjects": 3}, "vid": 347}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room adorned with photographs, a man on the left is holding a camera, taking pictures of a woman in red across from him. The camera angle switches to the man's perspective, as the woman looks into the camera lens and gives a slight nod.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 348, "extra_info": {"duration": 5.62, "events": ["A man holds a camera.", "The man takes pictures of the woman.", "The woman looks into the camera lens.", "The woman gives a slight nod."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 348}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman walks in front of the camera, moving from darkness into a lit area, then back into darkness again. Behind her, a car with its headlights on turns away, while the camera follows the woman's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 349, "extra_info": {"duration": 7.0, "events": ["A woman walks in front of the camera.", "The woman moves from darkness into a lit area.", "The woman then returns back into darkness.", "A car with headlights on turns away behind the woman.", "The camera follows the woman's movements."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 349}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the sky, an airplane spirals upward as it flies.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 350, "extra_info": {"duration": 6.38, "events": ["An airplane spirals upward in the sky."], "n_events": 1, "n_shots": 1, "n_subjects": 1}, "vid": 350}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is sitting with his mouth moving slightly. The camera cuts to the man sitting at the living room table, looking at something in front of him, then cuts to the man observing the outside through the window.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 351, "extra_info": {"duration": 20.75, "events": ["The man sits with his mouth moving slightly.", "The man looks at something on the living room table.", "The man observes the outside through the window."], "n_events": 3, "n_shots": 3, "n_subjects": 1}, "vid": 351}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera zooms out, three people stand on different balconies, their bodies moving slightly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 352, "extra_info": {"duration": 11.62, "events": ["The camera zooms out.", "Three people stand on different balconies.", "Bodies move slightly."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 352}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man receives an item with his right hand from another man. The two exchange a glance. The man on the left turns his head for a quick look to the left side and then walks towards the camera. After both men turn around and sit down, they make eye contact again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 353, "extra_info": {"duration": 9.75, "events": ["A man receives an item with his right hand from another man.", "The two men exchange a glance.", "The man on the left turns his head to the left side.", "The man on the left walks towards the camera.", "Both men turn around.", "Both men sit down.", "The two men make eye contact again."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 353}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The food rotates inside the microwave oven, the camera cuts to a man standing in front of a table operating a laptop, the camera then cuts to a close-up of the man's face.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 354, "extra_info": {"duration": 9.25, "events": ["Food rotates inside the microwave oven.", "Man operates a laptop.", "Camera shows a close-up of the man's face."], "n_events": 3, "n_shots": 3, "n_subjects": 1}, "vid": 354}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are looking at the ostrich in front of them. The ostrich moves forward and turns its head to look around. The camera cuts to the man on the left turning his head to look at the man on the right, and they both nod slightly and smile.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 355, "extra_info": {"duration": 11.12, "events": ["Two men look at the ostrich.", "The ostrich moves forward.", "The ostrich turns its head to look around.", "The man on the left turns his head.", "The two men nod slightly.", "The two men smile."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 355}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are sitting on the couch watching TV. The camera cuts to their front, and they each raise one hand, swinging it up and down, as the man on the left turns to look at the man on the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 356, "extra_info": {"duration": 5.62, "events": ["Two men sit on the couch.", "Men raise one hand each.", "Hands swing up and down.", "Man on left turns head.", "Man on left looks at man on right."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 356}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera moves downward, a group of people open a door and walk forward, a dog in front follows them, and then the camera switches to their backs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 357, "extra_info": {"duration": 5.0, "events": ["The camera moves downward", "A group of people open a door", "People walk forward", "A dog follows the group of people"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 357}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person picks a fruit, puts it in their mouth to eat, and the camera pans upward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 358, "extra_info": {"duration": 5.38, "events": ["Person picks a fruit.", "Person puts fruit in mouth.", "Person eats the fruit.", "Camera pans upward."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 358}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A crab is moving beside a person's foot; the camera switches to a long shot, showing the person squatting down and hammering at something. The camera then switches and pans up for a close-up of the person hammering.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 359, "extra_info": {"duration": 8.62, "events": ["A crab moves beside a person's foot", "The person squats down", "The person hammers at something"], "n_events": 3, "n_shots": 3, "n_subjects": 2}, "vid": 359}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "\"The woman blew out the candles, then turned to her side and leaned against something, covering her body with a blanket.\"", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 360, "extra_info": {"duration": 5.0, "events": ["The woman blew out the candles.", "The woman turned to her side.", "The woman leaned against something.", "The woman covered her body with a blanket."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 360}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person folds the paper with both hands, then pours the contents from the paper into a cup, as the camera switches to a close-up of the woman looking down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 361, "extra_info": {"duration": 5.62, "events": ["A person folds the paper with both hands.", "The person pours the contents from the paper into a cup.", "The camera shows the woman looking down."], "n_events": 3, "n_shots": 2, "n_subjects": 1}, "vid": 361}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The lights flicker as the woman walks forward, looking up above. The camera cuts to the light tube bursting and falling off.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 362, "extra_info": {"duration": 3.62, "events": ["The lights flicker.", "The woman walks forward.", "The woman looks up.", "The light tube bursts.", "The light tube falls off."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 362}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man reached out to take the object handed over by the person in front of him, examined it for a bit, then put it in his mouth and bit down, looked at it again, and then raised his head to look forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 363, "extra_info": {"duration": 6.5, "events": ["The man reached out to take the object.", "Person in front of the man handed over the object.", "The man examined the object.", "The man bit down on the object.", "The man looked at the object again.", "The man raised his head to look forward."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 363}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man picks up a metal coin and observes it; the scene switches to a close-up of the metal coin in his hand; it switches again to the man standing up and looking at a tree in front of him; then switches to the man reaching out and placing his hand on the tree.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 364, "extra_info": {"duration": 10.5, "events": ["The man picks up a metal coin.", "The man observes the metal coin.", "The man stands up.", "The man looks at a tree.", "The man reaches out.", "The man places his hand on the tree."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 364}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man points ahead, and the man behind him slightly turns his head to look forward. The camera cuts to the direction of the finger, focusing on a cross pattern on the tree, and zooms in for a close-up shot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 365, "extra_info": {"duration": 5.5, "events": ["The man points ahead.", "The man behind slightly turns his head."], "n_events": 2, "n_shots": 2, "n_subjects": 2}, "vid": 365}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman walks forward, reaches out with her right hand, the camera switches to the woman bumping into a lamp, covering her forehead with both hands, pulling back her left hand, saying something, then switches to another woman sitting and eating, looking up, and switches again to the woman reaching out to snatch snacks from the girl's hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 366, "extra_info": {"duration": 5.75, "events": ["The woman walks forward.", "The woman reaches out with her right hand.", "The woman bumps into a lamp.", "The woman covers her forehead with both hands.", "The woman pulls back her left hand.", "The woman says something.", "Another woman looks up while sitting and eating.", "The woman snatches snacks from the girl's hands."], "n_events": 8, "n_shots": 4, "n_subjects": 3}, "vid": 366}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is running along the highway, glancing to the left of the frame, as the camera follows the movement, a man riding a bicycle approaches from behind.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 367, "extra_info": {"duration": 7.88, "events": ["A woman runs along the highway.", "The woman glances to the left.", "A man on a bicycle approaches from behind."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 367}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman, with her back to the camera, takes off her coat and walks forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 368, "extra_info": {"duration": 4.0, "events": ["A woman takes off her coat.", "The woman walks forward."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 368}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person is wiping another person's knee with a cotton ball.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 369, "extra_info": {"duration": 4.12, "events": ["One person wipes another person's knee.", "A cotton ball is used during the wiping."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 369}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman sits on the steps smoking, turns around, rummages through her bag, and turns on the screen of her mobile phone.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 370, "extra_info": {"duration": 8.25, "events": ["The woman sits on the steps.", "The woman smokes.", "The woman turns around.", "The woman rummages through her bag.", "The woman turns on the mobile phone screen."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 370}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman dances in the center of the stage, the camera switches to the back of the woman; she swings her arms and steps back. The camera then cuts to a man raising his arms, removing his helmet to say something, and the surrounding audience looks towards him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 371, "extra_info": {"duration": 8.25, "events": ["A woman dances in the center of the stage.", "The woman swings her arms.", "The woman steps back.", "A man raises his arms.", "A man removes his helmet.", "A man says something.", "The audience looks towards the man."], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 371}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is hung up, with some people looking at him, others swaying their bodies. The camera cuts to a close-up as another man walks forward, reaches out with both hands, and pulls open the man's clothes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 372, "extra_info": {"duration": 5.75, "events": ["The man is hung up.", "Some people look at the man.", "Others sway their bodies.", "Another man walks forward.", "The man reaches out with both hands.", "The man pulls open the hung man's clothes."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 372}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people are fighting in the water, one man is pushed down, and the camera cuts to this man being held under the water by another person, struggling.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 373, "extra_info": {"duration": 5.75, "events": ["A group of people fight in the water.", "One man pushes another man down.", "Another person holds a man under the water.", "A man struggles underwater."], "n_events": 4, "n_shots": 2, "n_subjects": 3}, "vid": 373}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man stands with his arms raised, looking ahead. The camera switches to a woman holding an umbrella, turning her head to look down to her right. The scene cuts to a firearm, then cuts back to the woman as she turns to her right, reaching sideways to grab something, and then she lifts the gun to aim at the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 374, "extra_info": {"duration": 11.5, "events": ["The man raises his arms.", "The woman holding an umbrella turns her head.", "The woman reaches sideways to grab something.", "The woman lifts the gun to aim at the man."], "n_events": 4, "n_shots": 4, "n_subjects": 2}, "vid": 374}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man turned on the light, the camera switched to the woman gasping deeply. The man walked away, the shot switched to the man walking forward with his back to the camera, then turning around. It switched to the woman, who was panting and said something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 375, "extra_info": {"duration": 9.12, "events": ["The man turned on the light.", "The woman gasped deeply.", "The man walked away.", "The man turned around.", "The woman panted.", "The woman said something."], "n_events": 6, "n_shots": 4, "n_subjects": 2}, "vid": 375}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man walks forward to the bed and sits down, with the camera following his movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 376, "extra_info": {"duration": 5.5, "events": ["The man walks forward.", "The man sits down on the bed."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 376}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman walks down the stairs, with the man following behind. The camera cuts to the woman walking forward, following her movement as she opens the door and leans forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 377, "extra_info": {"duration": 8.12, "events": ["The woman walks down the stairs.", "The man follows the woman.", "The woman opens the door.", "The woman leans forward."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 377}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is holding a guitar with a smile, one hand placed on her chest, while the man beside her is playing the guitar with his head down. The camera shifts to the woman swaying her body as the audience below applauds.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 378, "extra_info": {"duration": 3.62, "events": ["A woman holds a guitar with a smile.", "The woman places one hand on her chest.", "A man plays the guitar beside the woman.", "The woman sways her body.", "The audience below applauds."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 378}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person shifted their chair, reached out to take a plate, and continued walking forward. The seated man turned his head to look at him; the camera angle moved up. The man holding the plate turned his head to look at the seated man, swung his hand, and smashed the plate onto the head of the seated man. The seated man then walked forward with his hands covering his head, and the camera switched to a side view of the standing man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 379, "extra_info": {"duration": 5.5, "events": ["A person shifted their chair.", "A person reached out to take a plate.", "A person continued walking forward.", "The seated man turned his head to look.", "The man holding the plate turned his head to look.", "The man holding the plate swung his hand.", "The man holding the plate smashed the plate onto the seated man's head.", "The seated man walked forward with hands covering his head."], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 379}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The shot looks down as a man opens a lid with both hands and takes out a bottle to examine it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 380, "extra_info": {"duration": 7.0, "events": ["The camera angle points downward.", "A man approaches the lid.", "The man grasps the lid with both hands.", "The man opens the lid.", "The man reaches into the open space.", "The man takes out a bottle.", "The man examines the bottle."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 380}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy and the girl are writing, the girl looks towards the camera, the camera switches to a woman making a phone call, who turns her head to look to the left of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 381, "extra_info": {"duration": 3.5, "events": ["The boy writes.", "The girl writes.", "The girl looks towards the camera.", "The woman makes a phone call.", "The woman turns her head to the left."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 381}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person reaches out to stack and pick up the books from the table, moves forward, and gathers the books from another table. The camera switches to a wide angle as the man turns around with his back to the camera and walks forward, collecting the books from the tables in turn. The camera then switches back to the man retracting his hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 382, "extra_info": {"duration": 11.5, "events": ["A person reaches out to stack books on the table.", "A person picks up books from the table.", "A person moves forward.", "A person gathers books from another table.", "The man turns around with his back to the camera.", "The man walks forward.", "The man collects books from the tables in turn.", "The man retracts his hand."], "n_events": 8, "n_shots": 3, "n_subjects": 1}, "vid": 382}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the forest, a group of people are running forward in succession. The camera switches to a man rowing backwards with both hands, turning his head to look behind him and then to his left. The scene shifts to a woman sitting on a wooden plank, floating on the water, turning her head to look behind her before turning forward again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 383, "extra_info": {"duration": 8.75, "events": ["A group of people runs forward in the forest", "A man rows backwards with both hands", "The man turns his head to look behind", "The man turns his head to his left", "A woman sits on a wooden plank on water", "The woman turns her head to look behind", "The woman turns her head forward"], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 383}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman lay on her side reading a book, sighed, put down the book, reached out to open the box, and rummaged through the things inside.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 384, "extra_info": {"duration": 9.0, "events": ["The woman lay reading a book.", "The woman sighed.", "The woman put down the book.", "The woman reached out.", "The woman opened the box.", "The woman rummaged through the box's contents."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 384}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person stirs an object; the camera cuts to a woman pouring liquid from a bottle, then with the bottle turned upside down, viewed from above, the camera cuts to the woman's profile, cuts to the object being poured into a mold, and then the camera follows the woman as she turns to the right, the object obscuring the woman's face.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 385, "extra_info": {"duration": 13.38, "events": ["A person stirs an object.", "A woman pours liquid from a bottle.", "The bottle is turned upside down.", "Liquid is poured into a mold.", "A woman turns to the right.", "The object obscures the woman's face."], "n_events": 6, "n_shots": 5, "n_subjects": 2}, "vid": 385}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At night, the rocket crashes to the ground and explodes, sending up a burst of flames. The scene switches to daytime, showing a rocket placed on flat ground exploding after falling over.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 386, "extra_info": {"duration": 7.5, "events": ["The rocket crashes to the ground at night.", "The rocket explosion sends up a burst of flames.", "The scene shows a rocket placed on flat ground in daytime.", "The rocket falls over.", "The rocket explodes after falling."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 386}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person turns on the tap, fills a cup with water, and the scene switches to someone cooking a steak with a spatula.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 387, "extra_info": {"duration": 4.25, "events": ["Person turns on the tap", "Person fills a cup with water", "Scene switches", "Someone cooks a steak", "Someone uses a spatula"], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 387}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person covered the man's face with a towel while another held a showerhead, spraying water onto the man's face, as the man struggled in pain.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 388, "extra_info": {"duration": 5.25, "events": ["One person covered the man's face with a towel", "Another person held a showerhead", "Water sprayed onto the man's face", "The man struggled", "The man experienced pain"], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 388}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hand opens the lid of a cardboard box, revealing the hand grenade inside. The camera then cuts to a man in military uniform leaning over, followed by a cut back to the cardboard box he has opened.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 389, "extra_info": {"duration": 5.5, "events": ["A hand opens the lid of a cardboard box.", "The hand grenade is revealed inside the cardboard box.", "A man in military uniform leans over.", "The camera cuts back to the cardboard box."], "n_events": 4, "n_shots": 3, "n_subjects": 1}, "vid": 389}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman took the wine bottle from the man's hand, then poured out all the alcohol inside. The camera pans to a close-up of the pouring process, then cuts back to the man opposite. He shows an expression of helplessness, slightly turning his head, while the woman also turns her head to look at the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 390, "extra_info": {"duration": 5.75, "events": ["The woman took the wine bottle from the man's hand.", "The woman poured out all the alcohol.", "The man showed an expression of helplessness.", "The man slightly turned his head.", "The woman turned her head to look at the man."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 390}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two women stood up one after another, and then embraced each other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 391, "extra_info": {"duration": 6.25, "events": ["The first woman stood up.", "The second woman stood up.", "The two women embraced each other."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 391}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man initially knelt on the mat spread out on the beach, then propped himself up with his hands to stand, and then turned to face the sea.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 392, "extra_info": {"duration": 4.62, "events": ["The man knelt on the mat.", "The man propped himself up with his hands.", "The man stood up.", "The man turned to face the sea."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 392}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman are crouched, searching through the belongings of a person lying on the ground, when a police car approaches from a distance and comes to a stop. The camera cuts to show the man and woman noticing the arrival of the police car. The scene switches again to show two police officers, one male and one female, exiting the vehicle and walking towards them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 393, "extra_info": {"duration": 9.0, "events": ["A man and a woman crouch.", "They search through someone's belongings.", "A person lies on the ground.", "A police car approaches.", "The police car comes to a stop.", "The man and woman notice the police car.", "Two police officers exit the police car.", "One male officer walks towards the man and woman.", "One female officer walks towards the man and woman."], "n_events": 9, "n_shots": 3, "n_subjects": 2}, "vid": 393}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man stretched out his hand to check his watch, then continued walking forward with his hands in his pockets, while a person dressed in white approached from the opposite direction, surrounded by other pedestrians walking by.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 394, "extra_info": {"duration": 5.12, "events": ["The man checked his watch.", "The man continued walking forward.", "A person dressed in white approached.", "Pedestrians walked by."], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 394}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The airplane passes by, the camera follows and pans to the left, revealing a herd of elephants on the grassland.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 395, "extra_info": {"duration": 4.75, "events": ["The airplane passes by.", "The camera follows the airplane.", "The camera pans to the left.", "A herd of elephants appears on the grassland."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 395}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are throwing a basketball to each other, while on their right side, another two men are conversing. As this is happening, two women are walking past them from the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 396, "extra_info": {"duration": 5.88, "events": ["Two men throw a basketball to each other", "Another two men converse", "Two women walk past from the left"], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 396}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is sleeping on his side, and the scene switches to a boy riding a bicycle on the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 397, "extra_info": {"duration": 5.75, "events": ["The man falls asleep.", "The boy rides a bicycle on the road."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 397}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At the beginning, the man and woman are holding hands, then they separate. The man pulls off the dust cover from a vehicle, revealing a Mercedes-Benz. He turns his head to look at the woman, who is standing by watching.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 398, "extra_info": {"duration": 5.38, "events": ["Man and woman hold hands.", "Man and woman separate.", "Man pulls dust cover off vehicle.", "Vehicle is revealed as a Mercedes-Benz.", "Man turns head to look at woman.", "Woman stands by watching."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 398}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man falls backwards, and the camera switches to a woman standing by the water, watching as water splashes around. The people nearby look towards the man who has fallen into the water. The camera then switches to a man and a woman embracing each other in a garden, turning their heads to look to the left of the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 399, "extra_info": {"duration": 2.5, "events": ["The man falls backwards into the water.", "Water splashes around from the impact.", "People nearby turn their heads towards the man in the water.", "A man and a woman embrace each other in a garden.", "The man and woman turn their heads to the left."], "n_events": 5, "n_shots": 3, "n_subjects": 3}, "vid": 399}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man took off his glasses, looked into the distance, and blinked.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/movie_live_action", "task": "video/caption", "idx": 400, "extra_info": {"duration": 4.25, "events": ["The man took off his glasses.", "The man looked into the distance.", "The man blinked."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 400}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of a white wall, a gentleman wearing a hooded sweatshirt and a lady in a short skirt danced together to an upbeat tune.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 401, "extra_info": {"duration": 15.18, "events": ["The gentleman donned a hooded sweatshirt.", "The lady chose a short skirt.", "They paired up.", "The upbeat tune started.", "The gentleman and the lady danced.", "Their feet shuffled to the rhythm.", "They spun together.", "Their hands clapped.", "They twirled individually.", "They synchronized their movements."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 401}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera focuses on a woman's profile, with one hand displaying a bottle of makeup in front of the lens. Then another hand taps the bottle with a small brush, after which foundation is applied to the lady's forehead. The camera switches to a side view, continuing to apply makeup to the lady's face.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 402, "extra_info": {"duration": 15.03, "events": ["The camera focuses on a woman's profile.", "A hand displays a bottle of makeup in front of the lens.", "Another hand taps the bottle with a small brush.", "Foundation is applied to the woman's forehead.", "Makeup is applied to the woman's face."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 402}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a lady dressed in black is taking a selfie in the park. The first shot shows her turning around to face the camera, putting on sunglasses. The second shot captures her walking gracefully towards the camera. In the third shot, she opens her arms and runs away from the camera. The fourth shot shows her waving trekking poles as she walks forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 403, "extra_info": {"duration": 9.57, "events": ["The lady turns to face the camera.", "The lady puts on sunglasses.", "The lady walks towards the camera.", "The lady opens her arms.", "The lady runs away from the camera.", "The lady waves trekking poles."], "n_events": 6, "n_shots": 4, "n_subjects": 1}, "vid": 403}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a tree-lined path, clusters of boys and girls carrying backpacks walked along, when a boy ran into the middle of the road and broke into a street dance. Some turned their heads to watch him dance, while others continued on their way.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 404, "extra_info": {"duration": 10.47, "events": ["Boys and girls walked along a tree-lined path.", "A boy ran into the middle of the road.", "The boy broke into a street dance.", "Some boys and girls turned their heads to watch.", "Others continued on their way."], "n_events": 5, "n_shots": 1, "n_subjects": 2, "old_duration": 10.47, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/4.mp4"}, "vid": 404}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Within a garden, a girl opens her arms and falls backward. The camera then switches to a bungee jumping platform, where a staff member gently pushes the girl in front. Like before, she opens her arms and falls backward. The girl repeatedly falls through the air and then gets pulled back up by the rope, with the camera following her movements up and down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 405, "extra_info": {"duration": 9.43, "events": ["A girl opens her arms in a garden.", "The girl falls backward.", "A bungee jumping platform appears.", "A staff member pushes the girl gently.", "The girl opens her arms on the platform.", "The girl falls backward off the platform.", "The girl repeatedly falls through the air.", "A rope pulls the girl back up.", "The camera follows the girl's movements.", "The girl moves up and down."], "n_events": 10, "n_shots": 2, "n_subjects": 3}, "vid": 405}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At the foot of a mountain, a man dressed in an orange storm jacket waves a hiking stick. The camera then switches to another man at the base of a different mountain; this one is wearing a blue storm jacket and a hat, also holding a hiking stick and posing. At the end of the video, the man in the blue storm jacket reaches out to grab the cellphone that he is being filmed with. The backgrounds of both shots feature blue skies, white clouds, and distant mountain peaks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 406, "extra_info": {"duration": 8.1, "events": ["Man in orange jacket waves hiking stick.", "Camera switches to another man.", "Man in blue jacket poses with hiking stick.", "Man in blue jacket reaches out to grab cellphone."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 406}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A gentleman is participating in an obstacle course game. He agilely jumps over two moving barriers, and in an attempt to dodge the third obstacle, his leap is slightly misaligned, causing his feet to miss the platform. However, he quickly braces himself with his arms against the platform and effortlessly climbs up, then proceeds to smoothly navigate through the subsequent obstacles, easily completing the challenge. There are captions at the top of the screen expressing admiration for the contestant's rapid passage through the course. The background is that of a variety show channel from a television station.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 407, "extra_info": {"duration": 14.7, "events": ["Gentleman jumps over two moving barriers.", "Gentleman attempts to dodge the third obstacle.", "Gentleman's leap misaligns causing a missed platform landing.", "Gentleman braces himself with his arms against the platform.", "Gentleman climbs up onto the platform.", "Gentleman navigates through subsequent obstacles.", "Gentleman completes the challenge."], "n_events": 7, "n_shots": 1, "n_subjects": 2, "old_duration": 20.5, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/7_no_audio.mp4"}, "vid": 407}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A tall and handsome man yawned and feigned surprise as he looked towards the camera. Then he gestured with his hands, stomped his foot and his pants changed from grey casual wear into black formal attire. Pretending to have come to a realization, he turned his head to complete a facial beauty touch-up. Afterwards, feigning helplessness, he took a couple of steps forward and changed into a fashionable jacket. Turning his head once more, he put on a pair of sunglasses.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 408, "extra_info": {"duration": 12.5, "events": ["Man yawned and feigned surprise.", "Man gestured with hands.", "Man stomped foot and pants changed to black formal attire.", "Man turned head for facial touch-up.", "Man feigned helplessness.", "Man took a couple of steps forward.", "Man changed into a fashionable jacket.", "Man turned head again.", "Man put on a pair of sunglasses."], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 408}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady is strutting in an empty parking lot, showing off black clothing and a white backpack; the camera switches from her back to her front, and then again to her side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 409, "extra_info": {"duration": 9.17, "events": ["A lady struts in an empty parking lot.", "Lady shows off black clothing.", "Lady displays white backpack."], "n_events": 3, "n_shots": 3, "n_subjects": 1}, "vid": 409}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a patch of grass, a handsome young man dressed in a brown suit stands sideways to the camera, tossing fresh flowers into the air with his hand and catching them again. The scene switches to show the young man walking away from the camera with flowers in his hand behind his back, then he turns around and looks back at the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 410, "extra_info": {"duration": 8.7, "events": ["Young man tosses flowers into the air.", "Flowers are caught by young man.", "Young man walks with flowers behind back.", "Young man turns around.", "Young man looks at the camera."], "n_events": 5, "n_shots": 2, "n_subjects": 1}, "vid": 410}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman sits on the suitcase moving towards the camera, then stands up and starts to dance. The scene shifts from the waiting hall to different locations, including a street with residential buildings in the background, a pedestrian walkway with an overpass in the background, the entrance of a mall, and the entrance to a scenic spot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 411, "extra_info": {"duration": 15.2, "events": ["The woman sits on the suitcase.", "The suitcase moves towards the camera.", "The woman stands up.", "The woman starts to dance.", "The scene shifts to a street.", "The scene shifts to a pedestrian walkway.", "The scene shifts to the entrance of a mall.", "The scene shifts to the entrance to a scenic spot."], "n_events": 8, "n_shots": 5, "n_subjects": 1}, "vid": 411}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person carrying a bag opens the door, and the camera cuts to this person opening a freezer, pouring the ice cream from the bag into it, and then closing the freezer door after a simple rearrangement. The camera switches to the person moving the keyboard forward, then placing a plate of ice cream on the computer desk.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 412, "extra_info": {"duration": 12.6, "events": ["Person opens the door", "Person opens the freezer", "Person pours ice cream into the freezer", "Person rearranges items in the freezer", "Person closes the freezer door", "Person moves the keyboard forward", "Person places a plate of ice cream on the desk"], "n_events": 7, "n_shots": 5, "n_subjects": 1}, "vid": 412}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman dressed in a white tank top is chopping lettuce, then she adds three raw eggs into the bowl, along with black pepper and sea salt, and then begins to stir with chopsticks. The camera angle remains unchanged. The background is a bright living room.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 413, "extra_info": {"duration": 7.3, "events": ["A woman chops lettuce.", "The woman adds three raw eggs into the bowl.", "The woman sprinkles black pepper into the bowl.", "The woman shakes sea salt into the bowl.", "The woman stirs the mixture with chopsticks."], "n_events": 5, "n_shots": 1, "n_subjects": 1, "old_duration": 10.43, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/13_no_audio.mp4"}, "vid": 413}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man dressed in red clothes puts the cow's head into the pot, sprinkles in salt and chicken essence. A chicken walks by in the background. He then adds soy sauce, gestures with his finger, says something to the camera, and then covers the pot with a lid. The background is a desolate snowy mountain. The camera angle remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 414, "extra_info": {"duration": 11.5, "events": ["Man puts cow's head into pot", "Man sprinkles salt into pot", "Man adds chicken essence", "Chicken walks by", "Man pours soy sauce into pot", "Man gestures to camera", "Man speaks to camera", "Man covers pot with lid"], "n_events": 8, "n_shots": 1, "n_subjects": 1, "old_duration": 15.53, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/14_no_audio.mp4"}, "vid": 414}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an expanse of sea where a person with a net is scooping up marine life in the water. The camera then cuts to three individuals standing at the side of a boat, pulling the creatures aboard. The process is filmed up close, and afterwards, they flip the marine life out and onto the deck of the ship.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 415, "extra_info": {"duration": 15.07, "events": ["A person scoops up marine life with a net.", "Three individuals pull creatures aboard the boat.", "They flip marine life onto the deck of the ship."], "n_events": 3, "n_shots": 4, "n_subjects": 3, "old_duration": 18.5, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/15_no_audio.mp4"}, "vid": 415}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman, holding a sickle, spins in a circle. The camera cuts to the woman as she swings her hand and slices open a bag, from which corn kernels fall out. The shot then switches to show the front legs of a pig standing up at the edge of its pen. The camera zooms in as the pig's mouth quivers slightly. The woman pours the corn kernels into a container in front of her. As the pig's body stirs, the woman, holding a small white scoop, gently pats the pig's head. The pig backs away. The woman is wearing brown boots and uses her foot to kick the corn kernels on the ground into the container. A white English subtitle appears in the middle of the frame while the woman, with her hands on her hips, walks towards the camera. The camera switches between these scenes, all set against the backdrop of a pigsty.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 416, "extra_info": {"duration": 10.3, "events": ["Woman spins holding a sickle.", "Woman slices open a bag with sickle.", "Corn kernels fall out of the bag.", "Pig’s mouth quivers slightly.", "Woman pours corn kernels into a container.", "Pig stirs in its pen.", "Woman pats pig's head with a scoop.", "Pig backs away from the woman.", "Woman kicks corn kernels into the container.", "Woman walks towards the camera."], "n_events": 10, "n_shots": 8, "n_subjects": 2}, "vid": 416}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A child in a blue and black school uniform is holding an umbrella. The camera pans to the right, and a child slides down the stairs. The camera switches to a side view as the child stands on the stairs and walks forward, then suddenly falls backwards and slides down the stairs. The people around look at him. After standing up, the child walks toward the steps. The background is an outdoor scene in the rain.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 417, "extra_info": {"duration": 15.03, "events": ["A child holds an umbrella.", "A child slides down the stairs.", "The child stands on the stairs.", "The child walks forward.", "The child falls backwards.", "The child slides down the stairs.", "People around look at the child.", "The child stands up.", "The child walks toward the steps.", "It rains in the background."], "n_events": 10, "n_shots": 2, "n_subjects": 3}, "vid": 417}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an outdoor fitness area surrounded by trees. A man wearing a brown hoodie leaps forward, grabs onto a bar and swings, flipping once in the air, then grabs the bar to do two spins, he hops over the bar and continues with another flip, then does two more flips in the air, grabs the bar again and keeps swinging, with onlookers watching him. The camera follows the man's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 418, "extra_info": {"duration": 15.05, "events": ["Man in brown hoodie leaps forward", "Man grabs onto a bar", "Man swings, flipping once in the air", "Man grabs the bar to do two spins", "Man hops over the bar", "Man performs another flip", "Man does two more flips in the air", "Man grabs the bar again", "Man keeps swinging", "Onlookers watch the man"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 418}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The person dressed in a green mascot suit sat in a chair and put down the helmet, then turned to look at the person next to them dressed in a brown mascot suit. They were startled and sat up straight, poking at them with an object. The person in the brown mascot suit fell backwards to the ground and then stood up. The person in the green mascot suit picked up the mascot's helmet, turned around, and ran towards the person dressed in brown.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 419, "extra_info": {"duration": 8.15, "events": ["The person in the green mascot suit sat in a chair.", "The person in the green mascot suit put down the helmet.", "The person in the green mascot suit turned to look at the companion.", "The person in the green mascot suit was startled.", "The person in the green mascot suit sat up straight.", "The person in the green mascot suit poked the companion with an object.", "The person in the brown mascot suit fell backwards to the ground.", "The person in the brown mascot suit stood up.", "The person in the green mascot suit picked up the mascot helmet.", "The person in the green mascot suit ran towards the person in brown."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 419}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little pig moves to open the wrapper of an ice cream, then pulls it back and moves toward the camera, eating the ice cream as it starts to melt. The pig then takes away the ice cream stick in its mouth and continues to eat. The camera remains fixed throughout the scene. The background shows a white tabletop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 420, "extra_info": {"duration": 15.03, "events": ["Little pig opens ice cream wrapper", "Little pig pulls back", "Little pig moves toward camera", "Little pig eats melting ice cream", "Ice cream starts to melt", "Little pig takes ice cream stick in mouth", "Little pig continues to eat"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 420}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a target range devoid of vegetation, a warrior presses bullets into a magazine on the ground, then walks up to a rifle, inserts the magazine into the rifle, followed by getting down, pulling the bolt, aiming, and firing. The scene switches to the side of the warrior, where a row of warriors is firing with the same weapons and stance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 421, "extra_info": {"duration": 15.2, "events": ["Warrior presses bullets into a magazine", "Warrior walks up to a rifle", "Warrior inserts the magazine into the rifle", "Warrior gets down", "Warrior pulls the bolt", "Warrior aims", "Warrior fires", "A row of warriors fires weapons", "Warriors adopt the same stance"], "n_events": 9, "n_shots": 4, "n_subjects": 2}, "vid": 421}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a corridor, a person is holding a pot of chrysanthemum water with one hand, preparing to water the plants. However, a cat is blocking the path ahead. Although the person stops in their tracks, the chrysanthemum water still spills onto the cat's head. The camera then cuts to the upstairs, where the same person is once again holding a pot of chrysanthemum water, preparing to descend the stairs. The video has subtitles that accompany the actions of the character.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 422, "extra_info": {"duration": 7.08, "events": ["Person holds pot of chrysanthemum water.", "Person prepares to water plants.", "Cat blocks the person's path.", "Person stops walking.", "Chrysanthemum water spills onto cat's head.", "Person holds pot of chrysanthemum water upstairs.", "Person prepares to descend the stairs."], "n_events": 7, "n_shots": 6, "n_subjects": 2}, "vid": 422}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A gentleman pours a basket of tea leaves into a large iron pot, then stirs them exaggeratedly, followed by pouring a large basin of water. The video includes subtitles that match the character's actions.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 423, "extra_info": {"duration": 9.95, "events": ["A gentleman pours tea leaves into a large iron pot.", "The gentleman stirs the tea leaves exaggeratedly.", "The gentleman pours a large basin of water."], "n_events": 3, "n_shots": 5, "n_subjects": 1}, "vid": 423}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A male diver was performing a springboard dive when he made a mistake and landed flat on the water's surface.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 424, "extra_info": {"duration": 4.27, "events": ["A male diver starts a springboard dive.", "The male diver performs somersaults in the air.", "The male diver makes a mistake during the dive.", "The male diver attempts to regain form.", "The male diver fails to recover posture.", "The male diver lands flat on the water's surface."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 424}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a beach, a swing hangs between two palm trees, with a lady sitting on it. Behind the lady, a man pushes her to start swinging quickly then moves out of the camera frame. Taking advantage of the moment when the lady swings backward, another man who had already been waiting in front heads towards her, running with his phone to capture the moment on the swing. As the lady reaches the peak of her swing and begins to move back, the man passes underneath and continues to film.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 425, "extra_info": {"duration": 10.8, "events": ["Swing hangs between two palm trees.", "Lady sits on the swing.", "Man pushes the lady to swing quickly.", "Pushing man moves out of the camera frame.", "Another man runs with his phone towards the swing.", "Lady reaches the peak of her swing.", "Lady begins to move back.", "Running man passes underneath the swing.", "Running man continues to film."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 425}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a basketball court, a team wearing black jerseys is playing a basketball game against various other teams. One of the guards, using skilled techniques of dribbling, penetrating, and passing, helps his team score goals frequently.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 426, "extra_info": {"duration": 15.05, "events": ["Team in black jerseys plays basketball game.", "Guard dribbles the ball.", "Guard penetrates the defense.", "Guard passes the ball.", "Team scores a goal."], "n_events": 5, "n_shots": 4, "n_subjects": 2}, "vid": 426}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a mountain road, several monkeys are attacking a group of tourists. Among them, a woman is scared into screaming and running away, and a man is also frightened by the monkeys and runs down the mountain. Eventually, after descending a step, the man turns around to face the monkey. The monkey dodges to the right, and the man supports himself with a tree branch. The dodging monkey, along with another one, comes back and once again makes an aggressive gesture towards the tourists.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 427, "extra_info": {"duration": 10.5, "events": ["Monkeys attack a group of tourists on a mountain road.", "A woman screams and runs away.", "A man runs down the mountain.", "The man descends a step.", "The man turns around to face the monkey.", "The monkey dodges to the right.", "The man supports himself with a tree branch.", "The dodging monkey comes back.", "Another monkey comes back.", "Monkeys make an aggressive gesture towards the tourists."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 427}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases four scenes. The first scene is in a spacious indoor area, where two people are cleaning. A person lying on the floor gets up and grabs a cleaning caution sign to hit one of the cleaners. The second scene is inside a subway, where someone suddenly snatches a piece of pizza from the hands of a seated person, then runs off the subway. The third scene features a person wearing a turban, who kisses the head of a seated man, who turns around in surprise to look at him. The fourth scene depicts a person with dreadlocks, reaching through a subway window from outside to snatch the hat off a person inside.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 428, "extra_info": {"duration": 15.08, "events": ["A person gets up from the floor.", "The person grabs a cleaning caution sign.", "The person hits a cleaner with the sign.", "Someone snatches a piece of pizza in a subway.", "The snatcher runs off the subway.", "A person wearing a turban kisses a seated man's head.", "The seated man turns around.", "A person with dreadlocks reaches through a subway window.", "The person with dreadlocks snatches a hat.", "The person inside the subway loses their hat."], "n_events": 10, "n_shots": 4, "n_subjects": 6}, "vid": 428}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady in a blue dress picked up a pillow with a portrait printed on it, claiming to perform a magic trick to turn it into a living person. She threw the pillow upwards, and when the pillow fell back down, her arms were holding another lady. The lady in the blue dress hugged the other woman and turned around with her, then set her down, and both of them looked back together. The video is accompanied by subtitles that match the actions of the two people.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 429, "extra_info": {"duration": 11.8, "events": ["Lady picked up a pillow with a portrait.", "Lady claimed to perform a magic trick.", "Lady threw the pillow upwards.", "Pillow fell back down.", "Lady's arms were holding another lady.", "Lady in blue dress hugged the other woman.", "Lady in blue dress turned around with the other woman.", "Lady in blue dress set the other woman down.", "Both women looked back together.", "Video subtitles matched the actions."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 429}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady wearing gloves is riding on a pelican. She forcefully pries open the pelican's beak and pours a little duck out of the pelican's mouth. Then, she steps over the pelican, pinches its beak, and carries it away. During the process, a little girl is watching the whole time by her side, and there are other people walking around nearby.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 430, "extra_info": {"duration": 15.13, "events": ["Lady rides on a pelican.", "Lady pries open the pelican's beak.", "Lady pours a little duck out of the beak.", "Lady steps over the pelican.", "Lady pinches the pelican's beak.", "Lady carries the pelican away.", "Little girl watches the process.", "People walk around nearby."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 430}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Background: Disneyland. A woman carrying a black backpack walks forward, greets someone in a fox costume, then approaches someone in a rabbit costume and hugs her. During the hug, the woman's hat falls off. The fox character points to the hat on the ground, and the woman squats down to pick up the hat and puts it back on her head. Afterwards, the woman and the fox character hug, but during the process, the hat falls off again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 431, "extra_info": {"duration": 15.2, "events": ["A woman greets someone in a fox costume.", "The woman approaches someone in a rabbit costume.", "The woman hugs the person in the rabbit costume.", "The woman's hat falls off during the hug.", "The character in the fox costume points to the fallen hat.", "The woman squats down to pick up the hat.", "The woman puts the hat back on her head.", "The woman and the character in the fox costume hug.", "The woman's hat falls off again during the hug."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 431}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a bookshelf. The camera moves to the right, as a woman wearing headphones looks upward, picks up a flashlight from the table, and turns it on. The flashlight is shining toward the camera, then the scene switches to show the woman holding the flashlight while walking down a smoke-filled corridor, ending with images of several pigs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 432, "extra_info": {"duration": 8.0, "events": ["The camera moves to the right", "A woman looks upward", "The woman picks up a flashlight", "The woman turns on the flashlight", "The flashlight shines toward the camera", "The woman walks down a smoke-filled corridor", "Images of several pigs appear"], "n_events": 7, "n_shots": 3, "n_subjects": 2, "old_duration": 10.03, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/32_no_audio.mp4"}, "vid": 432}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is holding a basket of mangroves, washing shells in the creek. He picks up the basket, and the camera cuts to him placing the basket on a table, picking up a shell and putting it on a wooden board. Standing on a rope, he presses down to secure the shell and saws it into long strips. The camera captures the man sawing the shell from different angles, then switches to a frontal view of the man as he pulls the saw vigorously. Afterward, it switches to him picking up one of the long strips and sawing it into a square shape.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 433, "extra_info": {"duration": 15.03, "events": ["Person holds basket of mangroves.", "Person washes shells in creek.", "Person picks up the basket.", "Person places basket on table.", "Person picks up a shell.", "Person puts shell on wooden board.", "Person stands on rope.", "Person secures shell with rope.", "Person saws shell into long strips.", "Person cuts long strip into square shape."], "n_events": 10, "n_shots": 6, "n_subjects": 1}, "vid": 433}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is lying on the ground, using his hands to scrape away the black sand. His left hand moves up and down, and a yellow creature emerges, which the man then picks up with both hands. The background is a green grassy field. The camera follows the man's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 434, "extra_info": {"duration": 11.8, "events": ["The man lies on the ground.", "The man uses his hands to scrape away black sand.", "The man's left hand moves up and down.", "A yellow creature emerges from the sand.", "The man picks up the yellow creature with both hands."], "n_events": 5, "n_shots": 1, "n_subjects": 1, "old_duration": 11.8, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/34.mp4"}, "vid": 434}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a forest. The woman reaches out to scoop out the white viscous material from the bowl and places it into the green basket behind her. The camera zooms in on the object as it falls into the basket. The woman bends down to pick up another piece and continues to place it into the basket. The camera then switches to the woman tearing the white viscous material off the tree, following the movement with the shot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 435, "extra_info": {"duration": 15.08, "events": ["The woman scoops out the white viscous material.", "The woman places the material into the green basket.", "The material falls into the basket.", "The woman bends down to pick up another piece.", "The woman continues to place the material into the basket.", "The woman tears the white viscous material off the tree."], "n_events": 6, "n_shots": 6, "n_subjects": 1}, "vid": 435}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angles down and captures a boat moving right towards the shore. People on the boat move back and forth. A truck cab drives onto the boat, and other large trucks follow, driving off the vessel one by one. One large truck is making a U-turn. The camera holds its position steady.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 436, "extra_info": {"duration": 15.03, "events": ["Boat moves towards the shore", "People move back and forth on the boat", "Truck cab drives onto the boat", "Large trucks follow onto the boat", "Trucks drive off the boat one by one", "One truck makes a U-turn"], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 436}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little boy is sitting at the dining table, scooping food into another boy's bowl with a spoon held by his foot. The other boy says something, and extends his chopsticks with his right hand. The little boy then switches the spoon from one foot to the other, and using his feet, he grabs the chopsticks attempting to pick up the green beans from the bowl. The camera follows the movement of the little boy's feet. The background is a living room.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 437, "extra_info": {"duration": 15.08, "events": ["A little boy sits at the dining table.", "The little boy scoops food into another boy's bowl with foot.", "Another boy speaks.", "Another boy extends chopsticks with right hand.", "The little boy switches the spoon from one foot to the other.", "Using feet, the little boy grabs the chopsticks.", "The little boy attempts to pick up green beans with chopsticks."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 437}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Fingers press the button on the table, and a little dog pops its head out from the center of the table. When a finger is placed on the dog's nose, the little dog spits a dice out from its mouth, and the two people begin to draw mahjong tiles. The scene shifts to the person opposite knocking down the mahjong tiles; white subtitles appear then disappear. The little dog lowers its head, and white subtitles appear again in the scene as the two people tidy up the mahjong tiles.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 438, "extra_info": {"duration": 11.1, "events": ["Fingers press the button on the table", "A little dog pops its head out from the table", "A finger is placed on the little dog's nose", "The little dog spits a dice out from its mouth", "Two people begin to draw mahjong tiles", "Opposite person knocks down the mahjong tiles", "White subtitles appear then disappear", "The little dog lowers its head", "White subtitles appear again", "Two people tidy up the mahjong tiles"], "n_events": 10, "n_shots": 2, "n_subjects": 3}, "vid": 438}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman pushes the door forcefully, then looks up. She picks up an iron shovel to prop against the eaves, pushes the door back with her right foot, turns around to put down the shovel, and the camera cuts to the side, showing the woman sweeping the floor with a broom. The scene switches to the woman washing yellow food in a bowl under a tap, then cuts again to the woman scattering the yellow food on the ground, lifting up a wooden lid, and a flock of chickens run out to eat the food.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 439, "extra_info": {"duration": 15.03, "events": ["The woman pushes the door forcefully.", "The woman looks up.", "The woman picks up an iron shovel.", "The woman props shovel against the eaves.", "The woman pushes the door with her right foot.", "The woman puts down the shovel.", "The woman sweeps the floor with a broom.", "The woman washes yellow food in a bowl.", "The woman scatters the yellow food on the ground.", "The chickens run out to eat the food."], "n_events": 10, "n_shots": 3, "n_subjects": 2}, "vid": 439}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures from various angles as a woman swings a hammer to strike bamboo strips. A yellow dog and a black dog run by her side. The shot switches to a frontal view of the woman, then to an overhead shot, with a close-up of the bamboo being tightly bound by ropes. The woman steps on the bamboo with her right foot, then with her left foot, she pulls up the ropes. She hoists the bamboo strips and walks forward, the two dogs following her. The camera then switches to show the woman stopping in front of a pile of bamboo strips. She drops the strips and jumps into a pit, turning around to grab a basket. The background is a forested mountain landscape.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 440, "extra_info": {"duration": 15.03, "events": ["A woman swings a hammer to strike bamboo strips.", "A yellow dog and a black dog run by the woman's side.", "The woman steps on the bamboo with her right foot.", "The woman pulls up the ropes with her left foot.", "The woman hoists the bamboo strips and walks forward.", "The two dogs follow the woman.", "The woman stops in front of a pile of bamboo strips.", "The woman drops the bamboo strips.", "The woman jumps into a pit.", "The woman grabs a basket."], "n_events": 10, "n_shots": 9, "n_subjects": 3, "old_duration": 15.03, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/40.mp4"}, "vid": 440}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The yellow text on the screen disappears, and the woman demonstrates the technique of quickly peeling bamboo shoots with a sickle. She first cuts the shoot from top to bottom, then peels back the skin to reveal the middle part of the shoot, chops off the top of the shoot, continues to use a hoe to dig around the base of the shoot, and then sits down to scrape and chop off the excess parts of the bamboo shoot with the sickle. The camera remains fixed. The background is a forest.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 441, "extra_info": {"duration": 15.08, "events": ["Yellow text disappears from the screen", "Woman cuts bamboo shoot from top to bottom", "Woman peels back skin of bamboo shoot", "Woman chops off top of bamboo shoot", "Woman uses hoe to dig around bamboo shoot base", "Woman sits down", "Woman scrapes bamboo shoot with sickle", "Woman chops off excess parts of bamboo shoot"], "n_events": 8, "n_shots": 4, "n_subjects": 1}, "vid": 441}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a field. The camera remains fixed. A person holds a knife with both hands, moves forward, bends over, and slices open a watermelon on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 442, "extra_info": {"duration": 7.83, "events": ["A person holds a knife with both hands.", "The person moves forward.", "The person bends over.", "The person slices open a watermelon on the ground."], "n_events": 4, "n_shots": 1, "n_subjects": 1, "old_duration": 7.83, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/42.mp4"}, "vid": 442}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera zooms in for a close-up on a person wearing brown gloves, drawing a straight line on marble with a pencil. The camera then pulls back, capturing the man from the front, with his back to the camera, using tools as he coordinates with both hands to repeatedly cut away the top layer of the stone. The camera zooms in again to show the details of the cut, then reveals a smooth, flat surface. The background features a room with a brown cabinet.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 443, "extra_info": {"duration": 12.63, "events": ["Person wearing brown gloves draws a line on marble.", "Man uses tools to cut away the top layer of stone.", "Camera reveals a smooth, flat surface.", "Background features a room with a brown cabinet."], "n_events": 4, "n_shots": 7, "n_subjects": 1}, "vid": 443}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The shot is taken from behind as the large dog stands up, opens the glass door, and runs out, only to get frightened and suddenly turn back. After opening the door, it takes refuge behind a table, sits down, and looks straight ahead while the camera follows the movement. The background is an office with a computer desk.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 444, "extra_info": {"duration": 8.53, "events": ["The large dog stands up", "The large dog opens the glass door", "The large dog runs out", "The large dog gets frightened", "The large dog turns back", "The large dog takes refuge behind a table", "The large dog sits down", "The large dog looks straight ahead", "The camera follows the large dog's movement"], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 444}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The person on the left pours a black liquid, the person in the middle catches it with both hands and then splashes it onto their cheeks, while the person on the right scatters powder. The individual on the left scoops up a bowl of liquid and pours it over the head of the person in the middle. A man dressed in white clothes approaches from the left and, together with the person on the right, sprinkles powder on the person in the middle. The background features yellow house buildings. The camera angle remains fixed and unchanged.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 445, "extra_info": {"duration": 14.83, "events": ["Person on the left pours black liquid.", "Person in the middle catches black liquid with hands.", "Person in the middle splashes black liquid onto cheeks.", "Person on the right scatters powder.", "Person on the left scoops up bowl of liquid.", "Person on the left pours liquid over person in the middle's head.", "Man in white clothes approaches from the left.", "Man in white and person on the right sprinkle powder on person in the middle."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 445}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a farm, a strange creature covered in a whole cowhide charged at a lady dressed in a long robe with its horns. Frightened by the encounter, the lady ran inside the house. A man inside, armed with a wooden stick, came out. The odd creature then stood upright, lifted the cowhide, revealing that it was actually two people playing a prank. The man with the stick chased after them, and as they threw away the cowhide, they fled.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 446, "extra_info": {"duration": 4.0, "events": ["Creature covered in cowhide charged at lady.", "Lady ran inside the house.", "Man armed with stick came out of house.", "Creature stood upright.", "Creature lifted the cowhide.", "People inside creature revealed as pranksters.", "Man with stick chased the pranksters.", "Pranksters threw away the cowhide.", "Pranksters fled."], "n_events": 9, "n_shots": 1, "n_subjects": 4}, "vid": 446}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two boys watched three basins filled with rice and flour, as well as a person holding a colander. The person scooped up some rice from one basin, and the rice fell through the colander back into the basin. Then, they used the colander to scoop up some millet from another basin, and the millet too fell through back into the basin. Lastly, the person used the colander to scoop up some flour from the final basin, but the flour did not fall through. Accompanying the person's actions, a voice in the video asks why the finest flour does not fall through. The background is an ordinary residential room.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 447, "extra_info": {"duration": 8.63, "events": ["Two boys watched three basins.", "A person scooped rice with a colander.", "Rice fell through the colander.", "The person scooped millet with the colander.", "Millet fell through the colander.", "The person scooped flour with the colander.", "Flour did not fall through the colander.", "A voice asked why flour did not fall through."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 447}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At the entrance of a room, a little boy placed a bundle of plants on the ground, then flipped up the hem of his jacket, shaking out a lot of elm seeds from his clothes. The boy then reached into his pants and pulled out more elm seeds. Following this, he shook his pant legs, and even more elm seeds fell out from there. Lastly, the boy picked up the bundle of plants that was at the doorway and set it down in front of the scattered elm seeds all over the floor. In the background, there was the sound of singing and a woman's laughter.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 448, "extra_info": {"duration": 12.13, "events": ["A little boy placed a bundle of plants on the ground.", "The little boy flipped up the hem of his jacket.", "Elm seeds shook out from the little boy's jacket.", "The little boy pulled out more elm seeds from his pants.", "The little boy shook his pant legs.", "More elm seeds fell out from the little boy's pant legs.", "The little boy picked up the bundle of plants.", "The little boy set the bundle down in front of scattered seeds.", "Sound of singing is audible in the background.", "A woman's laughter is heard in the background."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 448}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an open area, a man picked up a flatbread with both hands, threw it up into the air, and the flatbread flew in a circle before returning to the man's hands. Then the man took the flatbread and spun around to play with it behind his back, and finally threw the bread into the air once again. In the background, there were sounds of a boy exclaiming in disbelief.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 449, "extra_info": {"duration": 15.08, "events": ["Man picked up a flatbread with both hands.", "Man threw flatbread up into the air.", "Flatbread flew in a circle.", "Flatbread returned to man's hands.", "Man spun around with flatbread.", "Man played with flatbread behind his back.", "Man threw flatbread into the air again.", "Boy exclaimed in disbelief."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 449}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a waterlogged road, a man carrying a pair of shoes in his hand rolled up his trouser legs, walking barefoot through the water towards the other side of the street. Just as he was about to reach the curb, he leaped forward, jumping onto the curb, but immediately slipped and fell, ending up sitting on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 450, "extra_info": {"duration": 10.8, "events": ["A man carried shoes in his hand.", "The man rolled up his trouser legs.", "The man walked barefoot through water.", "The man approached the other side.", "The man leaped toward the curb.", "The man slipped on the curb.", "The man fell onto the ground.", "The man ended up sitting."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 450}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a road submerged in puddle water, a man, holding a pair of shoes in his hand, rolled up his trousers, and with bare feet, waded through the water towards the opposite side of the street. Just as he was about to reach the curb, he leapt forward, jumping onto the curb, but immediately slipped and fell, ending up sitting on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 451, "extra_info": {"duration": 10.8, "events": ["A man held a pair of shoes in his hand.", "The man rolled up his trousers.", "The man waded through puddle water with bare feet.", "The man walked towards the opposite side of the street.", "The man leapt onto the curb.", "The man immediately slipped.", "The man fell to the ground.", "The man ended up sitting on the ground."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 451}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A girl with both arms covered in sludge, lifts a bucket by the handle with her left hand and tilts it by the bottom with her right, tipping out a bucket of crawfish still smeared with mud. The camera pans right to a little boy, his body daubed in sludge, who brings over a pair of trousers caked with mud, from which he pours yet another heap of mud-covered crawfish. The backdrop is the entrance of a rural house.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 452, "extra_info": {"duration": 8.87, "events": ["A girl lifts a bucket by the handle with her left hand.", "The girl tilts the bucket with her right hand.", "The bucket tips out mud-smeared crawfish.", "A little boy brings over a pair of trousers.", "The boy pours out mud-covered crawfish from the trousers.", "Crawfish hit the ground near the rural house."], "n_events": 6, "n_shots": 1, "n_subjects": 2, "old_duration": 10.43, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/52_no_audio.mp4"}, "vid": 452}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At the entrance of a school, a group of students are walking out of the gates. A rope has been pulled up next to the gate, with relatives who came to pick up students being held back behind the rope. The closest to the camera is a boy in a blue outfit and a little girl in a white dress. The boy in blue looks back at the little girl, who watches another male student hopping out. When that student reaches the little girl, she opens her arms wide, and the student picks her up and carries her forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 453, "extra_info": {"duration": 9.2, "events": ["Students walk out of school gates.", "A rope has been pulled up.", "Relatives wait behind the rope.", "Boy in blue looks back.", "Little girl watches another student.", "Student hops out of school.", "Little girl opens her arms wide.", "Student picks little girl up.", "Student carries little girl forward."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 453}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "There was a circular damage on the ground. A person laid a long strip of gauze mesh over the damaged area, then used a tool to tear the mesh. After that, they layered another sheet of mesh and repeated the process until three layers of mesh were applied. The entire damaged ground was completely covered. Then the person used a tool to spread putty over the mesh and smoothed it out repeatedly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 454, "extra_info": {"duration": 9.13, "events": ["A circular damage appeared on the ground.", "A person laid a strip of gauze mesh over the area.", "The person used a tool to tear the mesh.", "The person layered another sheet of mesh.", "The process repeated until three layers were applied.", "The damaged ground got completely covered by mesh.", "The person spread putty over the mesh.", "The person smoothed out the putty repeatedly."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 454}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Underwater, a person wearing a wetsuit and fins lifts a square rock with both hands. Another person with a net bag swims up to the square rock, extends the net underneath it, where there are several large lobsters, and drives all the lobsters into the net bag with one hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 455, "extra_info": {"duration": 8.6, "events": ["Person dives underwater.", "Person wears wetsuit and fins.", "Person lifts square rock underwater.", "Second person swims up to square rock.", "Second person extends net under square rock.", "Several large lobsters appear under rock.", "Second person drives lobsters into net bag."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 455}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans down from the treetop to capture a person tapping the outer shell of a durian with the back of a small knife. Then, they pick the durian from the tree and toss it down, where another person, wearing white gloves, stretches out their arms to catch the durian and then turns to place it on the ground. The person on the tree continues to gently tap another durian with the back of the small knife, then picks it and tosses it down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 456, "extra_info": {"duration": 10.0, "events": ["Camera pans down from the treetop.", "Person taps durian with back of knife.", "Person picks the durian from the tree.", "Person tosses the durian down.", "Another person catches the durian with white gloves.", "Person wearing gloves places the durian on the ground.", "Person on the tree taps another durian with knife.", "Person picks the second durian.", "Person tosses the second durian down."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 456}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a living room, a man wearing a white T-shirt walks past the entrance, and a white cat jumps out of a cage beside the door, following the man to the center of the living room. A black dog springs out from the left side of the camera, heads toward the white cat, and nudges the cat with its nose. The white cat slightly backs away and then lifts its paws to swiftly and continuously swipe at the black dog's face, frightening the dog into retreating repeatedly. Throughout the entire process, the man in the white T-shirt stands by, watching them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 457, "extra_info": {"duration": 9.93, "events": ["Man walks past the entrance.", "White cat jumps out of cage.", "Cat follows man to living room center.", "Black dog springs out from left side.", "Dog heads toward white cat.", "Dog nudges cat with nose.", "Cat backs away from dog.", "Cat swipes at dog's face.", "Dog retreats from cat."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 457}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person spreads a sheet of dough on a pot, then drizzles yellow sauce horizontally across it, followed by a drizzle of black sauce vertically. Next, crispy corn snacks are poured in, and then spread out evenly by hand. Four pieces of black chocolate are placed on top. The background is set in a kitchen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 458, "extra_info": {"duration": 10.23, "events": ["Person spreads dough on a pot", "Person drizzles yellow sauce horizontally across dough", "Person drizzles black sauce vertically across dough", "Person pours crispy corn snacks into pot", "Person spreads crispy corn snacks by hand", "Person places four pieces of black chocolate on top"], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 458}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A gentleman, holding a beer bottle in his right hand, walked through the entrance. He glanced around and wedged the bottle cap on the edge of the door, then forcefully kicked the door with his foot. Afterward, he turned and walked back. The impact of the door shattered the bottle top. The gentleman looked at the broken bottle top in surprise, as the damaged door fell toward him, striking the back of his head. He sat down on the ground, dazed by the hit, rubbing the back of his head with his left hand while still holding the beer bottle in his right hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 459, "extra_info": {"duration": 6.75, "events": ["Gentleman walked through the entrance.", "Gentleman glanced around.", "Gentleman wedged bottle cap on door edge.", "Gentleman kicked the door with his foot.", "Gentleman turned and walked back.", "Door impact shattered the bottle top.", "Broken door fell toward the gentleman.", "Door struck the gentleman on the head.", "Gentleman sat down on the ground.", "Gentleman rubbed the back of his head."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 459}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A slender lady is seated in front of a square table, picking up a fork and preparing to eat the food on her plate. Behind her, a bearded man wearing gloves scatters a pile of powder onto her table and snatches the fork from the lady's hand to place it on the plate. He then pinches and pulls at the food on the plate. The lady stands up, takes her crossbody bag, and walks past the man, casually knocking over a drink cup on the table with her bag and then strides away. The background is within a restaurant, with other diners looking on at them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 460, "extra_info": {"duration": 13.63, "events": ["Lady prepares to eat with a fork.", "Man scatters powder onto table.", "Man snatches fork from lady's hand.", "Man places fork on the plate.", "Man pinches and pulls at food.", "Lady stands up from table.", "Lady takes her crossbody bag.", "Lady walks past the man.", "Lady knocks over drink cup with bag.", "Lady strides away from table."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 460}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A handsome long-haired gentleman kneels on the ground while a lady dressed in green holds a smartphone in front of him to record. Two assistants stand motionless behind him. The gentleman shakes his head in front of the camera to perform. The lady in green reaches out to caress the gentleman's cheek, and he grabs her hand, pulling it toward his shoulder. The lady then retracts her hand and lifts the gentleman's tie. He first bows his head, then lifts it up, and awkwardly shakes his body a bit before the video ends.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 461, "extra_info": {"duration": 15.03, "events": ["The gentleman kneels on the ground", "The lady holds a smartphone to record", "Two assistants stand behind the gentleman", "The gentleman shakes his head", "The lady caresses the gentleman's cheek", "The gentleman grabs the lady's hand", "The gentleman pulls the lady's hand toward his shoulder", "The lady retracts her hand", "The lady lifts the gentleman's tie", "The gentleman shakes his body"], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 461}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a rural lane, a shirtless man is chasing after a dog, which is holding a rabbit in its mouth. Just before reaching the camera, the man is about to catch up with the dog, which turns its head back while still holding the rabbit in its jaws.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 462, "extra_info": {"duration": 10.18, "events": ["A shirtless man chases a dog.", "A dog carries a rabbit in its mouth.", "The dog turns its head back.", "The shirtless man is about to catch the dog.", "The dog continues holding the rabbit.", "The shirtless man runs on a rural lane.", "The chase approaches the camera.", "The dog evades the shirtless man's grasp."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 462}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the snowy area in front of the house, a man came up behind a crouching child with a shovel in hand. He rapidly pushed the shovel along the ground until it reached under the child's feet, causing the child to fall and sit on the shovel. The man then ran forward, pushing the shovel with the seated child, turned at the roadside, and headed back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 463, "extra_info": {"duration": 9.28, "events": ["A man approached a crouching child from behind.", "The man readied a shovel in hand.", "Man rapidly pushed shovel along the ground.", "Shovel reached under the child's feet.", "Child fell and sat on the shovel.", "Man ran forward with the shovel.", "Seated child was pushed by the man.", "Man turned at the roadside.", "Man headed back with the child on shovel."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 463}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside a cement pipe, a little girl forcefully kicks a yellow soccer ball. The ball spins around the pipe twice, passing between the girl's legs, and continues to spin upwards. The girl leans forward slightly and catches the ball with the black bag she is carrying on her back. She then looks back to confirm the ball has fallen into the black bag, steps sideways twice to get out of the pipe, turns to face the camera, and with a smile, takes the black bag off her shoulder.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 464, "extra_info": {"duration": 9.4, "events": ["The girl kicks the yellow soccer ball inside the cement pipe", "The ball spins around the pipe twice", "Soccer ball passes between the girl's legs", "Ball continues to spin upwards", "The girl leans forward slightly", "The girl catches the ball with the black bag", "The girl looks back to confirm ball is in black bag", "The girl steps sideways twice to exit the pipe", "The girl turns to face the camera", "The girl smiles and removes the black bag from shoulder"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 464}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a green patch of ground, a man wearing jeans holds a jump rope in his hand. In front of him, there is a group of poodles. He starts to swing the jump rope and begins jumping, and the poodles follow suit. However, the first attempt fails as the rope catches on the man's legs. Then the man continues to swing the jump rope, and he and the poodles jump in unison, successfully completing two cycles of jumping. Around them, many people are filming the scene with their mobile phones.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 465, "extra_info": {"duration": 6.35, "events": ["Man holds a jump rope.", "Poodles stand in front of the man.", "Man starts swinging the jump rope.", "Man begins jumping.", "Poodles attempt to jump.", "Rope catches on the man's legs.", "Man continues to swing the jump rope.", "Man and poodles jump together.", "They complete two cycles of jumping.", "People film the scene with mobile phones."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 465}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three young men, each with both hands clutching a rope, ran forward together. One of them was wearing a life jacket. They ran to the riverbank, swinging out an arc over the water, aiming for the opposite shore. When they reached the middle of the river, the young man in white clothes fell into the water first, followed by the young man in the life jacket. The remaining young man held onto the rope, swinging a bit further, but he too quickly fell into the water. The young men held onto the rope and paddled forward some distance in the river before the video ended. Many people were watching them by the riverbank.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 466, "extra_info": {"duration": 9.2, "events": ["Three young men clutched a rope and ran forward.", "The young men swung out over the river.", "One young man in white clothes fell into the water.", "One young man in a life jacket fell into the water.", "The last young man fell into the water.", "The young men paddled forward in the river."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 466}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little dog has one of its front paws on a skateboard, while the other paws forcefully push off the ground. In front of a flower bed, the dog turns and places all of its paws on the skateboard, which glides through the crowd. With skilled movements, the dog weaves and turns among the people. The camera consistently captures the action from the dog’s perspective, following its path as it moves along.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 467, "extra_info": {"duration": 9.16, "events": ["A little dog places one paw on a skateboard.", "A little dog pushes off the ground with other paws.", "In front of a flower bed, the dog turns.", "The dog places all paws on the skateboard.", "The skateboard glides through the crowd.", "The dog weaves among the people.", "The dog turns skilled movements."], "n_events": 7, "n_shots": 1, "n_subjects": 2, "old_duration": 9.49, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/67_no_audio.mp4"}, "vid": 467}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a gauze rack hangs a nearly transparent piece of black gauze, embroidered with a monkey covering its mouth with its paws. An elderly grandma with graying hair threads her needle through the gauze. On the other side of the fabric is another elderly grandma wearing glasses. The camera pans around the gauze rack from the side of the grandma with graying hair to the grandma with glasses. The grandma with glasses receives the passed-through needle and thread, and threads it back again. On the side of the grandma with glasses, a white puppy with black eyes is being embroidered.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 468, "extra_info": {"duration": 10.63, "events": ["Piece of black gauze hangs on the rack", "Monkey is embroidered covering its mouth", "Grandma with graying hair threads her needle", "Needle passes through the gauze", "Camera pans around the gauze rack", "Grandma with glasses receives the needle", "Grandma with glasses threads the needle back", "A white puppy is being embroidered"], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 468}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "By the riverbank, a crew-cut man was walking while trying to strike up a conversation with a lady holding a cellphone next to him. As he got closer to the lady, he tried to act suave by lifting his arm, bending his leg, and leaning against a nearby pillar. No sooner had he leaned against the pillar than it collapsed, causing both him and the pillar to fall into the river.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 469, "extra_info": {"duration": 4.53, "events": ["Crew-cut man walked by the riverbank.", "Crew-cut man tried to strike up a conversation.", "Lady held a cellphone.", "Crew-cut man got closer to the lady.", "Crew-cut man lifted his arm.", "Crew-cut man bent his leg.", "Crew-cut man leaned against the pillar.", "The pillar collapsed.", "Crew-cut man fell into the river.", "The pillar fell into the river."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 469}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a training field, a little boy with both hands gripped a short crutch with a hook. He jumped off a mat, hooked the first ring with the crutch, and as he swung forward, he bypassed the second ring, directly hooking the third one. Then he continued to exert force, successively hooking each ring in front of him. Upon reaching the last one, he turned his body to face the approaching direction and hooked the second-to-last ring.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 470, "extra_info": {"duration": 9.2, "events": ["Little boy grips short crutch with both hands.", "Boy jumps off a mat.", "Boy hooks first ring with the crutch.", "Boy swings forward, bypassing second ring.", "Boy hooks third ring directly.", "Boy exerts force, hooking successive rings.", "Boy reaches last ring.", "Boy turns body to face approaching direction.", "Boy hooks second-to-last ring with crutch."], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 470}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A slightly overweight lady holding a tray walks forward. On the tray, there's a blue and white balloon. Suddenly, the balloon bursts, and a pigeon flies out. The lady gestures with her right hand, and the pigeon lands on it. The background is a stage glittering with lights.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 471, "extra_info": {"duration": 4.23, "events": ["Lady holds tray with balloon", "Balloon bursts unexpectedly", "Pigeon flies out of balloon", "Lady gestures with hand", "Pigeon lands on lady's hand"], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 471}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera faces a colorful corridor, where a lady appears from the far end. She is wearing roller skates and glides over the undulating ground. With her back to the camera, she skates into a spacious room. Once inside, the lady bends her whole body down, one foot on the ground, the other leg extended flat, and both arms open wide, gliding backwards close to the ground. Other people in the room watch the lady's performance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 472, "extra_info": {"duration": 5.48, "events": ["A lady appears from the far end of a corridor.", "The lady glides over undulating ground wearing roller skates.", "The lady skates into a spacious room.", "The lady bends her whole body down.", "The lady glides backwards close to the ground.", "Other people watch the lady's performance."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 472}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four people dressed in monkey costumes first stood in a line and then took turns strutting towards the camera, striking poses. In the end, all four of them danced together and struck a final pose to conclude the video shoot. The filming background was a small road by a street lined with rows of trees but devoid of any vehicles.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 473, "extra_info": {"duration": 7.05, "events": ["Four people in monkey costumes lined up", "Each person took turns walking towards the camera", "They each struck poses individually", "All four danced together", "Four people in monkey costumes ended with a final pose"], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 473}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside an off-road motorcycle track, a motorcycle flew off a dirt ramp and landed on the ground, followed closely by another motorcycle which flew off the ramp and landed right on the first one, knocking over the first motorcycle and its rider to the ground. The second motorcycle and its rider fell and tumbled forward. Several more motorcycles were approaching from behind.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 474, "extra_info": {"duration": 4.63, "events": ["A motorcycle flew off a dirt ramp.", "The motorcycle landed on the ground.", "Another motorcycle flew off the ramp.", "The second motorcycle landed on the first one.", "The first motorcycle and rider were knocked over.", "The second motorcycle and its rider fell.", "The second rider tumbled forward.", "Several more motorcycles were approaching.", "More motorcycles were reaching the dirt ramp."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 474}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a slope covered with snow, a person dressed in green is crawling forward, grasping a red rope. A person in a grey jacket is behind them, shouldering the rope and pulling forcefully. Behind the person in green, there is another individual wearing red who is holding a mountaineering pole, striking it beside the person in green, and then following behind them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 475, "extra_info": {"duration": 8.97, "events": ["A person in green crawls forward on snow.", "The person in green grasps a red rope.", "A person in grey pulls the rope forcefully.", "The person in red holds a mountaineering pole.", "The person in red strikes the pole beside the green person.", "The person in red follows the green person."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 475}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside a gym, two slim ladies are synchronously performing yoga. They start by standing up, then they stretch their bodies upward with their hands, followed by a forward bend and a back kick on one leg, and finally, they lift one foot to execute a challenging pose.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 476, "extra_info": {"duration": 15.05, "events": ["Two ladies start standing up inside a gym.", "Ladies stretch their bodies upward with their hands.", "They perform a forward bend.", "Each lady does a back kick on one leg.", "They lift one foot to execute a challenging pose."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 476}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an urban park, where a man is preparing to cross a small river in the park with a dog in tow. There are several stepping stones in the river for people to cross. The man lets the dog go ahead while he follows, holding the leash. Just as they are crossing a few of the stepping stones, the little dog accidentally falls into the river. After a few floundering moments in the water, it starts to swim backwards towards the shore. The man, holding the leash, helps the little dog back onto land. Other people in the park are watching them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 477, "extra_info": {"duration": 6.23, "events": ["A man prepares to cross the river.", "Dog goes ahead on stepping stones.", "Man follows holding the leash of the dog.", "The little dog falls into the river.", "The dog flounders in the water momentarily.", "Dog starts to swim backwards toward the shore.", "Man helps the dog back onto land.", "Other people in the park watch the incident."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 477}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a challenge-based show, a man dressed in a green short-sleeved white T-shirt prepares to start at the starting point. After raising his hand to signal, he quickly jumps onto the treadmill, runs swiftly across it, then nimbly and agilely sprints over one obstacle after another.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 478, "extra_info": {"duration": 15.08, "events": ["Man signals to start", "Man jumps onto treadmill", "Man runs across treadmill", "Man sprints over obstacles"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 478}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A row of motorcycle riders is poised to take off. Once the barriers in front of the motorcycles are lowered, the riders surge forth from the starting point. However, the barriers in front of two motorcycles are not properly set, preventing these two riders from immediately dashing onto the track. These two riders adjust their positions and mount the track only after the others have sped away. They raise their hands one after the other to signal backward before continuing with the race.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 479, "extra_info": {"duration": 11.1, "events": ["Motorcycle riders are poised at the starting point.", "Barriers in front of the motorcycles are lowered.", "Riders surge forth from the starting point.", "Two barriers fail to lower properly.", "Two riders are prevented from dashing onto the track.", "Riders adjust their positions after the barrier malfunction.", "Delayed riders mount the track.", "The first delayed rider raises hand to signal backward.", "The second delayed rider raises hand to signal backward.", "Both delayed riders continue with the race."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 479}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "During a speed climbing competition, a Chinese female athlete grasped a protruding rock with both hands, pushed against the wall with her right foot, and pointed her left toe to the ground. When the signal to start was given, the athlete quickly ascended upward, using her hands and feet without pause. Upon reaching the summit, she leaped to slap a red square with her hand, and the display above showed the abbreviation for China, the national flag, the athlete's name, and her time of 6.54. Following that, the rope attached to the athlete began to slide downward, while the athlete herself raised her arms in cheer. Below the display appeared orange text displaying \"First place, 6.54 seconds,\" and in smaller font below, the name and score of the second-place athlete were shown.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 480, "extra_info": {"duration": 12.27, "events": ["Chinese athlete grasped a rock with both hands.", "Athlete pushed against the wall with right foot.", "Athlete pointed left toe to the ground.", "Athlete quickly ascended the wall.", "Athlete leaped to slap a red square.", "Display showed China's national flag and athlete's time.", "Rope attached to athlete began to slide downward.", "Athlete raised her arms in cheer.", "Display updated with 'First place, 6.54 seconds'.", "Display showed second-place athlete's name and score."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 480}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "By the side of a pond, two workers dressed in red are cleaning. Inside the pool, an animal places a red buoy on the side of the pond, while a bald male worker is cleaning the pool from the shore with his hands. The animal then places a yellow buoy beside the pool as well. The camera cuts to the male worker sitting in front of a bucket with the small animal standing on the edge of the pool massaging his back. Then they both play a game of catching and throwing items. Following this, the male worker rubs the animal's head with a white object, and the small animal hands over white items one by one to the male worker. There are Chinese subtitles at the bottom of the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 481, "extra_info": {"duration": 10.4, "events": ["Two workers clean by the pond.", "Animal places red buoy on the pond side.", "Bald worker cleans the pond from the shore.", "Animal places yellow buoy beside the pond.", "Bald worker sits in front of a bucket.", "Small animal massages bald worker's back.", "Bald worker and small animal play catch.", "Bald worker rubs small animal's head with white object.", "Small animal hands white items to bald worker."], "n_events": 9, "n_shots": 6, "n_subjects": 3}, "vid": 481}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A high-mountain skier is carrying his skis, walking up along a mountain ridge. He uses his skis to cut a transverse groove into the accumulated snow on the ridge to facilitate stepping, forming a row of blocky paths in the snow behind him. The camera then cuts to the mountaintop, as the skier slides down from the summit. He leaps off a cliff, rotating in the air, and lands steadily on the slope, continuing to glide downwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 482, "extra_info": {"duration": 9.67, "events": ["High-mountain skier carries skis up mountain ridge.", "Skier cuts transverse groove in snow.", "Skier forms row of blocky paths.", "Skier walks along mountain ridge.", "Skier slides down from summit.", "Skier leaps off cliff.", "Skier rotates in the air.", "Skier lands steadily on slope.", "Skier continues to glide downwards."], "n_events": 9, "n_shots": 2, "n_subjects": 1}, "vid": 482}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Several lions stand atop the canopy of a withered tree, causing the tree to topple over. The lions run out from the foliage, and the scene shifts to an off-road vehicle. Two lions run towards the vehicle from the left side, and a few more lions are by the car. After lingering briefly beside the vehicle, they walk towards the direction of the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 483, "extra_info": {"duration": 7.2, "events": ["Several lions stand atop a withered tree's canopy.", "The withered tree topples over.", "Lions run out from the foliage.", "Two lions approach the off-road vehicle from the left.", "More lions congregate by the vehicle.", "Lions linger briefly beside the vehicle.", "Lions walk towards the camera's direction."], "n_events": 7, "n_shots": 2, "n_subjects": 3}, "vid": 483}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two black boys are competing in a game. They start by pushing a soccer ball from one end of a long red table toward the other end, where there is a hole with a net hanging underneath it. The soccer balls fall into the hole one after the other. The boy on the left runs out first, while the boy on the right, upon seeing the soccer ball go into the hole, gives a clap before running forward. They both reach the yellow table, with the left boy picking up three white balls and the right boy picking up three green balls. Starting from behind a line, they each push their balls forward. There are six holes at the far end of the table, and the balls from both sides roll past the holes, hit a bumper, and roll back toward the boys.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 484, "extra_info": {"duration": 10.2, "events": ["Two boys push a soccer ball across a table", "Soccer balls fall into a hole with a net", "Boy on the left runs out of balls first", "Boy on the right claps after scoring", "Both boys run to the yellow table", "Boy on the left picks up three white balls", "Boy on the right picks up three green balls", "Both boys push balls from behind a line", "Balls roll past the holes at the table's end", "Balls hit a bumper and roll back toward the boys"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 484}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady is exercising inside a room, her hands gripping loops of rope, suspending herself upside down in the air. She continuously performs a variety of high-difficulty maneuvers in the air, and finally, she flips down to stand on the table below.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 485, "extra_info": {"duration": 8.67, "events": ["A lady grips loops of rope.", "She suspends herself upside down.", "The lady performs high-difficulty maneuvers.", "She continuously moves in the air.", "Finally, the lady flips down.", "She stands on the table below."], "n_events": 6, "n_shots": 1, "n_subjects": 1, "old_duration": 8.67, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/85.mp4"}, "vid": 485}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a beach, some people in swimwear gathered around as a man clutching a skateboard charged forward. The man stepped with one foot onto a dirt mound and leaped like a fish, flying through the air for a distance before placing the skateboard under him and landing on a slide. Lying on the skateboard, he slid down the track towards point X, continuing until he reached the water. The man spread his arms and kept gliding forward in the water, heading towards a makeshift gate constructed with three white sticks. In front of this gate, there was another, slightly smaller makeshift gate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 486, "extra_info": {"duration": 8.67, "events": ["Man holding skateboard charges forward.", "Man steps onto a dirt mound.", "Man leaps through the air.", "Man places skateboard under himself.", "Man lands on a slide.", "Man slides down the track.", "Man reaches the water.", "Man spreads arms and glides in water.", "Man heads towards a makeshift gate.", "Another smaller makeshift gate stands in front."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 486}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a spacious area, a man rides a bicycle towards the camera. As he approaches a speed bump, he lifts the handlebars and the bike raises upwards, continuing on one wheel. The front wheel comes off the bike, hits the speed bump, bounces into the air, and then lands on a blue-patterned round barrel. It bounces up again and lands on a yellow-green patterned barrel, where the wheel bounces up once more. Just then, the man arrives, grabs the front wheel with one hand and continues forward. As he approaches a turn ahead, the man slows down, riding while preparing to reattach the front wheel back onto the bike.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 487, "extra_info": {"duration": 9.7, "events": ["Man rides bicycle toward camera", "Man lifts handlebars of bicycle", "Bike raises onto one wheel", "Front wheel detaches from bicycle", "Detached wheel hits speed bump", "Wheel lands on blue-patterned barrel", "Wheel bounces onto yellow-green patterned barrel", "Man grabs front wheel with hand", "Man continues forward on bicycle", "Man slows down to reattach front wheel"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 487}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a forest clearing, a person rides a mountain bike, soaring down from a high slope and swinging left and right as they plunge down the curving gully. The camera then cuts to another spot in the forest where a different rider, also on a mountain bike, leaps into the air from behind a dirt mound and quickly zooms down the slope. As they descend, the rider swings from side to side to maintain the bike's speed and balance, before the final shot switches to a stretch of woodland.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 488, "extra_info": {"duration": 7.53, "events": ["A person rides a mountain bike in a forest clearing.", "The person soars down from a high slope.", "The rider swings left and right down the curving gully.", "A different rider leaps into the air from a dirt mound.", "The different rider quickly zooms down the slope.", "The different rider swings side to side to maintain speed and balance."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 488}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man stands in the corner of a pool, holding a long iron pair of tongs. He leans forward and swiftly extends the tongs into the water, gripping a crocodile. The crocodile thrashes from side to side in a struggle, but the man flips it onto the ground and continues to hold it with the tongs. He also comes down to the ground, and as the crocodile rolls a few times unable to break free from the tongs, the man seizes the crocodile by its tail.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 489, "extra_info": {"duration": 9.35, "events": ["A man stands in the corner of a pool.", "The man leans forward.", "The man extends the tongs into the water.", "The tongs grip a crocodile.", "The crocodile thrashes from side to side.", "The man flips the crocodile onto the ground.", "The man holds the crocodile with the tongs.", "The man comes down to the ground.", "The crocodile rolls a few times.", "The man seizes the crocodile by its tail."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 489}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a rural kitchen, beside the stove, there are three large bowls arranged with seasonings inside. The dumplings in the big pot have been cooked, and a person is using a strainer ladle, scooping out the dumplings in turn and pouring them into the bowls.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 490, "extra_info": {"duration": 9.58, "events": ["Three large bowls are arranged beside the stove.", "Seasonings fill the three large bowls.", "Dumplings in the big pot finish cooking.", "A person scoops out dumplings with a strainer ladle.", "Person pours dumplings into the first bowl.", "Person pours dumplings into the second bowl.", "Person pours dumplings into the third bowl."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 490}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the edge of a field, a tricycle is parked, with two watermelons placed in front of it. A man, bare-chested, leaps up, grabbing the railings of the tricycle, spinning and inverting on top of the vehicle, performing multiple gymnastic moves.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 491, "extra_info": {"duration": 4.97, "events": ["A man leaps up.", "The man grabs the tricycle railings.", "The man spins on top of the tricycle.", "The man inverts on the tricycle.", "The man performs gymnastic moves."], "n_events": 5, "n_shots": 1, "n_subjects": 1, "old_duration": 6.63, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/91_no_audio.mp4"}, "vid": 491}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person stretches out their right hand, with a slice of ham sausage on the tip of each finger. A small dog approaches and eats the ham sausage off the index finger, revealing a long fingernail. Then, the dog proceeds to eat the ham sausages off the other fingers in succession, with each slice of ham sausage skewered on long fingernails.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 492, "extra_info": {"duration": 12.75, "events": ["A person stretches out their right hand.", "A slice of ham sausage is on each fingertip.", "A small dog approaches the person.", "The dog eats the ham sausage off the index finger.", "The dog reveals a long fingernail on the index finger.", "The dog eats the ham sausage off the middle finger.", "The dog eats the ham sausage off the ring finger.", "The dog eats the ham sausage off the little finger.", "The dog eats the ham sausage off the thumb."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 492}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the table, there is a utensil, and a small dog dressed in a floral outfit is using its right front paw to press a button on the utensil. With each press, a few black beans fall from underneath the utensil and land in a plate. The small dog finishes eating the beans, then presses the button again, and continues eating the beans.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 493, "extra_info": {"duration": 14.41, "events": ["A small dog presses a button on a utensil", "Black beans fall into a plate", "The small dog eats the beans", "The small dog presses the button again"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 493}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an open area, a truck is parked with two rows of people standing perpendicular to the truck, leaving a long passage between the truck door and the camera. In front of the camera, there is a soccer ball placed. A person takes a few steps to get a run-up and kicks the soccer ball, which flies towards the truck door, passes through the door, and then flies out from the door on the other side of the truck.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 494, "extra_info": {"duration": 5.15, "events": ["A truck is parked in an open area", "Two rows of people stand perpendicular to the truck", "A long passage is left between the truck door and the camera", "A soccer ball is placed in front of the camera", "A person takes a few steps to get a run-up", "The person kicks the soccer ball", "The soccer ball flies towards the truck door", "The soccer ball passes through the truck door", "The soccer ball flies out from the door on the other side of the truck"], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 494}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a small boat in the river, a person manipulated the oar and approached two cormorants. This person picked up a net with a long pole and scooped the cormorants out of the river; the two cormorants were fighting over a big fish.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 495, "extra_info": {"duration": 11.27, "events": ["Person on boat manipulated the oar.", "Person approached two cormorants.", "Person picked up a net with a long pole.", "Person scooped the cormorants out of the river.", "Two cormorants were fighting.", "Cormorants fought over a big fish."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 495}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady by a stream waters a fish with a mineral water bottle, and then throws the fish back into the stream.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 496, "extra_info": {"duration": 5.83, "events": ["A lady stands by a stream.", "The lady waters a fish with a mineral water bottle.", "The lady throws the fish back into the stream."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 496}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady wearing a blue skirt, holding a wine bottle in one hand and a wine glass in the other, is surfing on a surfboard in the sea. While surfing, she pours a glass of wine from the bottle into the glass and then drinks it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 497, "extra_info": {"duration": 4.73, "events": ["Lady holds wine bottle and glass while surfing.", "Lady pours wine from bottle into glass.", "Lady drinks wine from glass."], "n_events": 3, "n_shots": 1, "n_subjects": 1, "old_duration": 7.9, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/97_no_audio.mp4"}, "vid": 497}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At the entrance of a house, with hail falling outside, a person tapped a little boy on the shoulder. The boy quickly dashed into the hail, ran to the roadside, took out an electric scooter key, inserted it into the keyhole, unlocked the scooter seat, and took out a red rain cape. Then he ran back towards the entrance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 498, "extra_info": {"duration": 7.95, "events": ["A person tapped the boy on the shoulder.", "The boy dashed into the hail.", "The boy ran to the roadside.", "The boy took out an electric scooter key.", "The boy inserted the key into the keyhole.", "The boy unlocked the scooter seat.", "The boy took out a red rain cape.", "The boy ran back towards the entrance."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 498}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A bare-chested man, holding a plank of wood in his hand, accelerates forward into a run, reaching the riverside he throws the plank into the river, then leaps forward onto it. The plank glides through the water as the man stands atop it, maintaining his balance. He and the plank continue to slide until reaching the opposite bank, rushing up the shore, where a person wearing a hat records a video of the man with a cellphone.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 499, "extra_info": {"duration": 6.8, "events": ["A bare-chessed man accelerates into a run.", "Man reaches the riverside.", "Man throws the plank into the river.", "Man leaps onto the plank.", "The plank glides through the water.", "Man stands atop the plank, maintaining balance.", "Man and plank slide to the opposite bank.", "Man rushes up the shore.", "Person records a video of the man."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 499}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people carrying a large plastic bag approached the edge of a pond. They placed the bag into the water and withdrew the plastic, and a koi fish floated in the water, slowly swimming forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 500, "extra_info": {"duration": 9.15, "events": ["Two people approached the edge of a pond.", "People carried a large plastic bag.", "People placed the plastic bag into the water.", "People withdrew the plastic bag.", "A koi fish floated in the water.", "Koi fish began to swim forward."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 500}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man had a rope tied around his waist, stretched taut on both sides. He danced for a while and then stood up, spreading his arms wide with his fingers pointing to the sky. Next, the rope was pulled to the right, causing the man's body to lean to the right, hopping on his right foot. Then the rope was pulled to the left, making his body lean to the left, hopping on his left foot. The camera zoomed out to show two people pulling the rope on each side of the man, tugging it back and forth. In the distance, a fire truck was parked. Finally, the camera switched again, capturing a close-up of the man's dance moves.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 501, "extra_info": {"duration": 13.18, "events": ["A man danced with a rope tied around his waist.", "The man stood up.", "The man spread his arms wide.", "The man's fingers pointed to the sky.", "The rope pulled the man to the right.", "The man hopped on his right foot.", "The rope pulled the man to the left.", "The man hopped on his left foot.", "Two people tugged the rope back and forth.", "The man performed dance moves in close-up."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 501}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady has a glass bowl placed in front of her, with one hand on the left side of the frame adding seasoning to the bowl, and another hand on the right side pouring in clear water. Then, the lady stirs the seasoning in the bowl until it becomes a paste. After that, the lady takes a round wooden plate and places a cut cucumber in a raincoat style on it; she then pours the mixed seasoning over the cucumber. With chopsticks, she maneuvers the cucumber to coat it evenly with the seasoning, and finally, she picks up a piece of the cucumber and eats it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 502, "extra_info": {"duration": 9.63, "events": ["Lady adds seasoning to the glass bowl.", "Hand pours clear water into the bowl.", "Lady stirs seasoning in bowl into paste.", "Lady places cut cucumber on wooden plate.", "Lady pours seasoning over the cucumber.", "Lady coats cucumber with seasoning using chopsticks.", "Lady eats a piece of seasoned cucumber."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 502}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video is playing in slow motion, showing a scene where several characters with white hair are fighting in a room. In the center of the screen, a long-haired woman lifts a vase and smashes it towards the short-haired man on the right. The white-haired man behind raises a painting canvas and slams it downwards. The long-haired woman on the right side of the screen throws a stack of papers. The short-haired man is struck down, the canvas shatters, and after throwing the papers, the long-haired woman steps forward and kicks towards the long-haired woman in the middle. Paper and petals flutter through the air. On the left side of the screen, a woman with brown hair stands motionless.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 503, "extra_info": {"duration": 8.6, "events": ["Long-haired woman lifts a vase.", "Long-haired woman smashes vase towards short-haired man.", "White-haired man raises a painting canvas.", "White-haired man slams canvas downwards.", "Long-haired woman throws stack of papers.", "Short-haired man is struck down.", "Painting canvas shatters.", "Long-haired woman kicks towards another long-haired woman.", "Paper and petals flutter through the air."], "n_events": 9, "n_shots": 1, "n_subjects": 5}, "vid": 503}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the table, there is a ball of dough. Someone used a tool to cut four even lines into the dough without actually slicing it through. Then, they used scissors to snip a flower shape in the center of the dough, placing a red date in the center of the flower. Next, they used a comb to work on the dough's surface. Finally, they used scissors to cut out a flower shape on the outer side of the dough.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 504, "extra_info": {"duration": 15.1, "events": ["Someone cut four lines into the dough.", "Someone snipped a flower shape in the dough's center.", "Someone placed a red date in the flower center.", "Someone combed the dough's surface.", "Someone cut out a flower shape on the dough's outer side."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 504}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little calico cat was running along the side of the road, and when it reached a courtyard, the little cat turned into it and came to a door. The cat pressed its nose against the screen door and then looked behind. There was a string around the cat's neck, which was being pulled by someone outside the frame of the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 505, "extra_info": {"duration": 11.23, "events": ["Calico cat ran along the road.", "Calico cat turned into a courtyard.", "Calico cat reached a door.", "Calico cat pressed its nose against the screen door.", "Calico cat looked behind.", "String was around the cat's neck.", "Someone pulled the string."], "n_events": 7, "n_shots": 1, "n_subjects": 1, "old_duration": 11.23, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/105.mp4"}, "vid": 505}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside the room, a little white cat spread its hind legs and ran forward. It ran past a door and arrived in front of a black table, then crawled under the table.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 506, "extra_info": {"duration": 4.3, "events": ["A little white cat spread its hind legs.", "The little white cat ran forward.", "The little white cat ran past a door.", "The little white cat arrived in front of a black table.", "The little white cat crawled under the black table."], "n_events": 5, "n_shots": 1, "n_subjects": 1, "old_duration": 4.3, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/106.mp4"}, "vid": 506}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a field, a white dog joyfully bounds forward. It comes to a person's side who is jogging slowly, and the little white dog continues to hop along cheerfully. In the distance of the frame are palm trees, blue sky, and white clouds.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 507, "extra_info": {"duration": 13.23, "events": ["A white dog bounds forward in a field.", "The white dog reaches the person's side.", "The person jogs slowly.", "The white dog hops along cheerfully."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 507}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an aquarium, a caretaker was walking back and forth on the shore. A dolphin leaped out of the water and then splashed back in, followed by another dolphin doing the same. Then, both dolphins leaped out of the water together, twirled in the air before plunging back into the water while another dolphin continued to swim along the shore the whole time.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 508, "extra_info": {"duration": 8.9, "events": ["Caretaker walked back and forth on the shore", "First dolphin leaped out of the water", "First dolphin splashed back into the water", "Second dolphin leaped out of the water", "Second dolphin plunged into the water", "Both dolphins leaped out of the water together", "Dolphins twirled in the air", "Dolphins plunged back into the water", "Third dolphin swam along the shore"], "n_events": 9, "n_shots": 1, "n_subjects": 4, "old_duration": 8.9, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/108.mp4"}, "vid": 508}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On an outdoor dirt path, the camera captures from the front several puppies running towards the camera. As the puppies approach, the camera keeps retreating. Finally, the camera shoots from a low angle, focusing on one of the cream-colored puppies as it runs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 509, "extra_info": {"duration": 10.1, "events": ["Several puppies run towards the camera.", "The camera retreats from the approaching puppies.", "One cream-colored puppy becomes the focus.", "The puppy runs on an outdoor dirt path."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 509}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside a kindergarten classroom, a boy and a girl are having a competition. They use a long-handled wooden spoon to move a ball from one circle to another. The boy moves a green ball between green circles, while the girl moves a blue ball between blue circles. The boy quickly picks up the ball and runs back and forth, while the girl merely walks briskly. The boy's speed is clearly much faster than the girl's. A group of children sit nearby, watching their competition.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 510, "extra_info": {"duration": 10.03, "events": ["Boy and girl start a competition.", "Boy uses wooden spoon to move green ball.", "Girl uses wooden spoon to move blue ball.", "Boy quickly picks up the ball.", "Boy runs back and forth between circles.", "Girl walks briskly between circles.", "Children sit nearby watching the competition."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 510}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside a concrete pipe, a little girl kicks a black and white soccer ball forcefully. The ball rolls around the concrete pipe twice, passes between the girl's legs, and continues to spin upwards. The girl leans forward slightly and catches the soccer ball on the black backpack she is carrying on her back. The girl looks back to confirm the ball has fallen into the backpack, then smiles and walks out of the concrete pipe.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 511, "extra_info": {"duration": 6.13, "events": ["A little girl kicks a soccer ball inside a concrete pipe.", "The soccer ball rolls around the concrete pipe twice.", "The soccer ball passes between the little girl's legs.", "The soccer ball continues to spin upwards.", "The little girl leans forward slightly.", "The little girl catches the soccer ball on her backpack.", "The soccer ball falls into the backpack.", "The little girl looks back to confirm the ball.", "The little girl smiles.", "The little girl walks out of the concrete pipe."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 511}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of twin babies are playing in a room. They come to a cabinet, where one of them leans on it with both hands. The other comes behind him, lifts the skirt, and pulls down the pants to take a look at his buttocks. Then, they switch roles; one who was observing now leans on the cabinet with both hands, and the one who was in front moves behind, lifts the skirt, pulls down the pants, and takes a look at the buttocks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 512, "extra_info": {"duration": 15.1, "events": ["Twin babies play in a room.", "One twin leans on a cabinet.", "The other twin lifts one's skirt.", "The other twin pulls down one's pants.", "Twin babies switch roles.", "Former observing twin leans on the cabinet.", "Former leaning twin lifts other's skirt.", "Former leaning twin pulls down other's pants."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 512}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a zoo, a panda played on a makeshift swing composed of three poles, two ropes, and a tire. As it sat on the tire, ready to swing forward, it didn't even start to sway before the panda toppled backwards onto the ground. Afterwards, the panda rolled around on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 513, "extra_info": {"duration": 6.43, "events": ["A panda climbed onto a makeshift swing.", "A panda sat on the tire.", "A panda attempted to swing forward.", "A panda toppled backwards onto the ground.", "A panda rolled around on the ground."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 513}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A panda tumbles forward from a wooden ramp it's constructed, rolling down the slope and onto the grass. It takes a turn and continues to roll down the grassy incline, then it hits a rock, pivots around, and keeps rolling downhill.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 514, "extra_info": {"duration": 15.2, "events": ["A panda tumbles from a wooden ramp.", "Panda rolls down the slope.", "Panda lands on the grass.", "Panda takes a turn.", "Panda continues to roll down the incline.", "Panda hits a rock.", "Panda pivots around.", "Panda keeps rolling downhill."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 514}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an aquarium, a female diver dressed as a mermaid dances through the water. She makes a heart shape with her hands, followed by a gesture of shooting an arrow and blowing a kiss. A cluster of heart-shaped lights appears in the water, encircling the mermaid. Afterwards, several heart-shaped rings of light emerge, as the mermaid turns and swims away. Many spectators are seen through the glass, watching the mermaid's performance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 515, "extra_info": {"duration": 11.1, "events": ["A female diver dances through the water.", "The female diver makes a heart shape with her hands.", "The female diver acts like shooting an arrow.", "The female diver blows a kiss.", "Heart-shaped lights appear in the water.", "Heart-shaped lights encircle the mermaid.", "Heart-shaped rings of light emerge in the water.", "The mermaid turns in the water.", "The mermaid swims away.", "Spectators watch the mermaid's performance through the glass."], "n_events": 10, "n_shots": 1, "n_subjects": 2, "old_duration": 11.1, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/115.mp4"}, "vid": 515}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a black child dressed in a red ski suit skis down a slope. The camera follows the child as they make turns, sometimes slowing down, until finally, the child raises their arms in cheer and comes to a stop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 516, "extra_info": {"duration": 8.13, "events": ["A black child skis down a slope.", "The child makes turns.", "The child slows down.", "The child raises their arms.", "The child comes to a stop."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 516}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A girl in a short skirt is skateboarding. She is not very skilled yet, as her footing and mounting on the board are a bit shaky. The girl slides forward, turns around, and continues to skate. The skateboard comes to a stop due to insufficient momentum. The girl sways her body forward slightly, propelling the skateboard forward once more.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 517, "extra_info": {"duration": 9.0, "events": ["A girl mounts the skateboard.", "The girl exhibits shaky footing.", "The skateboard slides forward.", "The girl turns around on the skateboard.", "She continues to skate.", "The skateboard comes to a stop.", "The girl sways her body forward.", "The skateboard propels forward again."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 517}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A cartoon piglet is pole dancing; the piglet performs enchanting pole dance moves on the stage, then jumps onto the pole to spin, accelerates the spinning speed after looking into the camera, and finally spins out of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 518, "extra_info": {"duration": 7.2, "events": ["Cartoon piglet performs pole dance moves on stage.", "Cartoon piglet jumps onto the pole.", "Cartoon piglet starts to spin on the pole.", "Cartoon piglet accelerates spinning speed.", "Cartoon piglet spins out of the frame."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 518}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman struck a martial arts pose in front of a strength tester, then spun her body around, lifted her left foot, suspended herself in midair, and prepared to kick the strength tester with her right foot. Someone beside her admired the woman's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 519, "extra_info": {"duration": 3.4, "events": ["A woman struck a martial arts pose.", "The woman spun her body around.", "The woman lifted her left foot.", "The woman suspended herself in midair.", "The woman prepared to kick the strength tester.", "Someone admired the woman's movements."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 519}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An athlete is skiing, sliding up a slope, soaring into the air, spinning, and then landing steadily backwards. He then turns around, glides down a track, and once again lands steadily backwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 520, "extra_info": {"duration": 7.3, "events": ["Athlete skis up a slope", "Athlete slides into the air", "Athlete performs spinning", "Athlete lands backwards steadily", "Athlete turns around", "Athlete glides down a track", "Athlete lands steadily backwards"], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 520}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside an airplane cabin, skydivers jump out one after another from the hatch. After the last athlete jumps out of the plane, the camera follows the last athlete to film, with the sun in the distance, clouds beside them, and the earth below.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 521, "extra_info": {"duration": 11.03, "events": ["Skydivers line up inside the airplane cabin.", "First athlete jumps out of the airplane hatch.", "Subsequent skydivers jump out one after another.", "Last athlete exits the airplane.", "Camera follows the last skydiver.", "Skydiver freefalls.", "Sun appears in the distance.", "Clouds are visible beside the skydiver.", "Earth is seen below.", "Skydiver stabilizes body position."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 521}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three women are practicing rope gymnastics in a gym, moving in unison to extend their legs, then spinning in the air, followed by opening their legs and inverting their bodies.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 522, "extra_info": {"duration": 6.2, "events": ["Three women practice rope gymnastics in a gym.", "Women move in unison.", "Women extend their legs.", "Women spin in the air.", "Women open their legs.", "Women invert their bodies."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 522}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a person is making a dish. First, they cut the meat into small pieces, then add greens, pickled vegetables, and marinated dried tofu to a bowl of rice. Next, they add pork trotter meat and the sliced char siu, half an egg, and finally drizzle with sauce. Text on the screen highlights the key steps and asks how much this bowl of cuisine costs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 523, "extra_info": {"duration": 9.03, "events": ["A person cuts the meat into small pieces", "A person adds greens to a bowl of rice", "A person adds pickled vegetables to the bowl", "A person adds marinated dried tofu to the bowl", "A person adds pork trotter meat to the bowl", "A person adds sliced char siu to the bowl", "A person places half an egg into the bowl", "A person drizzles sauce over the bowl", "Text on the screen highlights the key steps", "Text asks the cost of the bowl of cuisine"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 523}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two little kittens are lying on the trunk of the tree. The camera shifts to the kitten above, which opens its mouth and makes a sound. Then, the camera zooms out to a full view, and the two little kittens begin to climb up the tree one after the other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 524, "extra_info": {"duration": 7.87, "events": ["Two kittens lie on the trunk of a tree.", "The camera shifts to the upper kitten.", "The upper kitten opens its mouth.", "The upper kitten makes a sound.", "The camera zooms out for a full view.", "The first kitten begins to climb the tree.", "The second kitten follows climbing up the tree."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 524}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases an advanced Mercedes-Benz truck, first presenting the front of the vehicle, followed by the rear, the front body and the back body. The top of the truck is also capable of rising in layers.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 525, "extra_info": {"duration": 8.9, "events": ["Video showcases an advanced Mercedes-Benz truck", "Presentation begins with the front of the vehicle", "Camera moves to present the rear of the truck", "Front body of the truck is displayed", "Back body of the truck comes into view", "The top of the truck rises in layers"], "n_events": 6, "n_shots": 5, "n_subjects": 1}, "vid": 525}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of motorcyclists are riding mini motorcycles on a racetrack. At a curve, the riders lean their bodies and take the turn with a tilt.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 526, "extra_info": {"duration": 6.2, "events": ["A group of motorcyclists ride mini motorcycles.", "Motorcyclists approach a curve on the racetrack.", "Motorcyclists lean their bodies.", "Motorcyclists take the turn.", "Motorcycles tilt on the curve."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 526}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The puppy chases after the duckling as it runs forward, then knocks it down to the ground. The duckling stands up, and then it turns around and continues the chase, running towards the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 527, "extra_info": {"duration": 7.8, "events": ["The puppy chases the duckling.", "The duckling runs forward.", "The puppy knocks the duckling down.", "The duckling stands up.", "The duckling turns around.", "The duckling continues the chase.", "The duckling runs towards the camera."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 527}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A puppy is lying down on the ground, while another puppy runs forward. The camera cuts to a puppy running forward, then tumbling and falling down the stairs. The background transitions from a summer park to a small path in a winter park, with the surroundings changing from green plants to snowy scenes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 528, "extra_info": {"duration": 3.83, "events": ["A puppy lies down on the ground.", "Another puppy runs forward.", "The puppy tumbles.", "The puppy falls down the stairs.", "The background transitions to winter.", "Snowy scenes appear."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 528}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two kittens were squatting at the corner, observing. The shadow of the cat on the right appeared on the adjacent glass. It leaned over to observe carefully, then suddenly leaped forward, extending its paw to interact with the other kitten. It then stood with one paw on the glass, while the other kitten reached out a paw and looked up at it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 529, "extra_info": {"duration": 10.7, "events": ["Two kittens were squatting at the corner.", "The shadow of the cat appeared on the glass.", "The cat on the right leaned over.", "The cat on the right leaped forward.", "The cat on the right extended its paw.", "The cat on the right stood with one paw on the glass.", "The other kitten reached out a paw.", "The other kitten looked up."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 529}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The puppies lay down in front of the food, and a person took the buns one by one and tossed them to them. The puppies grabbed the buns and then turned around to eat elsewhere, while the Border Collie in the middle watched them and walked forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 530, "extra_info": {"duration": 9.2, "events": ["Puppies lay down in front of the food", "Person took a bun", "Person tossed a bun to a puppy", "A puppy grabbed the bun", "Puppy turned around", "Puppy walked to eat elsewhere", "Border Collie watched the puppies", "Border Collie walked forward"], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 530}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three little puppies chased forward. The two on the sides tumbled down headfirst from the steps. The camera switched to the three little ones running in a line on the stairs, with the two at the back falling down. The camera then switched again, showing them running side by side, with the two on the left falling down, and the other one also falling down after running a distance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 531, "extra_info": {"duration": 10.27, "events": ["Three little puppies chased forward.", "Two puppies tumbled down from the steps.", "Three puppies ran in a line on stairs.", "Two back puppies fell down.", "Puppies ran side by side.", "Two left puppies fell down.", "Other puppy fell down after running a distance."], "n_events": 7, "n_shots": 3, "n_subjects": 3}, "vid": 531}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman stood up straight, her right foot stepping on an inline skate, while holding another in her left hand. She shifted slightly, glided to the left, then dropped the inline skate she was holding and stretched out her left leg to step on it. With both feet on the inline skates, she alternated her legs back and forth, moving to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 532, "extra_info": {"duration": 8.43, "events": ["The woman stood up straight.", "The woman's right foot stepped on an inline skate.", "The woman held another inline skate in her left hand.", "The woman shifted slightly.", "The woman glided to the left.", "The woman dropped the inline skate.", "The woman stretched out her left leg.", "The woman's left leg stepped on the inline skate.", "With both feet on skates, the woman alternated her legs.", "The woman moved to the left."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 532}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man, wearing a black vest, stepped on a skateboard and glided forward. Then, he looked down, bent his knees, and spread his arms to jump up and switch his stance on the skateboard to continue sliding. He turned to the left and attempted to repeat the previous maneuver but failed this time, falling to the ground and sitting back. Just then, an old man approached him head-on.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 533, "extra_info": {"duration": 6.37, "events": ["The man stepped on a skateboard.", "The man glided forward.", "The man looked down.", "The man bent his knees.", "The man spread his arms.", "The man jumped up.", "The man switched his stance on the skateboard.", "The man turned to the left.", "The man attempted to repeat the maneuver.", "The man fell to the ground."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 533}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A child wearing protective gear is skateboarding on a U-shaped track, sliding from left to right. The camera follows the movement, and as the child soars into the air, they quickly spin their body, switching directions before landing back on the track and continuing to slide left. In the air, the child flips both their body and skateboard, and after landing, they slide to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 534, "extra_info": {"duration": 5.7, "events": ["Child wears protective gear", "Child skateboards on U-shaped track", "Child slides from left to right", "Child soars into the air", "Child quickly spins body", "Child switches directions", "Child lands back on track", "Child slides left", "Child flips body and skateboard", "Child slides to the right"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 534}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person with a paraglider spirals down to the surface of the lake from the sky, gliding on with one foot touching the water, causing a spray of water to fly in all directions. The camera follows the movement of the individual,", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 535, "extra_info": {"duration": 10.53, "events": ["Person spirals down from sky", "Person with paraglider approaches lake surface", "Person touches water with foot", "Spray of water flies in all directions"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 535}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two teams, one dressed in red and the other in blue, are playing a volleyball match. The ball is hit into the air, with the camera following its movement. A woman uses both hands to hit the falling volleyball back into the air, while another woman runs forward, jumps up, and stretches out her right hand to strike the ball towards the opposite side. The players on the other side fail to catch it, and the ball hits the ground. The team in red gathers together, celebrating their point. Other team members can be seen moving around them, along with the seated referee.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 536, "extra_info": {"duration": 7.2, "events": ["Two teams play a volleyball match.", "The ball is hit into the air.", "A woman hits the volleyball back.", "Another woman runs forward.", "The running woman jumps up.", "The jumping woman strikes the ball.", "Players on the other side fail to catch the ball.", "The ball hits the ground.", "The team in red celebrates.", "Other team members move around."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 536}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures a low-angle shot of a man wearing a black coat moving a table to the doorway. After setting it down, he turns and walks towards the camera, bends over to pick up a chair, and then heads back towards the doorway. Turning to look at the camera, he swings one arm in front of the lens.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 537, "extra_info": {"duration": 7.3, "events": ["Camera captures a low-angle shot of a man.", "Man wearing black coat moves a table.", "Man positions the table at the doorway.", "Man turns and walks towards the camera.", "Man bends over to pick up a chair.", "Man carries the chair towards the doorway.", "Man turns to look at the camera.", "Man swings his arm in front of the lens."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 537}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera follows the girl as she moves forward, holding a bouquet of flowers and a yellow paper bag, running forward to approach a boy. The boy takes the flowers and laughs joyously, while the girl covers her mouth and giggles bashfully. The two make eye contact as the people around them applaud.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 538, "extra_info": {"duration": 12.97, "events": ["The girl runs forward.", "The girl holds a bouquet of flowers.", "The girl carries a yellow paper bag.", "The girl approaches a boy.", "The boy takes the flowers.", "The boy laughs joyously.", "The girl covers her mouth.", "The girl giggles bashfully.", "The girl and the boy make eye contact.", "The people around applaud."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 538}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The image on the screen is shaking. A man in a black short-sleeve shirt reaches upwards with both hands, looking up. A yellow box falls, and the man catches it and looks towards the camera. The shot switches to the man opening the box, then to him taking out a hamburger, to him taking a bite, to him taking a second bite, to him taking a third bite, and to the man stuffing the rest into his mouth and finishing it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 539, "extra_info": {"duration": 10.97, "events": ["The image on the screen shakes.", "A man reaches upwards with both hands.", "A yellow box falls.", "The man catches the yellow box.", "The man looks towards the camera.", "The man opens the yellow box.", "The man takes out a hamburger.", "The man takes a bite of the hamburger.", "The man takes a second bite of the hamburger.", "The man finishes the hamburger by stuffing the rest into his mouth."], "n_events": 10, "n_shots": 7, "n_subjects": 1}, "vid": 539}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman dribbles the basketball with one arm while squatting down to pick up a jacket and slips on one of its sleeves. Then, with a forceful pat, the basketball flies out of the frame. The woman looks up, and as it falls back, she moves forward, continuing to dribble the basketball. The camera follows the movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 540, "extra_info": {"duration": 15.1, "events": ["A woman dribbles the basketball with one arm.", "The woman squats down.", "The woman picks up a jacket.", "The woman slips on a jacket sleeve.", "The woman pats the basketball forcefully.", "The basketball flies out of the frame.", "The woman looks up.", "The basketball falls back.", "The woman moves forward.", "The woman continues to dribble the basketball."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 540}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man stands in front of the camera, stretches out his left hand, then looks down at the monkey hugging his leg. He strokes it with both hands and squats down. Suddenly, the monkey reaches out and hits the man in the face, and the man steps back to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 541, "extra_info": {"duration": 4.7, "events": ["The man stretches out his left hand.", "The man looks down at the monkey.", "The monkey hugs the man's leg.", "The man strokes the monkey with both hands.", "The man squats down.", "The monkey reaches out.", "The monkey hits the man in the face.", "The man steps back to the right."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 541}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A rooster runs forward, with the camera following the movement. A man stands in front of a fence with his hands in his pockets, smiling as he looks at the rooster and walks towards it. Other hens follow by his side. The camera then cuts to the man walking forward, with the hens following behind him, as he looks back over his shoulder.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 542, "extra_info": {"duration": 10.5, "events": ["A rooster runs forward.", "The camera follows the rooster.", "A man stands in front of a fence.", "The man walks towards the rooster.", "Hens follow by the man's side.", "The man walks forward.", "Hens follow behind the man.", "The man looks back over his shoulder."], "n_events": 8, "n_shots": 2, "n_subjects": 3, "old_duration": 10.5, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/142.mp4"}, "vid": 542}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A little donkey is running around in circles inside the pen. Another donkey walks over, and the camera follows the movement of the little donkey as it runs up to the other one. The background is a pen in winter, covered with a thick layer of snow.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 543, "extra_info": {"duration": 7.58, "events": ["A little donkey runs around in circles.", "Another donkey walks over.", "The little donkey runs up to the other donkey."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 543}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two athletes are riding forward on their bicycles when the person on the right suddenly falls to the left, knocking the person on the left down to the ground. The camera then cuts to an athlete bent over, repairing the bicycle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 544, "extra_info": {"duration": 8.5, "events": ["Two athletes ride forward on bicycles.", "Person on right falls to the left.", "Person on left is knocked down to the ground.", "Athlete bends over.", "Athlete repairs the bicycle."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 544}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman dismounted from the horseback, grabbed the saddle with an external force, stepped on the surface of the water, and moved forward following the horse. She turned her head to look at the camera, with water splashing around. The camera followed with the motion.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 545, "extra_info": {"duration": 8.33, "events": ["The woman dismounted from the horseback.", "The woman grabbed the saddle.", "The woman exerted external force.", "The woman stepped on the water surface.", "The woman moved forward.", "The horse moved.", "The woman turned her head towards the camera.", "Water splashed around.", "The camera followed the motion."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 545}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy glides forward with his left foot on the skateboard, then turns and continues to glide with both feet on the board. After crouching down, he slides off the steps, and then raises his hands. Meanwhile, a person walks ahead, with the camera following the movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 546, "extra_info": {"duration": 8.93, "events": ["The boy glides forward on the skateboard.", "The boy turns on the skateboard.", "The boy continues gliding with both feet on the board.", "The boy crouches down on the skateboard.", "The boy slides off the steps.", "The boy raises his hands.", "A person walks ahead."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 546}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three kittens climbed onto the old man's legs, and the old man one by one removed the kittens from his legs and put them back on the ground, while another kitten climbed under the tree on the right. The camera remained steady.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 547, "extra_info": {"duration": 6.1, "events": ["Three kittens climbed onto the old man's legs.", "The old man removed one kitten from his legs.", "The old man removed another kitten from his legs.", "The old man removed the third kitten from his legs.", "A kitten climbed under the tree on the right."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 547}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person dressed in a wingsuit bends over and jumps off a mountain, opening their limbs and gliding down through the valleys, with a backdrop of mountain peaks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 548, "extra_info": {"duration": 6.6, "events": ["Person in wingsuit bends over", "Person jumps off a mountain", "Person opens limbs", "Person glides down", "Person passes through valleys", "Mountain peaks serve as backdrop"], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 548}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman dressed in a floral skirt held a cane, took off her glasses to glance at the man beside her, then put her glasses back on and tapped the man with them before continuing to walk forward. The man suddenly stood up straight, touching his butt, then turned his head to look at the woman, and then moved forward, extending his right hand. He bent over twice to touch his knee that had been struck. The woman, holding the cane, tapped the ground left and right as she walked forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 549, "extra_info": {"duration": 10.43, "events": ["Woman took off her glasses.", "Woman glanced at the man.", "Woman put her glasses back on.", "Woman tapped the man with her glasses.", "Woman continued to walk forward.", "Man stood up straight.", "Man touched his butt.", "Man turned his head to look at the woman.", "Man moved forward, extending his right hand.", "Man bent over to touch his knee."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 549}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Some men are jumping together on a trampoline, and after one of them falls, he is bounced high into the air. The camera captures an overhead shot of the man being bounced upwards, then he falls back down, and the camera switches to show the men moving backwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 550, "extra_info": {"duration": 7.0, "events": ["Men jump together on a trampoline.", "One man falls on the trampoline.", "The man is bounced high into the air.", "The man falls back down onto the trampoline.", "Men move backwards."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 550}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man runs forward holding a skimboard, throws it onto the water's surface, quickly steps on it, and glides across the water. Then he jumps up, flips in the air, and falls back down, creating a splash. The camera follows the man as he moves. The background is a seaside setting.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 551, "extra_info": {"duration": 5.0, "events": ["A man runs forward holding a skimboard.", "Man throws skimboard onto the water's surface.", "Man quickly steps on the skimboard.", "Man glides across the water.", "Man jumps up from the skimboard.", "Man flips in the air.", "Man falls back down into the water.", "Water creates a splash."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 551}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man runs forward, plunges into the water with his hands hugging his knees, causing water to splash in all directions. The camera follows the shot, then switches to beneath the surface, where a man is seen moving underwater while holding something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 552, "extra_info": {"duration": 6.28, "events": ["The man runs forward.", "The man plunges into the water.", "The man hugs his knees.", "Water splashes in all directions.", "The man moves underwater.", "The man holds something."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 552}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man swims in the pool of a train carriage, then turns around to sit up, wipes the water off his face, and each takes a slice of watermelon to eat with the two men beside him. The camera pans to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 553, "extra_info": {"duration": 9.18, "events": ["A man swims in the pool of a train carriage.", "The man turns around to sit up.", "The man wipes the water off his face.", "The man takes a slice of watermelon.", "Two men beside the man each take a slice of watermelon.", "The men eat the watermelon."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 553}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man walks forward, picks up a piece of meat from the bag, extends his left hand to toss it into the air and then steps back. The lion leaps into the air, catches the chunk of meat in its jaws, and lands on the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 554, "extra_info": {"duration": 9.85, "events": ["The man walks forward.", "The man picks up a piece of meat.", "The man extends his left hand.", "The man tosses the meat into the air.", "The man steps back.", "The lion leaps into the air.", "The lion catches the chunk of meat in its jaws.", "The lion lands on the ground."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 554}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of hands scoop up a school of small fish from the water, their bodies wiggling as the stream flows on.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 555, "extra_info": {"duration": 8.1, "events": ["Hands scoop up a school of small fish.", "Small fish wiggle their bodies.", "The stream flows on."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 555}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person descends from the sky with a paraglider, skimming the water with their feet, creating a splash, then lands on the grass. They grab the paraglider and sprint forward rapidly. The camera follows the movement, then cuts to a snowy day, where one person lands with a paraglider while another person holds onto the ropes beneath, descending together forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 556, "extra_info": {"duration": 10.2, "events": ["Person descends from sky with paraglider.", "Person skims water with feet creating splash.", "Person lands on grass.", "Person grabs the paraglider.", "Person sprints forward.", "Person lands with paraglider on snowy day.", "Another person holds onto ropes beneath paraglider.", "Persons descend together forward."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 556}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background features an indoor climbing wall. A woman is moving and climbing up the wall, coordinated with her hands and feet, then looks down while holding onto an object with one hand. The camera follows the movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 557, "extra_info": {"duration": 5.9, "events": ["A woman climbs up the indoor wall.", "The woman coordinates her hands and feet.", "The woman looks down.", "The woman holds onto an object with one hand."], "n_events": 4, "n_shots": 1, "n_subjects": 1, "old_duration": 5.9, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/157.mp4"}, "vid": 557}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman runs forward a bit, then jumps and mounts the cow's back. The cow is grazing; the man walks up beside the cow, bends down to look, then pulls the grass away, and the cow turns and follows, running forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 558, "extra_info": {"duration": 9.83, "events": ["The woman runs forward.", "The woman jumps.", "The woman mounts the cow's back.", "The cow grazes.", "The man walks up beside the cow.", "The man bends down.", "The man pulls the grass away.", "The cow turns.", "The cow follows the man.", "The cow runs forward."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 558}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men are standing in the middle of a stone wall, one taller and one shorter, passing the ball back and forth, with another person walking around on the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 559, "extra_info": {"duration": 7.93, "events": ["Two men stand in the middle of a stone wall.", "One man passes the ball to the other man.", "The other man passes the ball back to the first man.", "A person walks around on the left."], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 559}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is washing their hands when a little dog, carrying a cloth toy in its mouth, jumps onto the basin. The person takes away the toy, and the little dog drinks water flowing from the tap. The camera then cuts to footage of the dog continuing to drink water. The background is an indoor living room.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 560, "extra_info": {"duration": 8.68, "events": ["A person washes their hands", "A little dog jumps onto the basin", "The little dog carries a cloth toy in its mouth", "The person takes the toy from the little dog", "The little dog drinks water from the tap", "The camera shows the dog drinking water", "The background shows an indoor living room"], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 560}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man pulls on the bar with both hands, leaps upward and then releases his hands, clapping them in front of his chest. After landing, he reaches out to grab the bar again, then jumps down to the ground, performing a muscle-flexing pose. The camera then cuts to another man who is standing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 561, "extra_info": {"duration": 2.7, "events": ["The man pulls on the bar", "The man leaps upward", "The man releases the bar", "The man claps hands in front of chest", "The man lands", "The man reaches out to grab the bar", "The man jumps down to the ground", "The man performs a muscle-flexing pose"], "n_events": 8, "n_shots": 2, "n_subjects": 2}, "vid": 561}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an interior with colorful lighting, surrounded by many mirrors. The girl runs forward and suddenly another girl appears from the right. The two collide, stumble backward, and fall to the ground, with their images of kneeling on the floor reflected in the mirrors.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 562, "extra_info": {"duration": 2.75, "events": ["The girl runs forward", "Another girl appears from the right", "The two collide", "The two stumble backward", "The two fall to the ground", "Their images are reflected in the mirrors"], "n_events": 6, "n_shots": 1, "n_subjects": 2, "old_duration": 2.88, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/162_no_audio.mp4"}, "vid": 562}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man in a black short-sleeved shirt walks towards the camera, picks up the nunchucks in front of him, taps them together and gives a couple of pulls as a demonstration, then performs nunchuck movements with hooking actions to the left and right. The scene ends with a cross of his arms. The background is an indoor living room with a computer and a computer desk around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 563, "extra_info": {"duration": 10.36, "events": ["The man walks towards the camera", "The man picks up the nunchucks", "The man taps nunchucks together", "The man gives a couple of pulls", "The man performs nunchuck movements", "The man hooks to the left", "The man hooks to the right", "The scene ends with the man crossing arms"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 563}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The puppy bites the columnar object that is fixed to the wall in front of it. It leans forward then backs away, trying repeatedly, then it reaches out with its paws and looks down. The camera moves to the right. The background is an indoor corridor.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 564, "extra_info": {"duration": 9.53, "events": ["The puppy bites the columnar object.", "The puppy leans forward.", "The puppy backs away.", "The puppy tries repeatedly.", "The puppy reaches out with its paws.", "The puppy looks down."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 564}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a vast expanse of grassland. In the distance, a person is descending from the sky with a paraglider, gliding toward the camera. With bent legs, they slide forward on the grassland. The camera pans to the right as the person continues to glide. Next to them, another person is holding a phone to film the scene, while another man in a black short-sleeved shirt turns to look at the camera, walks a few steps to the right, and raises his hands to gesture.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 565, "extra_info": {"duration": 7.8, "events": ["Person descends from sky with paraglider.", "Person slides forward on grassland.", "Camera pans to the right.", "Second person holds phone to film.", "Man in black shirt turns to camera.", "Man walks a few steps to the right.", "Man raises hands to gesture."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 565}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is filled with a bunch of weeds and leaves. A boy in a white short-sleeved shirt is sitting on the ground, placing a spoon on the ground, then he looks up at the camera before picking up a flaky yellow object and placing it inside a mango peel. He puts on the shoes made out of mango peels, and the camera switches to the boy standing up and alternating his feet to show off the mango shoes. Then, he looks up, claps his hands and smiles.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 566, "extra_info": {"duration": 9.57, "events": ["A boy sits on the ground.", "The boy places a spoon on the ground.", "The boy looks up at the camera.", "The boy picks up a flaky yellow object.", "The boy places the object inside a mango peel.", "The boy puts on shoes made of mango peels.", "The boy stands up.", "The boy alternates his feet.", "The boy claps his hands.", "The boy smiles."], "n_events": 10, "n_shots": 1, "n_subjects": 1, "old_duration": 12.53, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/166_no_audio.mp4"}, "vid": 566}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Background is a classroom. Two girls stretch their arms out, suspended in the air. The girl behind runs up beside them, grabs their arms with both hands, stretches her legs out, and spins around in the air before landing on the ground, stepping back a few paces together. They laugh joyously, and the girl on the right gives a thumbs-up to the camera, then the three stand together.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 567, "extra_info": {"duration": 6.23, "events": ["Two girls stretch their arms out.", "The girl behind runs up beside them.", "The girl behind grabs their arms with both hands.", "The girl behind stretches her legs out.", "The girl behind spins around in the air.", "The girl behind lands on the ground.", "They step back a few paces together.", "They laugh joyously.", "The girl on the right gives a thumbs-up to the camera.", "The three girls stand together."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 567}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three people wearing blue short sleeves came out through the door one after another, jumping up to touch an object tied to the door frame with their heads, and then walked out of the frame. A girl walked under the door frame and looked up, with others following behind her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 568, "extra_info": {"duration": 4.68, "events": ["Three people came out through the door.", "First person jumped to touch the object.", "Second person jumped to touch the object.", "Third person jumped to touch the object.", "Three people walked out of the frame.", "A girl walked under the door frame.", "Girl looked up at the object.", "Others followed the girl behind her."], "n_events": 8, "n_shots": 1, "n_subjects": 3, "old_duration": 5.53, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/168_no_audio.mp4"}, "vid": 568}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The girls, dressed in green coats, black skirts, and wearing white gloves, performed a routine with their guns. First, they held the guns with both hands at their left side, muzzles pointing straight up. Then, they touched their mouths with their left hands, stretched forward, tilted the guns to the right and then straightened them, rotated, held them horizontally, and rotated again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 569, "extra_info": {"duration": 5.83, "events": ["The girls held guns with both hands at left side.", "Guns' muzzles pointed straight up.", "The girls touched their mouths with left hands.", "The girls stretched forward.", "The girls tilted guns to the right.", "The girls straightened the guns.", "The girls rotated.", "The girls held guns horizontally.", "The girls rotated again."], "n_events": 9, "n_shots": 1, "n_subjects": 2, "old_duration": 5.83, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/169.mp4"}, "vid": 569}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man alternates his hands to grasp objects as he climbs up the wall, his body swaying as it moves back and forth from left to right. The camera follows the man's movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 570, "extra_info": {"duration": 5.13, "events": ["The man alternates hands to grasp objects.", "The man climbs up the wall.", "The man's body sways back and forth.", "The man moves from left to right.", "The camera follows the man's movements."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 570}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The child wearing a blue coat opens the wooden fence, then turns around, extends their left hand to push the door open, walks forward, glances at the camera then looks away, as a yellow ox walks out from the cattle pen. The camera follows the movement of the child. The background is the cattle pen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 571, "extra_info": {"duration": 10.23, "events": ["Child opens the wooden fence", "Child turns around", "Child extends left hand to push door", "Child walks forward", "Child glances at the camera", "Child looks away", "Yellow ox walks out from cattle pen"], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 571}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man wearing a shirt stands with his right hand grasping a stick stretched forward. Another man, clad in a black coat, glances at a stool beside him, then extends his right hand to move the stool away, placing it behind himself to sit down. The man in the shirt shakes his arms, leans his body backward, and falls into the water pool behind him, causing water to splash in all directions. The camera follows the movement as the man in the black coat turns his head to look at him. The background features a pond surrounded by a stand of trees.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 572, "extra_info": {"duration": 5.33, "events": ["Man grasps stick with right hand.", "Man in black coat glances at stool.", "Man in black coat moves stool away.", "Man in black coat sits down.", "Man in shirt shakes arms.", "Man in shirt leans body backward.", "Man in shirt falls into water pool.", "Water splashes in all directions.", "Man in black coat turns head to look."], "n_events": 9, "n_shots": 1, "n_subjects": 2, "old_duration": 5.33, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/172.mp4"}, "vid": 572}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person wearing an orange helmet and dressed in a black short-sleeve shirt rides a motorcycle towards the mountain top. The camera follows the movement; dust flies along the way. As they are about to reach the summit, the person stands up from the motorcycle, plants their feet on the ground, then turns around, swings one leg over the bike, and continues to rev the engine, with dust enveloping them. The background is a mountain.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 573, "extra_info": {"duration": 10.6, "events": ["Person rides motorcycle towards the mountain top.", "Motorcycle stirs up dust along the way.", "Person stands up from the motorcycle.", "Person plants feet on the ground.", "Person turns around.", "Person swings leg over the bike.", "Person continues to rev the motorcycle's engine.", "Dust envelops the person."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 573}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Background of a chicken coop. A group of chickens are walking around a boy wearing a blue short-sleeved shirt. The boy is bending over with a basin in his hands, scooping out a basin of corn from the bucket in front of him, then turns around and jumps up, swinging his arms, scattering the corn around. The chicks are startled by the falling corn and run away in all directions.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 574, "extra_info": {"duration": 4.03, "events": ["A group of chickens walk around.", "A boy bends over with a basin.", "The boy scoops out corn from a bucket.", "The boy turns around.", "The boy jumps up.", "The boy swings his arms.", "The boy scatters the corn around.", "The chicks are startled by the corn.", "The chicks run away in all directions."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 574}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A green parrot walks forward, bends down to pick up a bottle cap, steps on the trash can beside it, and the lid of the can is opened. It tosses the bottle cap into the can, moves its foot away, and the lid closes automatically. Then it picks up another cap nearby and repeats the same action. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 575, "extra_info": {"duration": 6.43, "events": ["A green parrot walks forward.", "Parrot bends down.", "Parrot picks up a bottle cap.", "Parrot steps on the trash can.", "Trash can lid opens.", "Parrot tosses the bottle cap into the can.", "Parrot moves its foot away.", "Trash can lid closes automatically.", "Parrot picks up another bottle cap.", "Parrot repeats the toss action."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 575}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person threw a watermelon into the gaping mouth of a hippopotamus, which then crushed the watermelon. The camera panned downward, showing the watermelon juice flowing out. The background featured a pool with a forest behind it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 576, "extra_info": {"duration": 6.13, "events": ["Person threw a watermelon into the mouth of a hippopotamus", "Hippopotamus crushed the watermelon", "Watermelon juice flowed out", "Camera panned downward"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 576}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man opens the trunk of the car and turns to run forward; three puppies jump out and run ahead. The camera follows the movement; the puppies look back as they continue to run forward. The man walks on, with one puppy running up behind him, and together they look to their right. The background is a mountain road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 577, "extra_info": {"duration": 7.18, "events": ["The man opens the trunk of the car", "The man turns to run forward", "Three puppies jump out of the trunk", "The puppies run ahead", "The puppies look back while running", "The man walks on", "One puppy runs up behind the man", "The man and one puppy look to their right"], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 577}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man opens the car door, stretches out his right leg, jumps into the water, lies forward, then stands up straight, holding mud in both hands. In a rush, he looks around, and finally, he looks down. The background is a road amidst the mountains and forests.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 578, "extra_info": {"duration": 6.48, "events": ["The man opens the car door", "The man stretches out his right leg", "The man jumps into the water", "The man lies forward in the water", "The man stands up straight", "The man holds mud in both hands", "The man looks around in a rush", "The man looks down"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 578}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three people are riding on a motorcycle, with the two people at the sides moving forward. The woman is holding the front of the motorcycle and suddenly stands up and turns around. The camera pans to the left, and the woman sets down the motorcycle, riding forward with her feet on the motorcycle. Then she stretches her legs out straight and leans back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 579, "extra_info": {"duration": 6.17, "events": ["Three people are riding on a motorcycle.", "Two people at the sides move forward.", "The woman holds the front of the motorcycle.", "The woman suddenly stands up.", "The woman turns around.", "The woman sets down the motorcycle.", "The woman rides forward with her feet on the motorcycle.", "The woman stretches her legs out straight.", "The woman leans back."], "n_events": 9, "n_shots": 1, "n_subjects": 3, "old_duration": 6.17, "old_video_file": "/mnt/bn/videonaslq/videos/test_videos/DREAM-1k_fixed/179.mp4"}, "vid": 579}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A high-rise building caught fire, with black smoke billowing around. The phrases \"119 Training for Combat\" and \"Drone Firefighting\" were written on the building. A red drone, carrying two hoses on its left and bottom sides, was spraying water into the building to extinguish the fire.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 580, "extra_info": {"duration": 10.0, "events": ["High-rise building caught fire.", "Black smoke billowed around high-rise building.", "Phrases appeared on high-rise building.", "Red drone carried two hoses.", "Red drone sprayed water into high-rise building.", "Water extinguished the fire in high-rise building."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 580}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four cars drive down the hillside in succession. In the middle of the scene, a large hammer is spinning, knocking the first car out of the frame, while the other three continue moving forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 581, "extra_info": {"duration": 6.13, "events": ["Four cars drive down the hillside.", "A large hammer spins in the middle of the scene.", "The hammer knocks the first car out of the frame.", "The second car continues moving forward.", "The third car continues moving forward.", "The fourth car continues moving forward."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 581}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an outdoor playground. A group of people are standing face to face on a suspension bridge, swaying it. A girl in black falls from the bridge. Afterwards, a man in a black coat jumps down to check on the girl's condition, followed by another man wearing a hat who also jumps down. The two men walk over to the girl, and the first man reaches out to pull the girl up, while the group on the suspension bridge watches them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 582, "extra_info": {"duration": 10.2, "events": ["Group of people sway the suspension bridge", "Girl in black falls from the bridge", "Man in black coat jumps down from bridge", "Man wearing hat jumps down from bridge", "Two men walk over to the fallen girl", "First man reaches out to pull the girl up", "Group on the bridge watches the situation"], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 582}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a kindergarten playground where the children are playing a game of jumping over bamboo poles. Four little kids are standing with their hands on their hips, stretching out their right leg then retracting it, and then jumping over the bamboo pole with alternating legs to the left. Another child sits behind, holding onto the bamboo pole with hands opening and closing it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 583, "extra_info": {"duration": 3.6, "events": ["Children play in a playground.", "Kids stretch out their right legs.", "Kids retract their right legs.", "Kids jump over a bamboo pole.", "Children alternate legs for jumping.", "A child sits behind the pole.", "The sitting child holds onto the bamboo pole.", "The sitting child opens and closes hands on the pole."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 583}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an indoor volleyball court. A woman in a turquoise short-sleeved shirt steps forward with hands behind her and then leaps into the air, the camera follows her movement. Beside her, a woman in a yellow short-sleeved shirt jumps up and reaches out with both hands to hit the falling volleyball into the air. The woman in turquoise leaps, stretching out her right hand to hit the volleyball over the net to the opposite side, where a woman in a red short-sleeved shirt watches the approaching ball.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 584, "extra_info": {"duration": 4.7, "events": ["Woman in turquoise shirt steps forward.", "Woman in turquoise shirt leaps into the air.", "Woman in yellow shirt jumps up.", "Woman in yellow shirt reaches out to hit volleyball.", "Woman in turquoise shirt stretches out right hand.", "Volleyball is hit over the net.", "Woman in red shirt watches approaching ball."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 584}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pile of duck corpses is dumped from a basket into a pit in the ground. As the camera zooms out, a girl clad in blue sets down the basket, crouches down to press on the bodies of the ducks, and then pushes the adjacent soil over to bury them, while a small dog behind her turns its head to look over. The background features a woodland mountain landscape.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 585, "extra_info": {"duration": 7.83, "events": ["A pile of duck corpses is dumped into a pit.", "A girl sets down the basket.", "The girl crouches down.", "The girl presses on the duck bodies.", "The girl pushes soil over the duck corpses.", "A small dog turns its head."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 585}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is an interior undergoing renovation. The man is wearing a mask and dressed in a green short-sleeved shirt, tightly gripping a sledgehammer and repeatedly smashing the wall. The wall cracks, and bricks and plaster fall off one after another.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 586, "extra_info": {"duration": 6.0, "events": ["Interior is undergoing renovation.", "Man wears a mask.", "Man dresses in a green short-sleeved shirt.", "Man tightly grips a sledgehammer.", "Man repeatedly smashes the wall.", "Wall cracks.", "Bricks fall off.", "Plaster falls off."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 586}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a courtyard, surrounded by a small grove of trees and a house. The girl is running in front, occasionally looking back, with a little donkey following her as they circle around a pillar in the middle. The camera cuts to subtitles and images appearing on the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 587, "extra_info": {"duration": 7.72, "events": ["The girl runs in the courtyard", "The girl occasionally looks back", "A little donkey follows the girl", "They circle around a pillar"], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 587}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is wearing a red hat and a black coat. He kneels on one knee and firmly grips an axe, chopping at the tree in front of him. Then, he looks up, stands up, and steps back as the tree falls to the ground. A video appears in the bottom right corner of the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 588, "extra_info": {"duration": 9.6, "events": ["The man kneels on one knee.", "The man grips an axe.", "The man chops at the tree.", "The man looks up.", "The man stands up.", "The man steps back.", "The tree falls to the ground."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 588}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a peach grove. The man is dressed in a blue short-sleeved shirt, tightly gripping a pair of tongs as he picks a peach from the tree. He turns around and places it into a basket on the ground. The camera switches to show the man looking upwards, reaching to take a peach off the tongs, and then turning towards the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 589, "extra_info": {"duration": 3.98, "events": ["Man grips a pair of tongs", "Man picks a peach from the tree", "Man turns around", "Man places the peach into a basket", "Man looks upwards", "Man reaches to take a peach", "Man takes a peach off the tongs", "Man turns towards the camera"], "n_events": 8, "n_shots": 2, "n_subjects": 1}, "vid": 589}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person holds a strawberry, subtitles appear and then disappear on the screen. A small dog turns around and runs into the dog crate, subtitles appear and then disappear on the screen. The little dog turns around, extends its paw to close the door, and then sits down, subtitles appear and then disappear on the screen. The person brings the strawberry close to the dog's nose, and it takes a sniff. The background is an indoor living room. The camera follows the movement of the person's hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 590, "extra_info": {"duration": 8.63, "events": ["Person holds a strawberry.", "Subtitles appear and disappear.", "Small dog turns around.", "Small dog runs into the crate.", "Subtitles appear and disappear.", "Small dog extends paw to close the door.", "Small dog sits down.", "Subtitles appear and disappear.", "Person brings strawberry close to dog's nose.", "Dog takes a sniff."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 590}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is inside a bowling alley. A woman wearing yellow leggings and a black short-sleeved shirt stands with her back to the camera, holding a bowling ball with both hands, leaning forward and stepping out to roll the ball, her right hand swinging. The bowling ball rolls forward and knocks down the white pins ahead. The woman straightens up and watches.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 591, "extra_info": {"duration": 5.43, "events": ["The woman leans forward", "The woman steps out", "The woman's right hand swings", "The bowling ball rolls forward", "The bowling ball knocks down pins", "The woman straightens up", "The woman watches"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 591}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a path in a park. Two little monkeys are running forward. A person holding food appears in front of the camera. The little monkey wearing yellow clothes runs over, grabs the food with its mouth, and turns to leave. The little monkey in red clothes didn't get any and reaches out to grab the clothes of the other monkey, trying to snatch the food from its mouth. A person holding food appears again in front of the camera. They turn to face the camera. The little monkey wearing pink clothes successfully snatches the food and eats with it in its mouth, while the other monkey is eating food in front of the camera. It looks back and then the food is snatched away by another monkey beside it and eaten up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 592, "extra_info": {"duration": 12.8, "events": ["Two little monkeys run forward on a path.", "Monkey in yellow clothes grabs food with mouth.", "Monkey in yellow clothes turns to leave.", "Monkey in red clothes tries to snatch food.", "Person holding food reappears.", "Monkey in pink clothes snatches the food.", "Monkey in pink clothes eats the food.", "Monkey in front of camera eats food.", "Monkey looks back.", "Another monkey snatches and eats the food."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 592}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a yard. A man wearing an apron is holding a basin of water, splashing it toward the security guard sitting and sleeping next to him. The security guard is startled awake, stands up and looks down, then turns to the right, extends his right hand forward, and says something. Another man wearing a green hat, carrying a bucket, comes over and says something to the security guard. After sitting down, the security guard looks down at his water-soaked clothes, shakes his body, turns his head to look at the man and says something, then looks down at his own pants. The man with the green hat lifts the bucket and splashes the security guard with water. The security guard raises both hands, shaking his body.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 593, "extra_info": {"duration": 8.23, "events": ["Man splashes water on security guard", "Security guard stands up", "Security guard looks down", "Security guard turns right and extends hand", "Man in green hat approaches security guard", "Security guard sits down", "Security guard shakes body", "Security guard turns head towards man", "Man with green hat splashes security guard", "Security guard raises both hands"], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 593}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The yellow little bird picks up a small ring and walks to the left, standing on a wooden board, it slips the small ring in its beak onto the pole, then turns around and walks to the right, picks up another small ring and returns to the pole to slip it on. The bird repeats the actions above, and the video ends with the bird approaching the pole with the last ring in its beak.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 594, "extra_info": {"duration": 10.8, "events": ["Yellow bird picks up a small ring", "Bird walks to the left", "Bird slips ring onto the pole", "Bird turns around", "Bird walks to the right", "Bird picks up another small ring", "Bird returns to the pole", "Bird slips second ring on", "Bird repeats actions", "Bird approaches pole with last ring"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 594}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a small path at the entrance of a residential house. The man is holding dolls in both hands, looking down, he takes a step forward and steps on an object. His eyes move, then he looks downward, discards the doll in his right hand, and takes a step back. He repeats the stepping forward and stepping back actions, while also throwing away the doll in his left hand, and takes off his black coat. The final scene shows the man bending over with both hands open, standing on one foot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 595, "extra_info": {"duration": 8.4, "events": ["Man holds dolls in both hands.", "Man takes a step forward.", "Man steps on an object.", "Man's eyes move.", "Man looks downward.", "Man discards doll from his right hand.", "Man takes a step back.", "Man throws away doll from his left hand.", "Man takes off his black coat.", "Man stands on one foot bending over."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 595}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The backdrop is inside a soccer field. A woman dressed in a black coat is running forward, while a soccer player on the right, wearing a red short-sleeved shirt, turns around. As he bends over with one hand supporting himself on the ground, his right leg stretches out toward the woman. She falls forward and rolls on the ground. The soccer player turns his head to look at the woman, and the two teammates on the left also turn to look at her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 596, "extra_info": {"duration": 4.3, "events": ["A woman runs forward on the soccer field.", "A soccer player turns around.", "The soccer player bends over.", "The soccer player's right leg stretches out.", "The woman falls forward.", "The woman rolls on the ground.", "The soccer player looks at the woman.", "Two teammates turn to look at the woman."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 596}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A monkey sits in front of the camera, with its hands placed on a plate, looking upwards. Some colorful candies are poured in from the right side. The monkey's gaze follows the movement of the candies, then it reaches out to grab the candies and puts them into its mouth to eat, occasionally looking up. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 597, "extra_info": {"duration": 4.35, "events": ["Monkey sits in front of the camera.", "Monkey places hands on the plate.", "Colorful candies are poured onto the plate.", "Monkey's gaze follows the movement of the candies.", "Monkey reaches out to grab the candies.", "Monkey puts candies into its mouth.", "Monkey eats the candies.", "Monkey occasionally looks up."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 597}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is the interior of a taekwondo dojo. A woman in a blue short-sleeved shirt stretches her hands forward, shouts loudly, and kicks out with her right leg towards the balloons in front of her. The balloons are kicked and burst one by one. After retracting her leg, the woman stands up and repeats the previous action, kicking towards higher balloons. As the balloons burst, the woman stands again after lowering her right leg. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 598, "extra_info": {"duration": 6.33, "events": ["Woman stretches hands forward", "Woman shouts loudly", "Woman kicks right leg out", "Balloons burst", "Woman retracts her leg", "Woman stands up", "Woman kicks towards higher balloons", "Higher balloons burst", "Woman lowers her right leg", "Camera remains fixed"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 598}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a water channel at the foot of a mountain. A sheep walks out of the channel, shakes its head, and moves forward. The man retracts his stick, placing it on another sheep, holding its neck as he guides it forward, while several other men are busy within the sheep pen. The camera angle remains fixed and unchanged.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 599, "extra_info": {"duration": 5.83, "events": ["A sheep walks out of the water channel.", "Sheep shakes its head.", "Sheep moves forward.", "The man retracts his stick.", "Man places the stick on another sheep.", "Man holds the sheep's neck.", "Man guides the sheep forward.", "Several men are busy within the sheep pen."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 599}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a kindergarten corridor. Four children and a long-haired woman are sitting against the wall. Another woman with short hair, wearing a red short-sleeved top and black sports pants, is using both hands to pull the children up from their seats in turns. After they jump up, they sit back down. Finally, the woman starts to pull on the long-haired woman who is still seated. A group of children run out from the classroom, with a little girl in front leading the charge, grabbing the short-haired woman's clothing to help. In the end, the long-haired woman is successfully pulled up, and the video ends with her being lifted and jumping into the air.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/vertical_video", "task": "video/caption", "idx": 600, "extra_info": {"duration": 15.2, "events": ["Children and woman sit against the wall.", "Woman in red pulls children up in turns.", "Children jump up.", "Children sit back down.", "Woman in red attempts to pull long-haired woman up.", "Group of children run out from the classroom.", "Little girl leads group, grabbing woman in red's clothing.", "Children help to pull long-haired woman up.", "Long-haired woman gets pulled up successfully.", "Long-haired woman jumps into the air."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 600}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four people are gathered around a wooden table laden with books. The person seated at the center of the frame is operating a computer, then rotates the laptop to show the screen to the others. Holding a pen, they point at something on the screen and on the paper in front of them. The person on the right writes something on the paper, while the two people on the left raise and lower their hands. The camera angle remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 601, "extra_info": {"duration": 18.1, "events": ["Four people gather around a wooden table.", "Person at center operates a computer.", "Person at center rotates the laptop.", "Person at center shows screen to others.", "Person at center points at the screen with a pen.", "Person at center points at the paper with a pen.", "Person on the right writes on the paper.", "Person on the left raises hand.", "Person on the left lowers hand.", "Another person on the left raises hand."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 601}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera remains fixed. The background is a cafe. Three people are seated around a coffee table, a man dressed in a suit holding a pen and occasionally looking up to exchange words with the other two while pointing at a chart. The woman lifts her coffee cup to drink and then sets it down, the man in yellow clothes stretches out his hand to point at the chart, the woman turns to look at the man in yellow, then points at the chart and says something, setting down her coffee after finishing it, and the man in yellow nods in response.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 602, "extra_info": {"duration": 28.18, "events": ["Man in suit holds pen", "Man in suit exchanges words with others", "Man in suit points at chart", "Woman lifts coffee cup", "Woman sets down coffee cup", "Man in yellow clothes stretches out hand", "Man in yellow points at chart", "Woman turns to man in yellow", "Woman points at chart and speaks", "Man in yellow nods in response"], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 602}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The setting is a bar. A waiter walks towards the camera, places down two glasses of drinks, then turns and leaves. The two people sitting opposite each other at the table reach out to pick up the glasses, drink them down, and then put them back. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 603, "extra_info": {"duration": 24.72, "events": ["Waiter walks towards the camera", "Waiter places down two glasses", "Waiter turns", "Waiter leaves", "First person reaches out to pick up a glass", "Second person reaches out to pick up a glass", "First person drinks the drink", "Second person drinks the drink", "First person puts the glass back", "Second person puts the glass back"], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 603}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person opens the cover of a book, places a green twig inside, and then closes the book. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 604, "extra_info": {"duration": 9.16, "events": ["Person opens the book cover.", "Person places a green twig inside the book.", "Person closes the book.", "Camera stays fixed."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 604}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a grassy field. Many bubbles are drifting in from the left, and the girl on the right reaches out to catch them. The girl on the left spins around, and then the two of them grasp hands tightly and happily spin around on the grass. Afterwards, they continue to catch bubbles. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 605, "extra_info": {"duration": 7.7, "events": ["Bubbles drift in from the left", "Girl on the right reaches out to catch bubbles", "Girl on the left spins around", "Both girls grasp hands tightly", "Girls happily spin around on the grass", "Girls continue to catch bubbles"], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 605}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures footage inside a clear swimming pool, with the surface of the water gently rippling. On the right side of the frame, a woman's legs enter the water vertically from above. After bobbing up and down in the water twice, the woman turns around, pushes off the edge of the pool, and emerges from the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 606, "extra_info": {"duration": 10.42, "events": ["Camera captures footage inside a swimming pool", "Water surface gently ripples", "Woman's legs enter the water vertically", "Woman's legs bob up in the water", "Woman's legs bob down in the water", "Woman turns around in the water", "Woman pushes off from the pool's edge", "Woman emerges from the water"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 606}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Against the backdrop of the starry sky, light radiates outward from the center. Shortly thereafter, fragments of letters fly from off-screen to the center of the screen to form the name of the movie. After the movie title briefly and slowly enlarges forward, it disintegrates back into fragments and flies off the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 607, "extra_info": {"duration": 10.03, "events": ["Light radiates outward from the center.", "Fragments of letters fly from off-screen to the center.", "The movie title forms from the flying fragments.", "The movie title enlarges forward.", "The movie title disintegrates into fragments.", "Fragments fly off the screen."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 607}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a chocolate cake, three rows of candles were inserted. A pair of clenched fists moved slightly backward, then the candles were blown out. Following this, the pair of clenched hands opened up and began to clap, with another pair faintly visible clapping on the left side of the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 608, "extra_info": {"duration": 8.36, "events": ["Three rows of candles were inserted into a chocolate cake.", "A pair of clenched fists moved slightly backward.", "The candles were blown out.", "The pair of clenched hands opened up.", "The opened hands began to clap.", "Another pair of hands started clapping on the left side."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 608}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the screen, there is a counter with several boxes of popcorn on it. Behind the counter, a person's hand manipulates an electronic device a few times, and then places the device on top of the counter. In front of the counter, on the left side, a person takes out a mobile phone to scan the electronic device, while on the right side, another person places their right hand on the counter.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 609, "extra_info": {"duration": 6.2, "events": ["A counter displays several boxes of popcorn.", "A person manipulates an electronic device behind the counter.", "The person places the electronic device on the counter.", "A person takes out a mobile phone in front of the counter.", "The person scans the electronic device with the mobile phone.", "Another person places their right hand on the counter."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 609}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room, a child scoops a shovel of soil from a white basin and pours it into the flower pot in front of them, then uses the shovel to level the soil.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 610, "extra_info": {"duration": 8.32, "events": ["Child scoops soil with a shovel", "Child pours soil into the flower pot", "Child uses shovel to level the soil"], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 610}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background features a blurry silhouette of a person. After waving goodbye, the figure turns and leaves to the left, lightly pushing the door with their left hand. A hand lightly sways in front of the camera, and the man reaches forward to close the door before leaving. The camera then moves to the right, ending with a close-up of the room's door lock.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 611, "extra_info": {"duration": 8.68, "events": ["Person waves goodbye", "Person turns", "Person leaves to the left", "Person pushes door with left hand", "Hand sways in front of camera", "Man reaches forward", "Man closes door", "Camera moves to the right", "Close-up of door lock"], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 611}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people dressed in white lab coats are standing in front of a sink, rubbing their hands as they wash. They then reach out to turn off the faucet, and the person closer to the camera lifts their hands up. The camera angle remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 612, "extra_info": {"duration": 7.62, "events": ["Two people stand in front of a sink", "People dressed in white lab coats wash hands", "They rub their hands together", "People reach out to turn off the faucet", "Person closer to the camera lifts hands up"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 612}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a soccer field, and in the distance, yellow and blue chairs can be seen, as well as a patch of trees. Four men dressed in white short sleeves run into the soccer field with their backs to the camera, then stand facing each other in pairs. The man on the right kicks the soccer ball to the left, kicks it back, and then kicks it to the man beside him. The camera follows their movements.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 613, "extra_info": {"duration": 8.32, "events": ["Four men run into the soccer field", "Pairs of men stand facing each other", "Man on the right kicks soccer ball to the left", "Receiver kicks ball back to the man on the right", "Man on the right kicks ball to man beside him"], "n_events": 5, "n_shots": 1, "n_subjects": 4}, "vid": 613}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a gym, with several treadmills around. A person in a white short-sleeved shirt is holding a spray bottle in their left hand, spraying cleaner on the handle and screen of a treadmill, while wiping with a cloth in their right hand. The camera remains fixed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 614, "extra_info": {"duration": 12.63, "events": ["Person sprays cleaner on treadmill handle.", "Person sprays cleaner on treadmill screen.", "Person wipes handle with a cloth.", "Person wipes screen with a cloth."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 614}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The background is a bright, tidy living room. A woman in a blue dress is half-kneeling on the ground, with her right hand stretched out. A puppy is walking towards the left, the camera pans to the right, and a baby walks towards the camera. Then the baby moves to the front of a man in a green short-sleeved shirt, who picks up the baby into his arms.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 615, "extra_info": {"duration": 10.21, "events": ["Woman half-kneels on the ground.", "Woman stretches out her right hand.", "Puppy walks towards the left.", "Baby walks towards the camera.", "Baby moves to the front of the man.", "Man picks up the baby into his arms."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 615}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a small hillside, a man rides a mountain bike downhill from left to right. Then he turns his body, the mountain bike skids to a stop, the man plants his left foot on the hillside and looks back toward the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 616, "extra_info": {"duration": 5.29, "events": ["A man rides a mountain bike downhill.", "The mountain bike skids to a stop.", "The man turns his body.", "The man plants his left foot on the hillside.", "The man looks back toward the camera."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 616}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Against a blurred background, a man looks ahead, then puts on his helmet, fastens the chin strap, straightens his collar, and adjusts the helmet again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 617, "extra_info": {"duration": 10.26, "events": ["Man looks ahead", "Man puts on helmet", "Man fastens chin strap", "Man straightens collar", "Man adjusts helmet"], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 617}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people, each holding a bucket of popcorn, face a character in a yellow outfit who is carefully inspecting the tickets. Afterward, they tear the tickets with both hands, hand the stubs to the person on the right, and raise a hand toward the left. Then the two individuals turn and walk to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 618, "extra_info": {"duration": 11.72, "events": ["Two people hold buckets of popcorn.", "Character in yellow outfit inspects tickets.", "Character tears tickets with both hands.", "Character hands stubs to person on the right.", "Character raises hand toward the left.", "Two individuals turn.", "Two individuals walk to the left."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 618}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a barren loess land with no grass or trees, a leopard is running in pursuit of a quickly moving small object. They pass by another leopard walking in the opposite direction. Upon catching up with the small object, the chasing leopard pounces towards it, kicking up a cloud of dust, but the small object still manages to escape. The leopard watches the direction in which the small object fled and then slows to a gentle run forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 619, "extra_info": {"duration": 16.92, "events": ["A leopard runs after a small object.", "A small object moves quickly.", "Another leopard walks in the opposite direction.", "The chasing leopard pounces towards the small object.", "A cloud of dust rises.", "The small object escapes.", "The leopard watches the small object's escape direction.", "The leopard slows to a gentle run."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 619}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person wearing a belt slices an avocado and arranges it on chocolate toast, then pinches the crumbs from the bowl in front, sprinkling them on the avocado. Afterwards, they take a cherry tomato, cut it into four pieces, and also place it on top of the avocado.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 620, "extra_info": {"duration": 37.92, "events": ["Person slices an avocado", "Person arranges avocado on chocolate toast", "Person pinches crumbs from the bowl", "Person sprinkles crumbs on the avocado", "Person takes a cherry tomato", "Person cuts cherry tomato into four pieces", "Person places tomato pieces on the avocado"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 620}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hummingbird rapidly flutters its wings, hovering in the air. Then, the hummingbird flies in front of a flower, extends its beak into the stamen, and sips the nectar before flying out backward. Following that, it flies in front of the flower again and sips the nectar. This process is repeated three times before the hummingbird finally flies upwards and away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 621, "extra_info": {"duration": 13.41, "events": ["A hummingbird flutters its wings.", "The hummingbird hovers in the air.", "Hummingbird extends beak into the flower's stamen.", "Hummingbird sips nectar from the flower.", "The hummingbird flies out backward.", "Hummingbird approaches the flower again.", "Hummingbird repeats nectar sipping.", "The process is repeated three times.", "Hummingbird flies upwards.", "Hummingbird flies away."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 621}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the stable, a horse is bowing its head to eat the hay on the ground. The camera begins to show the movements of the horse's hooves and muzzle, then gradually pans up to reveal the entire head and mane of the horse. It then pans back down to the horse's legs and mouth. Finally, after the horse finishes eating the hay, it lifts its head away from the screen, leaving only the shot of the horse's legs as it ends.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 622, "extra_info": {"duration": 21.88, "events": ["Horse bows head to eat hay.", "Horse's hooves move.", "Horse's muzzle moves.", "Horse's head and mane become visible.", "Horse's legs come into view.", "Horse's mouth eats hay.", "Horse lifts head away from ground.", "Horse's legs remain in the shot."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 622}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a lush green field, a lady wearing long sleeves intercepts a red ball rolling to the right. She then runs to the right side of the ball, gently nudges it forward, and then kicks it to the left. A lady wearing short sleeves runs over from the left side, meets the ball, and with one kick, sends it flying.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 623, "extra_info": {"duration": 6.47, "events": ["A red ball rolls to the right.", "Lady intercepts the red ball.", "Lady runs to the right side of the red ball.", "Lady nudges the red ball forward.", "Lady kicks the red ball to the left.", "Another lady runs over from the left side.", "Another lady meets the red ball.", "Another lady kicks the red ball, sending it flying."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 623}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the wooden floor, a person sits cross-legged in front of a square cushion, first picking up headphones and putting them on, then picking up the drink beside the square cushion and taking a sip through the straw before placing it back in its original spot. Lastly, the scene concludes with this person bringing both hands together in a gesture of salute or prayer.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 624, "extra_info": {"duration": 13.8, "events": ["A person sits cross-legged on the wooden floor.", "The person picks up headphones.", "The person puts on the headphones.", "The person picks up a drink.", "The person takes a sip through the straw.", "The person places the drink back.", "The person brings hands together in a gesture."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 624}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Several people holding weapons passed by the corner of a house, one of them holding a long rifle provided cover at the window, then two others holding rifles approached the front door. After that, a person wearing a hat, holding a pistol, approached the door from another direction. The camera panned left and right around the three individuals in front of the door. Then, someone inside the house opened the door, and the three at the entrance quickly rushed into the house. The door slowly closed, and a hand reached out to block it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 625, "extra_info": {"duration": 22.27, "events": ["Several people holding weapons passed by a house corner.", "One person provided cover at the window with a long rifle.", "Two individuals approached the front door with rifles.", "A person wearing a hat approached the door with a pistol.", "The camera panned around the three individuals at the door.", "Someone inside the house opened the door.", "Three individuals quickly rushed into the house.", "The door slowly closed.", "A hand reached out to block the door."], "n_events": 9, "n_shots": 1, "n_subjects": 5}, "vid": 625}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the center of a black background screen, there is a rounded rectangle with a downward triangle. At its center is a white heart shape and a number. The number starts at 1 and increases one by one: 2, 3, 4, 5, 6... then quickly increases to two-digit numbers, three-digit numbers, until it reaches five digits, finally stopping at 99999. As the number of digits increases, the length of the rounded rectangle also expands accordingly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 626, "extra_info": {"duration": 12.0, "events": ["The number starts at 1.", "The number increases to 2.", "The number increments to 3.", "The number counts up to 4.", "The number ascends to 5.", "The number progresses to 6.", "The number quickly increases to two-digit numbers.", "The number rapidly climbs to three-digit numbers.", "The number swiftly jumps to five-digit numbers.", "The number stops at 99999."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 626}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features several quick-cut shots, displaying various patterns and numbers, until a shot reveals \"HEAD\", followed by \"Image Start\", and then a countdown number appears inside a circle on the screen. Concurrently, a radius begins at the 12 o'clock position, sweeping clockwise around the entire circle. When the countdown number reaches 2, the screen turns black, and remains so until the video ends.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 627, "extra_info": {"duration": 12.96, "events": ["Video displays various patterns.", "Video displays various numbers.", "Shot reveals the word 'HEAD'.", "'Image Start' appears on screen.", "Countdown begins with a number inside a circle.", "A radius sweeps clockwise around the circle.", "Countdown number decreases.", "Countdown reaches number 2.", "Screen turns black.", "Video ends with black screen."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 627}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A fixed camera captures the activities of a day in the square in front of it, with numerous individuals engaging in various activities. The passage of time on camera hastens, and the activities of the people blur rapidly. The camera starts in the morning, with the sunlight gradually brightening and people's activities steadily increasing. By evening, the sunlight fades, street lights come on, and a light show begins in the distance. People's activities reach a climax, and eventually, everyone gradually disperses.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 628, "extra_info": {"duration": 49.32, "events": ["Camera captures a day's activities in the square.", "People start their morning routines.", "Sunlight brightens the environment.", "Activities of individuals increase.", "Evening arrives and sunlight fades.", "Street lights come on.", "A light show begins in the distance.", "People's activities reach a climax.", "Individuals gradually disperse."], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 628}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera focuses on the side profile of a man's nose and mouth; a hand holding a comb runs through the man's beard, and then a trimmer is used to meticulously groom the beard on the man's face and around his mouth. Finally, the man says two sentences.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 629, "extra_info": {"duration": 16.68, "events": ["Camera focuses on the side profile of a man's nose and mouth.", "A hand runs a comb through the man's beard.", "A trimmer grooms the beard on the man's face.", "A trimmer grooms around the man's mouth.", "The man says two sentences."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 629}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of feet wearing orange shoes jumped onto a platform, then walked backward down the platform, then jumped back onto the platform, and walked down again, repeating this process until the video ended. The background was an outdoor sports field.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 630, "extra_info": {"duration": 11.88, "events": ["Feet wearing orange shoes jumped onto a platform", "Feet walked backward down the platform", "Feet jumped back onto the platform", "Feet walked down the platform again"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 630}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The traffic light at the side of the street is green, with bicycles passing by in both directions. As the light turns from yellow to red, the flow of bicycles comes to a stop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 631, "extra_info": {"duration": 18.08, "events": ["The traffic light turns green.", "Bicycles pass by in both directions.", "The traffic light turns yellow.", "The traffic light turns red.", "The flow of bicycles comes to a stop."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 631}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A baby is clutching a fork in its hand, trying to pick up fruit from a plate. The baby pokes at the fruit several times without success, until finally spearing a piece and bringing it towards its mouth. Then, the baby prods the plate a few times with the fork. The camera moves upwards to capture the baby's entire body and head, as well as the motion of the baby using the fork to stab downwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 632, "extra_info": {"duration": 12.02, "events": ["A baby clutches a fork.", "The baby tries to pick up fruit.", "The baby pokes at the fruit several times.", "The baby finally spears a piece of fruit.", "The baby brings the speared fruit towards its mouth.", "The baby prods the plate with the fork."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 632}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera focuses on a person's legs; they're wearing a pair of orange running shoes while jogging. Suddenly, he stops, ties the lace of his right shoe, then stands up and continues running. The background is an outdoor sports stadium.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 633, "extra_info": {"duration": 11.5, "events": ["Person jogs in orange running shoes.", "Person stops jogging.", "Person ties lace of right shoe.", "Person stands up.", "Person resumes running."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 633}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is using a pair of pet nail clippers to trim a dog's nails. They first clip the nails on the dog's left front paw, then pick up the right front paw to continue the trimming. The dog turns its head back and forth to cooperate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 634, "extra_info": {"duration": 19.35, "events": ["A person clips a dog's left front paw nails", "The person picks up the dog's right front paw", "The dog turns its head back", "The dog turns its head forth", "The person continues trimming the nails"], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 634}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person dressed in white clothing and wearing a white headscarf is distributing food to people on the left. After the people on the left receive the food, they walk away, and someone on the right brings out new food to give to the person in white clothing. They continue to hand out food to the new arrivals on the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 635, "extra_info": {"duration": 8.71, "events": ["Person in white distributes food to people on the left.", "People on the left receive the food.", "People on the left walk away.", "Someone on the right brings out new food.", "Person in white receives new food.", "Person in white continues to hand out food.", "New arrivals on the left receive food."], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 635}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Around a long table, there are a few people. The table is covered with many food containers filled with food. The two people on the left are wrapping the containers with aluminum foil, while a character in blue clothing on the right is handing over the unwrapped containers to the two people on the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 636, "extra_info": {"duration": 18.49, "events": ["People sit around a long table", "Table is covered with food containers", "People on left wrap containers with aluminum foil", "Character in blue hands over containers", "Food is contained for storage"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 636}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a baking pan, there are several pancakes, and a person is holding tongs, showing the movement of a few pancakes' positions, and then flips the pancakes one by one from left to right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 637, "extra_info": {"duration": 5.42, "events": ["A baking pan holds several pancakes.", "A person grasps tongs.", "The person shows movement of pancakes' positions.", "The person flips the pancakes one by one."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 637}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Several flower pots are laid out on the ground, some of which already have plants in them. A pair of hands holds a handful of potting soil and places it into the middle flower pot. The potting soil is then spread around the plant in the pot and pressed down firmly by hand. Afterward, more potting soil is grabbed and sprinkled evenly into the flower pot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 638, "extra_info": {"duration": 12.01, "events": ["Hands hold a handful of potting soil.", "Hands place potting soil into the middle flower pot.", "Hands spread potting soil around the plant.", "Hands press down the potting soil firmly.", "Hands grab more potting soil.", "Hands sprinkle potting soil evenly into the flower pot."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 638}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans from above a palm tree, rotating to the left while simultaneously descending in height. Upon reaching the beach, the shot changes to move forward, gradually catching up to two ladies wearing bikinis, who are running along the beach.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 639, "extra_info": {"duration": 16.18, "events": ["Camera pans from above a palm tree.", "Camera rotates to the left.", "Camera descends in height.", "Two ladies run along the beach."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 639}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person holds a pair of tweezers, twirling them in a stainless steel plate full of food, rolling up a bundle of food, and then placing the rolled-up food onto a white plate nearby, using their hand to push the food off the tweezers onto the plate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 640, "extra_info": {"duration": 6.88, "events": ["Person holds tweezers.", "Person twirls tweezers in plate.", "Food rolls up into bundle.", "Person places rolled-up food on white plate.", "Person uses hand to push food off tweezers."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 640}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two children are staring at the automatic dough mixer in front of them; the little girl wearing a red and white checked shirt lifts her head and says a few words, while the child next to her keeps watching the rotor turning inside the dough mixer.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 641, "extra_info": {"duration": 20.12, "events": ["Two children stare at the automatic dough mixer.", "The little girl lifts her head.", "The little girl says a few words.", "The child next to the little girl watches the rotor."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 641}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person lifts one corner of a semi-circular pastry with their left hand, draping it over the condiments in the middle. Then, with a knife in their right hand, they gently push the pastry, and lift up another corner to cover all of the condiments, turning the entire pastry into a triangle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 642, "extra_info": {"duration": 9.72, "events": ["A person lifts one corner of the semi-circular pastry.", "The corner drapes over the condiments.", "The person uses a knife to gently push the pastry.", "A person lifts another corner of the pastry.", "The person covers the condiments with the pastry.", "The pastry turns into a triangle."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 642}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hand is lifting the mats layer by layer; the first mat is facing up, the second with its back facing up, and so on alternately. When the hand reaches the fifth mat, the video ends.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 643, "extra_info": {"duration": 6.36, "events": ["A hand lifts the first mat.", "The first mat is facing up.", "The hand lifts the second mat.", "The second mat's back is facing up.", "The hand lifts the third mat.", "The hand lifts the fourth mat.", "The hand reaches the fifth mat.", "The video ends."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 643}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A car passes by; the reflection in the car's windows shows two people walking past. After the car has completely passed by, a parking station on the road is revealed, and the front of a black sedan peeks out from behind a billboard, with other vehicles also parked on the street. At the end of the video, someone walks from right to left on the sidewalk.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 644, "extra_info": {"duration": 9.8, "events": ["A car passes by.", "Two people walk past.", "The car reveals a parking station.", "A black sedan peeks from behind a billboard.", "Other vehicles are parked on the street.", "Someone walks from right to left on the sidewalk."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 644}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera shoots from below the surface of shallow water, capturing a person's foot in the water. A hand releases a fish into the water and, once it enters, the fish quickly twists its body and swims away. The hand that released the fish withdraws from the water and gradually moves away from the surface while a clump of yellow floating matter is stirred and tumbled by the movements of the fish.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 645, "extra_info": {"duration": 6.5600000000000005, "events": ["Camera captures a person's foot in water.", "Hand releases a fish into the water.", "Fish twists its body in the water.", "Fish swims away in the water.", "Hand withdraws from the water.", "Yellow floating matter is stirred.", "Yellow floating matter tumbles by fish's movement."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 645}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A fixed camera shot displays a coast and the distant sunset, with the sun dyeing the sky and clouds in shades of orange. As the sun gradually sets, the clouds slowly ascend, and the shades of the clouds and sky change, while the water's surface, constantly rippling, reflects various colors in the sunlight.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 646, "extra_info": {"duration": 16.08, "events": ["The sun sets on the horizon.", "The sky turns shades of orange.", "Clouds gradually ascend in the sky.", "The clouds change colors.", "Water's surface ripples continuously.", "Reflected sunlight colors the water."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 646}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is in the kitchen peeling an onion. After peeling off a layer of the onion, they throw the peeled skin into the waste pile. Then, they wave their hands about, gesturing something, and afterwards, they pick up a kitchen knife with their right hand and start chopping the onion.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 647, "extra_info": {"duration": 17.25, "events": ["Person peels off a layer of the onion.", "Person throws peeled skin into the waste pile.", "Person waves their hands.", "Person picks up a kitchen knife.", "Person starts chopping the onion."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 647}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A knife blade has a bright red chili pepper skewered on its tip, with a droplet of water poised to fall on either side of the pepper. The camera gradually zooms in, magnifying the knife tip and the pepper slowly. Then, a mist of water showers down from above, covering the surface of the chili with beads of water, making the entire scene even more vivid and tantalizing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 648, "extra_info": {"duration": 8.77, "events": ["A knife blade skewers a bright red chili pepper.", "A droplet of water poises on either side of the pepper.", "The camera zooms in on the knife tip and pepper.", "A mist of water showers down from above.", "Water covers the chili with beads."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 648}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an ice house, the camera slowly pans from left to right, displaying a variety of colorful ice sculptures within the ice house.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 649, "extra_info": {"duration": 9.68, "events": [], "n_events": 0, "n_shots": 1, "n_subjects": 1}, "vid": 649}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A butterfly is fluttering its wings on a yellow flower. In the background, the blurred figure of a maroon butterfly first flies from left to right, then from right to left, and finally, the video ends with the butterfly flying away from the yellow flower.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 650, "extra_info": {"duration": 12.22, "events": ["Butterfly flutters wings on yellow flower", "Maroon butterfly flies from left to right", "Maroon butterfly flies from right to left", "Maroon butterfly flies away from yellow flower"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 650}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a person walking across a sandy area in a long black skirt, barefoot, leaving behind a trail of footprints. The video only displays the lower part of the walker's legs, from the calves down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 651, "extra_info": {"duration": 8.76, "events": ["Person walks across sandy area.", "Person wears long black skirt.", "Person goes barefoot.", "Person leaves trail of footprints."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 651}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A well-built man is wrapping his right hand fingers, palm, and wrist with a black cloth bandage, and his left hand is already wrapped with the bandage.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 652, "extra_info": {"duration": 22.86, "events": ["Man wraps right hand fingers with black cloth bandage.", "Man wraps right hand palm with black cloth bandage.", "Man wraps right hand wrist with black cloth bandage.", "Left hand is already wrapped with black cloth bandage."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 652}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is washing lettuce in the sink, carefully inspecting each leaf as clean water drips from the lettuce into the sink. The person holds the lettuce upright to let it drain, while another person holds a white towel with deep blue stripes, placing it to the left of the sink.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 653, "extra_info": {"duration": 8.08, "events": ["Person washes lettuce in the sink.", "Person inspects each lettuce leaf.", "Clean water drips from lettuce into sink.", "Person holds lettuce upright to drain.", "Another person holds a white towel.", "Second person places towel to left of sink."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 653}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The lionfish is slowly swimming in the water by waving its pectoral fins, with other fish swimming nearby. The background is a shallow sea with corals, where the water surface is gently undulating.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 654, "extra_info": {"duration": 11.34, "events": ["The lionfish is swimming in the water.", "The lionfish is waving its pectoral fins.", "Other fish are swimming nearby.", "The water surface is gently undulating."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 654}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person wearing a blue sweater is sitting behind a white table, fiddling with the vegetables in a bowl with a fork, then picking up a few slices and bringing them to their mouth. A strand of long hair sways in front of their chest as their body moves. The video shows the person's head and chest area.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 655, "extra_info": {"duration": 6.71, "events": ["Person sits behind the white table.", "Person fiddles with vegetables in bowl with fork.", "Person picks up vegetable slices.", "Person brings vegetable slices to mouth.", "Strand of long hair sways in front of chest."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 655}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a grassy field, four people are holding hands in a circle, with adult men and women facing each other, each holding the hands of two little girls, as they turn counterclockwise.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 656, "extra_info": {"duration": 8.13, "events": ["Four people hold hands in a circle.", "Adult men and women face each other.", "Each adult holds the hands of two little girls.", "The group turns counterclockwise."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 656}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Below the black screen, a burst of flame suddenly ignites, with little sparks floating above the fire. The light of the fire gradually dims until it fades away, leaving behind the sparks, which also slowly extinguish, decreasing in number until the screen turns black again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 657, "extra_info": {"duration": 7.03, "events": ["Flame ignites below the black screen", "Sparks float above the fire", "Fire light gradually dims", "Fire light fades away", "Sparks slowly extinguish", "Number of sparks decreases", "Screen turns black again"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 657}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the sky, numerous seagulls flap their wings, trying to hover in place. A person placed some food from their left hand onto their right hand, then stretched the right hand upwards. A nearby seagull flew over, pecked at the food in the hand, and immediately flew away. Other seagulls continued to dance in the air. The person tossed the food from their right hand upwards, and the seagulls swooped toward the food. The video background features a sheer cliff with thick white fog above. Only the upper part of the person's arms and above is captured in the video.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 658, "extra_info": {"duration": 14.15, "events": ["Seagulls flap their wings in the sky.", "Person transfers food from left hand to right hand.", "Person stretches right hand upwards.", "Seagull pecks at food in person's hand.", "Seagull flies away immediately.", "Other seagulls continue hovering in air.", "Person tosses food upwards.", "Seagulls swoop toward the thrown food."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 658}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a grassy soccer field, a pair of feet wearing black soccer shoes approach the ball. The left foot nudges the ball to the right, and then the right foot pulls back to scoop the ball up. After that, the right foot juggles the ball twice, followed by the left and right feet alternately keeping the ball in the air. The video almost exclusively captures the person's shadow and the movement of the ball. Then, the camera gradually moves upward from the feet, sequentially showing the lower legs, knees, thighs clad in black shorts, hands, and the upper body dressed in a blue T-shirt. Meanwhile, the person continues to juggle the ball with alternating left and right feet. Just before the video ends, the individual turns their back to the camera and flicks the ball with the lower leg.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 659, "extra_info": {"duration": 12.22, "events": ["Feet approach the soccer ball.", "Left foot nudges ball to the right.", "Right foot scoops up the ball.", "Right foot juggles the ball twice.", "Left and right feet alternately juggle the ball.", "Camera moves upward from feet to lower legs.", "Camera shows knees, then thighs in black shorts.", "Camera displays hands and upper body in blue T-shirt.", "Person continues to juggle ball with feet.", "Individual turns back and flicks ball with lower leg."], "n_events": 10, "n_shots": 3, "n_subjects": 1}, "vid": 659}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captured the process of mass printing of the US dollar. Initially, uncut sheets of US dollars quickly flashed one by one from the right to the left of the screen. Subsequently, the full sheets moved continuously upwards. Then, the sheets slowly progressed from the top down through a chute in the machine, followed by a slow push to the right. The entire process was operated by machines.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 660, "extra_info": {"duration": 27.19, "events": ["Uncut sheets of dollars flashed from right to left.", "Full sheets moved continuously upwards.", "Sheets progressed top down through a chute.", "Sheets received a slow push to the right.", "Machines operated the entire printing process."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 660}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angles down from above to capture the rippling surface of the water, then pushes forward and gradually lifts, reaching the riverbank to reveal the stones by the river, a stretch of land devoid of vegetation, and shrubbery in the distance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 661, "extra_info": {"duration": 7.09, "events": ["The camera angles down from above.", "The water surface ripples.", "The camera pushes forward.", "The camera gradually lifts.", "The camera reaches the riverbank.", "The riverbank reveals the stones.", "The land devoid of vegetation appears.", "Shrubbery in the distance becomes visible."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 661}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In an open field, two groups of horses gallop from right to left across the plain. The dust raised by their hooves catches the red glow of the setting sun. The camera follows the herd, moving from a position far behind the horses, gradually transitioning to the side of the herd.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 662, "extra_info": {"duration": 28.4, "events": ["Two groups of horses gallop across the plain.", "Horses raise dust with their hooves.", "Dust catches the glow of the setting sun.", "Camera follows the herd of horses.", "Camera moves from behind to the side of the herd."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 662}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the basketball court, in the foreground, a person is continuously dribbling while slightly squatting with bent knees, while in the distance, another person is dribbling towards the basket for a layup.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 663, "extra_info": {"duration": 9.83, "events": ["Person dribbles continuously while squatting.", "Person dribbles towards the basket.", "Person performs a layup."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 663}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the sky, an eagle is soaring. The camera follows it, first moving downward, then moving to the left to keep the eagle in the center of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 664, "extra_info": {"duration": 10.94, "events": ["An eagle is soaring in the sky.", "The camera follows the eagle.", "The camera moves downward.", "The camera moves to the left."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 664}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, three people are sitting at a table drinking. A man and a woman face the camera holding brown bottles, while another person, only their hand visible to the camera, is holding a green bottle. The three of them exchange a few words and then clink their bottles together.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 665, "extra_info": {"duration": 6.08, "events": ["Three people sit at a table.", "A man holds a brown bottle.", "A woman holds a brown bottle.", "Another person's hand holds a green bottle.", "The three people exchange words.", "They clink their bottles together."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 665}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, the lady on the right gestures with both hands while talking to someone beside her, while the gentleman on the left takes notes with a pen. The camera pans to the left, focusing on another lady with brown hair. This lady points at the computer screen in front of her and says a few words, while the man who was taking notes earlier stops writing and also says something.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 666, "extra_info": {"duration": 12.53, "events": ["A lady gestures with both hands while talking.", "A gentleman takes notes with a pen.", "The camera pans to the left.", "A lady with brown hair points at the computer screen.", "The lady with brown hair says a few words.", "The gentleman stops writing.", "The gentleman says something."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 666}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person scoops the chopped cherry tomatoes and cucumber from the cutting board into a glass bowl, with other vegetables also in the bowl and on the table.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 667, "extra_info": {"duration": 5.94, "events": ["Person scoops chopped cherry tomatoes into glass bowl.", "Person scoops chopped cucumber into glass bowl.", "Other vegetables rest in the bowl.", "Vegetables lay on the table."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 667}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A leopard walks from right to left across an upside-down tree trunk, then turns around and walks to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 668, "extra_info": {"duration": 8.98, "events": ["A leopard walks across an upside-down tree trunk.", "The leopard turns around on the tree trunk.", "The leopard walks to the right on the trunk."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 668}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and woman with graying hair danced in the room, and after the woman did a spin, the two hugged each other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 669, "extra_info": {"duration": 5.8, "events": ["Man and woman danced in a room.", "Woman did a spin.", "Man and woman hugged each other."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 669}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a black guy is gesturing with his right hand towards the front while continuously talking. In front of the camera, a white guy is wearing VR glasses and holding a game controller. He glances at it, and then repeatedly gestures towards the front.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 670, "extra_info": {"duration": 20.88, "events": ["A black guy gestures with his right hand towards the front.", "A black guy continuously talks.", "A white guy wearing VR glasses glances at a game controller.", "A white guy repeatedly gestures towards the front."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 670}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman, both dressed in white, argue in front of the camera. In the end, the woman pushes the man and walks off to the right side of the screen, and the man follows suit and leaves as well.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 671, "extra_info": {"duration": 22.8, "events": ["A man and a woman argue in front of the camera.", "The woman pushes the man.", "The woman walks off to the right side of the screen.", "The man follows the woman."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 671}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A large group of penguins jumped into the water one by one from the rocky shore. Just as the last penguin jumped in, the tide receded, leaving it lying on the rocks. It wasn't until the water rose again that it swam into the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 672, "extra_info": {"duration": 19.72, "events": ["A large group of penguins jumped into the water.", "Penguins jumped from the rocky shore one by one.", "The last penguin jumped towards the water.", "The tide receded after the last penguin jumped.", "The last penguin lay on the rocks.", "Water rose again.", "The last penguin swam into the water."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 672}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A girl is playing with clay on the table, flattening the clay with her hands. Next to her is a child wearing a yellow sweater, who starts with one hand on the table, and at the end of the video, he stands up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 673, "extra_info": {"duration": 13.16, "events": ["A girl plays with clay on the table.", "The girl flattens the clay with her hands.", "A child with a yellow sweater has one hand on the table.", "The child in the yellow sweater stands up."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 673}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "First, a series of small explosions occurred on the ground, followed by a large explosion that caused the ground to bounce up, and then the entire screen was filled with dust.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 674, "extra_info": {"duration": 9.38, "events": ["A series of small explosions occurred on the ground.", "A large explosion caused the ground to bounce up.", "Dust filled the entire screen."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 674}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A delivery guy is riding a bicycle, carrying a yellow delivery box as he cycles forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 675, "extra_info": {"duration": 9.37, "events": ["A delivery guy rides a bicycle.", "The delivery guy carries a yellow delivery box."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 675}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera focuses on the profile of a female character, then she turns her head to face the camera. Her glasses have a mechanical sheen, and the skin texture on the right side of her face is illuminated by an internal light.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 676, "extra_info": {"duration": 10.12, "events": ["The camera focuses on the female character's profile.", "The female character turns her head to face the camera.", "Glasses on the female character's face gleam mechanically.", "The internal light illuminates the female character's skin texture."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 676}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On an assembly line, workers dressed in white and blue clothes are operating on a moving platform. The video is played in fast-forward from beginning to end.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 677, "extra_info": {"duration": 8.08, "events": ["Workers operate on a moving platform", "Assembly line moves continuously", "Workers dress in uniforms", "Video plays in fast-forward", "Workers perform tasks efficiently", "Platform carries products along the line", "Video starts", "Video ends"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 677}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A black car slowly makes its way through the crowd of protesters, with onlookers watching as the vehicle passes by, some taking out their phones to take pictures of it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 678, "extra_info": {"duration": 11.14, "events": ["A black car moves through the crowd.", "Protesters watch the black car passing by.", "Onlookers take out their phones.", "People take pictures of the black car."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 678}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A young woman is live streaming with her phone set up in front of her. She holds a pen in her right hand and lifts a notebook with her left, pointing to a drawing inside as she explains it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 679, "extra_info": {"duration": 10.33, "events": ["A young woman starts live streaming.", "The young woman sets up her phone in front of her.", "The young woman holds a pen in her right hand.", "The young woman lifts a notebook with her left hand.", "The young woman points to a drawing inside the notebook.", "The young woman explains the drawing."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 679}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men and two women are playing the game of Rock-Paper-Scissors together. They played two rounds; in the first round, the man in red on the left side of the camera was eliminated. Then they proceeded to the second round of the game.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 680, "extra_info": {"duration": 10.85, "events": ["Two men and two women start playing Rock-Paper-Scissors.", "Players show their chosen signs simultaneously.", "First round of the game concludes.", "The man in red is eliminated.", "Four players prepare for another round.", "Second round of the game begins.", "Players reveal their choices again."], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 680}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the room, a man and a woman dressed in white opened a paper box and took out a piece of white clothing. The woman happily twirled and bounced around, while the man looked at her with a smile.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 681, "extra_info": {"duration": 10.94, "events": ["A man and a woman opened a paper box", "The man and the woman took out a piece of clothing", "The woman twirled", "The woman bounced around", "The man looked at the woman with a smile"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 681}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the room, one person is mopping the floor with a mop, another person is wiping the table with a spray bottle, and another person is cleaning the dishes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 682, "extra_info": {"duration": 16.16, "events": ["One person mops the floor.", "Another person wipes the table.", "That person uses a spray bottle.", "Another person cleans the dishes."], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 682}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A vehicle moves forward along a winding road, with the camera following it. Gradually, the camera catches up and overtakes the vehicle to film another car at a fork in the road ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 683, "extra_info": {"duration": 20.72, "events": ["A vehicle moves forward along a winding road.", "The camera follows the vehicle.", "The camera catches up with the vehicle.", "The camera overtakes the vehicle.", "The camera films another car.", "Another car waits at a fork in the road ahead."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 683}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures the view of the sunset from the mountaintop. A woman in a denim jacket and a man in a windbreaker enter the screen from the right, one after another. They both glance at the sunset, then look at each other. The woman smiles, while the man speaks to her.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 684, "extra_info": {"duration": 7.19, "events": ["The camera captures the sunset from the mountaintop.", "A woman in a denim jacket enters the screen from the right.", "A man in a windbreaker enters the screen from the right.", "The woman glances at the sunset.", "The man looks at the sunset.", "The woman and the man look at each other.", "The woman smiles.", "The man speaks to the woman."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 684}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a lady dressed in a yellow windbreaker walks up from a slope filled with gravel, reaching the camera, a man in grey clothes waiting there reaches out a hand to pull the lady up the slope, and the lady climbs up and walks out of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 685, "extra_info": {"duration": 11.43, "events": ["A lady walks up a gravel slope.", "The man waits at the top of the slope.", "The man reaches out his hand.", "The lady grabs the man's hand.", "The man pulls the lady up the slope.", "The lady climbs up the slope.", "The lady walks out of the frame."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 685}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the track field, a black athlete is in a sprinting starting position. Subsequently, he raises his hips then takes off running. The camera captures the athlete running towards the lens from the front until the athlete runs out of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 686, "extra_info": {"duration": 10.72, "events": ["A black athlete assumes a sprinting starting position.", "The black athlete raises his hips.", "The black athlete takes off running.", "The athlete runs towards the camera.", "The athlete runs out of the frame."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 686}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the table, there are three hands, each holding a pen, with a notebook underneath. The hand on the right side of the screen is writing something in the notebook. Then, a person walks in from the left side of the frame and sits down facing the camera. The newcomer places a laptop on the table with both hands, then opens the laptop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 687, "extra_info": {"duration": 5.82, "events": ["Three hands hold pens above a notebook.", "The right hand writes in the notebook.", "A person walks into the frame from the left.", "The person sits down facing the camera.", "The person places a laptop on the table.", "The person opens the laptop."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 687}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a forest, a beautiful lady looks towards the camera, then walks into the depths of the woods. As she walks, she looks back at the camera, walks for a while, then runs for a few steps before turning back to face the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 688, "extra_info": {"duration": 16.58, "events": ["A lady looks towards the camera.", "The lady walks into the woods.", "The lady looks back at the camera.", "The lady walks for a while.", "The lady runs for a few steps.", "The lady turns to face the camera."], "n_events": 6, "n_shots": 2, "n_subjects": 1}, "vid": 688}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In front of the camera, a brown-haired girl is playing with a Tower of Hanoi puzzle. She places the disk on the last vacant peg of the Tower of Hanoi, then pushes the tower towards the direction of the camera before reaching out to grab another toy. In the distance, the silhouettes of two blurry children can be seen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 689, "extra_info": {"duration": 14.68, "events": ["Brown-haired girl plays with Tower of Hanoi puzzle.", "Girl places disk on last vacant peg.", "Girl pushes tower towards camera.", "Girl reaches out to grab another toy.", "Two blurry children appear in the distance."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 689}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the city's evening, a homemade tank travels on the road. From afar to nearby, one can see the lookout personnel on top of the tank and the driver in the middle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 690, "extra_info": {"duration": 20.19, "events": ["The homemade tank travels on the city road.", "The evening falls over the city.", "The tank moves from afar to nearby.", "The lookout personnel surveys from atop the tank.", "The driver operates the tank from the middle."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 690}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows an empty expanse of ice, after which several ice hockey players rush in from the right side of the frame. They come to a stop in the center of the screen, sliding back, but one of them fails to stop properly and slips, falling down onto the ice. The other players have already slid out of the frame, leaving only him to slowly and painfully get back up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 691, "extra_info": {"duration": 7.13, "events": ["Ice expanse appears on the screen.", "Ice hockey players rush in from the right.", "Players come to a stop in the center.", "One player fails to stop properly.", "The failing player slips on the ice.", "Player falls down onto the ice.", "Other players slide out of the frame.", "The fallen player slowly gets back up."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 691}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a scene where a man is leaning against a wall, his mouth moving as if talking or singing. The camera pans down from his head, gradually revealing the man's hands playing a guitar. Finally, the camera slowly moves up from the guitar, panning back towards the man's head.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 692, "extra_info": {"duration": 22.96, "events": ["The man leans against the wall.", "The man's mouth moves.", "The camera pans down from the man's head.", "The man's hands play the guitar.", "The camera moves up from the guitar."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 692}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the foreground, there is an open laptop on the table. In the background, a little girl is lying on her side in bed, getting ready to sleep. A lady opens a blanket and drapes it over the little girl, then she leans down to kiss the little girl. The little girl also turns her head towards the lady, returning the kiss.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 693, "extra_info": {"duration": 9.51, "events": ["Laptop is open on the table.", "Little girl lies in bed.", "Lady opens the blanket.", "Lady drapes the blanket over the little girl.", "Lady leans down.", "Lady kisses the little girl.", "Little girl turns her head.", "Little girl returns the kiss."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 693}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pig is walking on the ground, rooting in the soil. Then, the camera shows two pigs, one large and one small. The new, larger pig sniffs the pig in front, followed by another pig that crosses the frame from left to right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 694, "extra_info": {"duration": 10.21, "events": ["A pig walks on the ground.", "A pig roots in the soil.", "A larger pig sniffs the smaller pig.", "Another pig crosses the frame."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 694}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a night, with streetlights illuminated, a roller coaster slides past the camera, and in the distance, a pirate ship is swaying.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 695, "extra_info": {"duration": 6.01, "events": ["Roller coaster slides past the camera.", "Pirate ship sways in the distance."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 695}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person holds an envelope in their right hand and a lighter in their left. They light the bottom left corner of the envelope, and it continues to burn, moving towards the right side of the person. The video ends after most of the paper has been burned away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 696, "extra_info": {"duration": 20.32, "events": ["A person holds an envelope in their right hand.", "A person holds a lighter in their left hand.", "The person lights the bottom left corner of the envelope.", "The flame moves towards the right side of the envelope.", "The envelope continues to burn.", "The video ends after most of the envelope has burned away."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 696}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and a woman are sitting on the snow facing the camera. The man is talking non-stop, while the woman is holding a cup and drinking water. In front of the camera, a figure runs from left to right, and after a while, runs back from the right to the left. After a bit more time, a woman wearing a thick scarf runs over from the left and sits to the right of the first woman, who then passes the water cup to the woman with the thick scarf.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 697, "extra_info": {"duration": 18.92, "events": ["A man and a woman sit on the snow.", "The man talks non-stop.", "The woman holds a cup and drinks water.", "A figure runs from left to right in front of the camera.", "The same figure runs back from right to left.", "A woman with a thick scarf runs over from the left.", "The woman with a thick scarf sits to the right of the first woman.", "The first woman passes the cup to the woman with the thick scarf."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 697}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, it starts with darkness, then from the top it begins to turn red, as the red area gradually expands, the screen begins to brighten. Soon after, the sun emerges from the dark clouds, illuminating the sky.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 698, "extra_info": {"duration": 11.6, "events": ["Darkness envelops the screen.", "Red color begins to appear from the top.", "The red area expands gradually.", "Screen starts to brighten.", "Sun emerges from dark clouds.", "Sun illuminates the sky."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 698}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A foot clad in orange sports shoes steps onto a platform, then a pair of hands reach down to tie the shoelaces, and then the foot is placed back down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 699, "extra_info": {"duration": 12.4, "events": ["Foot steps onto platform", "Hands reach down", "Hands tie shoelaces", "Foot is placed back down"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 699}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the gym, a lady swings a rope up and down with alternating hands, and after a while, she changes to swinging the rope up and down with both hands together. Next to her, a gentleman is kneeling on one leg and clapping his hands, while a lady first stands and then bends over to clap her hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 700, "extra_info": {"duration": 10.23, "events": ["Lady swings rope with alternating hands", "Lady switches to swinging rope with both hands", "Gentleman kneels on one leg", "Gentleman claps hands", "Lady stands next to gentleman", "Lady bends over to clap hands"], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 700}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, there are two birds: one perched on a tree branch and another fluttering its wings in the air. Then, a third bird flies up from below. The bird that was flying leaves the screen, while the third bird flaps its wings for a while. The bird that was perched on the branch flies down and joins the third bird, flying away together.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 701, "extra_info": {"duration": 10.64, "events": ["One bird is perched on a tree branch.", "Another bird is fluttering its wings in the air.", "A third bird flies up from below.", "The flying bird leaves the screen.", "The third bird flaps its wings.", "The perched bird flies down.", "The perched bird joins the third bird.", "The perched bird and the third bird fly away together."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 701}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the water, an octopus swims away into the distance followed by a group of small fish. The octopus stops at a spot, its body quickly blending in with the surrounding environment. The following group of small fish quickly disperses, and then a few of them swim back towards the octopus.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 702, "extra_info": {"duration": 20.81, "events": ["An octopus swims away in the water.", "A group of small fish follows the octopus.", "The octopus stops at a spot.", "The octopus camouflages with the environment.", "The group of small fish quickly disperses.", "A few small fish swim towards the octopus."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 702}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angle looks down as a hand reaches forward, pinching a small piece of food between its fingers. A little dog, squatting on the ground, watches the hand holding the food. Then, the dog stands up on its hind legs and takes the food from the hand using its mouth to eat it, before settling back on all fours and looking upwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 703, "extra_info": {"duration": 5.0, "events": ["A hand pinches a piece of food.", "A little dog watches the hand.", "The dog stands up on hind legs.", "The dog takes the food using its mouth.", "The dog eats the food.", "The dog settles back on all fours.", "The dog looks upwards."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 703}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of feet walks to the left on the grass, followed one after another by little dogs appearing in the frame. With each step, these feet are lifted high to avoid stepping on the puppies. Moving from the left to the right and back again, the little dogs frolic alongside these feet.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 704, "extra_info": {"duration": 16.95, "events": ["A pair of feet walks to the left on the grass.", "Little dogs appear following the feet.", "Feet are lifted high to avoid stepping on puppies.", "Feet move from the left to the right.", "Feet move back again.", "Little dogs frolic alongside the feet."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 704}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of hands wearing blue gloves is preparing for a nucleic acid test. A person opens their mouth, and the tester inserts a cotton swab deep into their mouth for sampling. After the sampling is completed, the person being tested puts on a mask, and the tester seals the test reagent in a test tube. Then, the next person comes forward to prepare for the test.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 705, "extra_info": {"duration": 26.89, "events": ["Hands wearing blue gloves prepare for a nucleic acid test", "A person opens their mouth", "Tester inserts a cotton swab into the person's mouth", "Tester takes a sample from the person's mouth", "The person being tested puts on a mask", "Tester seals the test reagent in a test tube", "The next person comes forward for the test"], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 705}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person sprinkled some salt onto the vegetables in a glass bowl and then used a pair of spatulas, one in each hand, to toss and turn them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 706, "extra_info": {"duration": 16.35, "events": ["Person sprinkled salt onto the vegetables.", "Person picked up a pair of spatulas.", "Person used spatulas to toss the vegetables.", "Person used spatulas to turn the vegetables."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 706}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a leopard is seen running forward. The camera gradually speeds up and pulls away from the running leopard, then captures another leopard also in motion. The camera continues to accelerate slowly, filming the scene of the two leopards running one after the other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 707, "extra_info": {"duration": 20.27, "events": ["A leopard is running forward.", "Another leopard starts running."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 707}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a man and two women are chatting and drinking together. The two ladies are talking and smiling, while the gentleman picks up the bottle and takes a sip.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 708, "extra_info": {"duration": 9.38, "events": ["A man and two women chat.", "Two women talk.", "Two women smile.", "The man picks up the bottle.", "The man takes a sip."], "n_events": 5, "n_shots": 1, "n_subjects": 3}, "vid": 708}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures the side profile of a person's face, focusing on the eye area. The person blinks frequently and their head shakes slightly, as if they are crying. Then, the person uses a tissue to wipe away tears from the corner of their eyes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 709, "extra_info": {"duration": 8.08, "events": ["The person blinks frequently.", "The person's head shakes slightly.", "The person uses a tissue.", "Tears are wiped from the corner of the eyes."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 709}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The screen transitions from flickering to a normal state, divided into many small squares by black bars, with each square containing an eye of a different color. Each eye is moving, and the camera gradually focuses on a yellow eye, before the screen reverts to a flickering state again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 710, "extra_info": {"duration": 12.54, "events": ["Screen transitions from flickering to normal state.", "Black bars divide screen into small squares.", "Each square contains a moving eye.", "Eyes display different colors.", "Camera focuses on a yellow eye.", "Screen reverts to a flickering state."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 710}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, there is an ongoing game of chess. A hand picks up a knight and captures an opposing pawn. In the foreground, there is a silhouette of a person who makes a slight movement with their hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 711, "extra_info": {"duration": 6.7, "events": ["A game of chess is ongoing.", "A hand picks up a knight.", "The knight captures an opposing pawn.", "A silhouette makes a slight hand movement."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 711}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Beside a black car on the snowy ground, a person bent down for a look at the tire, then stepped forward to kick it, followed by pressing on the tire with their hand, and then stood up straight.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 712, "extra_info": {"duration": 7.54, "events": ["A person bent down beside a black car.", "The person examined the tire.", "The person kicked the tire.", "The person pressed on the tire with their hand.", "The person stood up straight."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 712}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the snowy ground, the camera captures the backs of two people who start to run forward. Another person with a backpack appears in the shot, running behind them. The two in front run over a low wall, and the person in the lead glances back before continuing to run forward. The last person also crosses the low wall, and the three of them run along the winding road towards the forest, one after the other.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 713, "extra_info": {"duration": 21.66, "events": ["Two people start to run on the snowy ground.", "Another person with a backpack runs behind the two.", "The front runners cross over a low wall.", "The person in the lead glances back.", "The last person crosses the low wall.", "The three individuals run towards the forest."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 713}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A submarine slowly surfaced, with many ships in the distance. The camera panned around the submarine from left to right, and at the end of the video, a speedboat moved forward along the submarine.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 714, "extra_info": {"duration": 8.88, "events": ["Submarine surfaced slowly.", "Ships appeared in the distance.", "Speedboat moved forward alongside the submarine."], "n_events": 3, "n_shots": 1, "n_subjects": 3}, "vid": 714}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "One person places the soccer ball on the line, and in the distance, another person stands in front of the goal, assuming the position of a goalkeeper. The person kicks the ball towards the goal. The goalkeeper dives in the wrong direction, and the ball enters the net. The shooter slowly runs towards the goal.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 715, "extra_info": {"duration": 7.98, "events": ["One person places the soccer ball on the line", "Another person assumes the position of a goalkeeper", "The person kicks the soccer ball towards the goal", "The goalkeeper dives in the wrong direction", "The soccer ball enters the net", "The shooter runs towards the goal"], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 715}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A young man and woman are playing video games with controllers. They both synchronize a movement with their bodies, then the woman lifts her arm to press down on the man's arm. The man dodges and continues operating the controller. Afterwards, the woman lets go of her controller with her left hand, covers the man's eyes for a short moment then brings her hand back to continue playing. The man shows a look of frustration, while the woman laughs joyfully. She twists her body, and then she turns around, caresses the man's face with her left hand, kisses him, and rests her head against his face. Finally, she speaks to the man.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 716, "extra_info": {"duration": 18.32, "events": ["A young man and woman play video games with controllers.", "Both synchronize a movement with their bodies.", "The woman lifts her arm to press the man's arm.", "The man dodges and continues playing.", "The woman covers the man's eyes briefly.", "The man shows frustration.", "The woman laughs joyfully.", "The woman twists her body.", "The woman turns around and caresses the man's face.", "The woman kisses the man and rests her head against his."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 716}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera shakes as it films the Mona Lisa painting. As the lens pulls back, many people's heads come into view, with numerous individuals holding up their mobile phones or cameras to take pictures. A man dressed in black steps out from the crowd and turns to walk away. He then looks down at the camera in his hands. The camera angle changes to another direction, showing a dense crowd of visitors moving around inside the exhibition hall.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 717, "extra_info": {"duration": 20.77, "events": ["The camera films the Mona Lisa painting.", "The lens pulls back.", "People's heads come into view.", "Individuals hold up their mobile phones or cameras.", "A man dressed in black steps out from the crowd.", "The man turns to walk away.", "The man looks down at his camera.", "Visitors move around inside the exhibition hall."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 717}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person jumps onto a platform with both legs, then squats down, followed by returning to the ground, jumps back onto the platform to squat again, and finally steps down from the platform.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 718, "extra_info": {"duration": 5.23, "events": ["Person jumps onto platform", "Person squats down", "Person returns to ground", "Person jumps back onto platform", "Person squats down again", "Person steps down from platform"], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 718}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A lady shows an expression of surprise at something ahead, then opens her arms and runs over to embrace the three ladies in front of the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 719, "extra_info": {"duration": 14.3, "events": ["Lady shows an expression of surprise.", "Lady opens her arms.", "Lady runs over to the three ladies.", "Lady embraces the three ladies."], "n_events": 4, "n_shots": 1, "n_subjects": 4}, "vid": 719}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a full screen of starlight moves towards the direction of the screen. Metallic text flies from the bottom left of the screen to the center, and then the text rotates to reveal the words “HAPPY BIRTHDAY”. Afterwards, the text stops, and countless stars fall from above. Finally, the stars slowly fade away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 720, "extra_info": {"duration": 14.01, "events": ["Starlight moves towards the screen", "Metallic text flies from bottom left to center", "Metallic text rotates to reveal 'HAPPY BIRTHDAY'", "Metallic text stops in the center", "Countless stars fall from above", "Stars slowly fade away"], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 720}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person rides a mountain bike on a winding path, cycling quickly uphill and downhill, turning and moving forward, then gradually moving away from the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 721, "extra_info": {"duration": 8.12, "events": ["Person rides mountain bike on winding path.", "Person cycles quickly uphill.", "Person cycles quickly downhill.", "Person turns on the path.", "Person moves forward.", "Person gradually moves away from the camera."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 721}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men and one woman are playing cards. The lady on the right is resting her cheeks in her hands. The man in the middle flips the cards onto the table, then collects all the chips. The man on the left and the lady on the right give a round of applause. Afterward, the man in the middle raises his hand and high-fives the man on the left. Finally, he turns to look at the lady, who returns to her pose with her cheeks resting in her hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 722, "extra_info": {"duration": 7.67, "events": ["Two men and one woman play cards.", "The woman rests her cheeks in her hands.", "The man in the middle flips cards onto the table.", "The man in the middle collects all the chips.", "The man on the left and the woman applaud.", "The man in the middle raises his hand.", "The man in the middle high-fives the man on the left.", "The woman returns to her pose."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 722}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Inside the cabin, a person at the cabin door gestured upwards with their hand, then pointed outside. Then, one by one, individuals jumped out of the door. After a brief pause, another two people jumped out from the door, and each person in front then successively deployed their parachutes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 723, "extra_info": {"duration": 18.82, "events": ["A person gestured upwards at the cabin door.", "The same person pointed outside the cabin.", "An individual jumped out of the door.", "Another individual jumped out of the door.", "Yet another individual jumped out of the door.", "Two people consecutively jumped out from the door.", "The first person deployed their parachute.", "The second person deployed their parachute."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 723}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera is filming underwater, a little girl stands in the pool, then she pinches her nose with both hands and squats into the water, and then slowly floats in the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 724, "extra_info": {"duration": 13.55, "events": ["Camera films underwater.", "Little girl stands in the pool.", "Little girl pinches her nose with both hands.", "Little girl squats into the water.", "Little girl slowly floats in the water."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 724}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A black woman wearing glasses is taking a selfie with her phone; she waves at the camera. In the distance, a man is also waving at the camera. Then, facing the camera, he performs a series of movements and strikes various poses.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 725, "extra_info": {"duration": 12.38, "events": ["A woman takes a selfie with her phone.", "A woman waves at the camera.", "A man in the distance waves at the camera.", "The man performs a series of movements.", "The man strikes various poses."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 725}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man jumped up and grabbed the bar with both hands, then did three pull-ups.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 726, "extra_info": {"duration": 14.51, "events": ["The man jumped up.", "The man grabbed the bar.", "The man did a pull-up.", "The man did a second pull-up.", "The man did a third pull-up."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 726}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people are gathered around a table with an architectural model, discussing the layout plan. A man reaches out to pick up a white windmill model, while another woman moves a blue cardboard representing a lake to the right side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 727, "extra_info": {"duration": 6.6, "events": ["A group of people gather around a table.", "People discuss the architectural layout plan.", "A man picks up a white windmill model.", "A woman moves a blue cardboard.", "The blue cardboard representing a lake shifts to the right side."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 727}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman places slices of the cut lemon into the cup one by one.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 728, "extra_info": {"duration": 5.11, "events": ["The woman cuts a lemon.", "The woman places a lemon slice into the cup."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 728}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera affixed to the train captures from the side the train moving forward in a snowy landscape, with the image gradually darkening as the train enters a tunnel.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 729, "extra_info": {"duration": 22.36, "events": ["The camera captures the train moving forward.", "The train advances in a snowy landscape.", "The image darkens gradually.", "The train enters a tunnel."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 729}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera captures a group of horses on the grassland, focusing on one of them. At first, it faces the camera, then turns and runs to the right. Soon after, another horse sprints forward, overtaking the former. The camera follows their movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 730, "extra_info": {"duration": 14.01, "events": ["Camera captures a group of horses on grassland", "One horse faces the camera", "The horse turns", "The horse runs to the right", "Another horse sprints forward", "The second horse overtakes the first horse", "Camera follows the movement of the horses"], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 730}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A woman is holding a mobile phone and wearing headphones while dancing. During this time, she taps on the phone screen a few times and then raises her right hand and waves it as she continues to dance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 731, "extra_info": {"duration": 28.2, "events": ["A woman holds a mobile phone.", "A woman wears headphones.", "A woman dances.", "A woman taps on the phone screen.", "A woman raises her right hand.", "A woman waves her right hand."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 731}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera moves forward along the road, capturing the seaside buildings, and ultimately, the camera pans left following the road to film the vast expanse of the ocean.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 732, "extra_info": {"duration": 10.08, "events": ["The camera moves forward along the road.", "The camera captures seaside buildings.", "The camera pans left.", "The camera films the ocean."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 732}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person walks up to the scooter, withdraws the kickstand with the left foot, steps on the deck with the right foot, and rides away forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 733, "extra_info": {"duration": 8.17, "events": ["A person walks up to the scooter.", "Person withdraws the kickstand with the left foot.", "Person steps on the deck with the right foot.", "Person rides away forward."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 733}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A pair of hands picked a few fruits of varying sizes from the branches.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 734, "extra_info": {"duration": 17.85, "events": ["Hands reached towards the branches.", "Hands grasped fruits of varying sizes.", "Hands picked fruits from the branches."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 734}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The eagle soars in the sky, the camera follows and films it, then it smoothly lands on a cliff face.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 735, "extra_info": {"duration": 24.49, "events": ["The eagle soars in the sky.", "The camera follows the eagle.", "The eagle lands on a cliff face."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 735}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The golf ball rolls into the hole, a foot steps forward, and then a hand picks the ball out of the hole and leaves the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 736, "extra_info": {"duration": 6.84, "events": ["Golf ball rolls into the hole.", "Foot steps forward.", "Hand picks the ball out of the hole."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 736}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person moves a wooden spoon up and down with both hands, stirring the vegetables, and then reaches out to pick up a cellphone, swiping across the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 737, "extra_info": {"duration": 13.3, "events": ["Person moves wooden spoon up.", "Person moves wooden spoon down.", "Person stirs the vegetables.", "Person reaches out.", "Person picks up cellphone.", "Person swipes across the screen."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 737}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person threw a pile of documents into a burning fire, the one on top with the title \"contract agreement\" written on it, which quickly burned to ashes.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 738, "extra_info": {"duration": 20.95, "events": ["A person threw a pile of documents into a fire.", "The fire burned the documents.", "The top document titled 'contract agreement' quickly turned to ashes."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 738}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The puppy squatted down, then suddenly backed away, wagging its tail and moving back and forth, revealing two other puppies beside the food bowl behind it, one of which withdrew its paw that had been reaching into the food bowl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 739, "extra_info": {"duration": 15.05, "events": ["The puppy squatted down.", "The puppy suddenly backed away.", "The puppy wagged its tail.", "The puppy moved back and forth.", "Two other puppies appeared beside the food bowl.", "One of the other puppies withdrew its paw.", "The other puppy had been reaching into the food bowl."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 739}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person reaches out to pluck the lettuce leaves one by one, placing them on their left hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 740, "extra_info": {"duration": 16.58, "events": ["A person reaches out.", "Person plucks lettuce leaves.", "Person places leaves on their left hand."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 740}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Animated image, showing several red blood cells flowing smoothly in a blood vessel, followed by a sudden constriction in the vessel where the red blood cells become blocked.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 741, "extra_info": {"duration": 13.4, "events": ["Animated image appears", "Red blood cells flow in a blood vessel", "Blood vessel suddenly constricts", "Red blood cells become blocked"], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 741}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The boy sits on the ground holding a toy, stands up with one hand supporting him on the floor, walks towards the camera, and then turns his head to look to his right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 742, "extra_info": {"duration": 5.62, "events": ["The boy sits on the ground with a toy.", "The boy stands up supporting himself with one hand.", "The boy walks towards the camera.", "The boy turns his head to look right."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 742}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person uses a colander to scoop food from a metal basin into a bowl in their hand twice, then places the colander back in the metal basin, and sets the bowl on the table.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 743, "extra_info": {"duration": 16.88, "events": ["Person scoops food with a colander.", "Person transfers food from colander to a bowl.", "Person repeats the scooping action.", "Person repeats the transferring action.", "Person places the colander in the metal basin.", "Person sets the bowl on the table."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 743}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A dark shadow against a red background moved upwards, then morphed into a humanoid shape, and subsequently transformed into black lines that vanished upwards. Next, it changed into a terrifying human face with an agape bloodied mouth flying towards the camera. Blackness took over the entire screen, and finally, red once again spread across the entire screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 744, "extra_info": {"duration": 8.24, "events": ["A dark shadow moved upwards against a red background.", "The shadow morphed into a humanoid shape.", "The humanoid shape transformed into black lines.", "Black lines vanished upwards.", "The shape changed into a terrifying human face.", "A bloodied mouth flew towards the camera.", "Blackness took over the entire screen.", "Red spread across the entire screen."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 744}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is counting money and then touching the embossed print on the bills with their right hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 745, "extra_info": {"duration": 14.48, "events": ["A person counts money.", "The person touches the embossed print on the bills."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 745}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Coins bearing portraits continually fall down, as the clock in the background moves upwards.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 746, "extra_info": {"duration": 24.9, "events": ["Coins fall down continuously.", "The clock moves upwards."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 746}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman with blonde hair opens her mouth wide in awe as she watches the woman with black hair in front of her, who places a headpiece on the blonde-haired woman's head. The blonde woman places her hands on her cheeks, shaking her head and swaying with joy. Afterwards, the woman with black hair turns around to receive a piece of gauze handed over by another woman on the left, and then she places it on the blonde woman's head. As the camera zooms out, the woman with blonde hair waves her hands excitedly and twists her body.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 747, "extra_info": {"duration": 18.9, "events": ["Blonde-haired woman opens mouth in awe", "Woman with black hair places headpiece on blonde-haired woman", "Blonde-haired woman puts hands on cheeks", "Blonde-haired woman shakes head", "Blonde-haired woman sways with joy", "Woman with black hair receives piece of gauze from another woman", "Woman with black hair places gauze on blonde-haired woman's head", "Blonde-haired woman waves hands excitedly", "Blonde-haired woman twists body"], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 747}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man smiled and stretched out both hands forward, the girl ran over and embraced the man. In the girl's hand, she was holding a greeting card with a heart drawn on it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 748, "extra_info": {"duration": 5.51, "events": ["The man smiled.", "The man stretched out both hands.", "The girl ran over.", "The girl embraced the man.", "The girl held a greeting card."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 748}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The dog shook the water off its body, then turned its head to look back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 749, "extra_info": {"duration": 10.71, "events": ["The dog shook the water off its body.", "The dog turned its head to look back."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 749}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four people are playing soccer, with two passing the ball and two blocking. One of them runs forward and kicks the ball, which goes in, and teammates raise their hands to cheer. One person from the defending team holds their head with both hands in dismay, while another retrieves the ball from the goal, cradling it in their arms.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 750, "extra_info": {"duration": 9.15, "events": ["Four people play soccer.", "Two players pass the ball.", "Two players attempt to block.", "One player runs forward.", "One player kicks the ball.", "Ball goes into the goal.", "Teammates raise hands to cheer.", "One defender holds head in dismay.", "Another defender retrieves the ball.", "Defender cradles ball in arms."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 750}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person holds a shovel, steps on it to turn over the soil in front of them, then moves one step to the right, and continues to turn over the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 751, "extra_info": {"duration": 17.95, "events": ["Person holds a shovel.", "Person steps on the shovel.", "Shovel turns over the soil.", "Person moves one step to the right.", "Person continues to turn over the ground."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 751}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of bats are flying in the night sky against the backdrop of a bright full moon.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 752, "extra_info": {"duration": 27.03, "events": ["A group of bats flies in the night sky.", "The moon reaches full brightness."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 752}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two children are repeatedly exchanging the toys in their hands on a red tabletop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 753, "extra_info": {"duration": 6.88, "events": ["Two children exchange toys", "Toys move from one hand to another", "Exchange happens on a red tabletop"], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 753}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman squeezes the hand cream onto the back of her hand, then closes the lid and sets it aside, spreads it out with her fingers, and then rubs her hands together reciprocally.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 754, "extra_info": {"duration": 11.61, "events": ["The woman squeezes hand cream onto her hand.", "The woman closes the hand cream's lid.", "The woman sets the hand cream aside.", "The woman spreads the cream with her fingers.", "The woman rubs her hands together."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 754}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two metal dice fall down, roll forward, and then come to a stop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 755, "extra_info": {"duration": 5.34, "events": ["Two metal dice fall down.", "Metal dice roll forward.", "Metal dice come to a stop."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 755}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of children run forward, among them a boy bends over to pick up a football and walks to the right, the other children look towards him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 756, "extra_info": {"duration": 11.2, "events": ["Group of children run forward", "Boy bends over", "Boy picks up football", "Boy walks to the right", "Other children look towards boy"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 756}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person stands in the swimming pool, with both hands braced against the edge of the pool. With a forceful leap, they step onto the ground and finally stand up and walk forward out of the frame.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 757, "extra_info": {"duration": 6.63, "events": ["Person stands in the swimming pool.", "Person braces hands against the pool edge.", "Person leaps forcefully.", "Person steps onto the ground.", "Person stands up.", "Person walks forward.", "Person exits the frame."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 757}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A blue sedan drifts into the frame and then drives away, with the passenger holding onto the safety handle in the car.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 758, "extra_info": {"duration": 4.59, "events": ["A blue sedan drifts into the frame.", "The blue sedan drives away.", "Passenger holds onto the safety handle."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 758}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is carrying a bicycle on their shoulders while walking up a hillside, then puts down the bicycle and continues forward by pushing it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 759, "extra_info": {"duration": 10.44, "events": ["Person carries bicycle on shoulders.", "Person walks up a hillside.", "Person puts down the bicycle.", "Person pushes the bicycle forward."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 759}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people and a man in a spacesuit are dancing indoors.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 760, "extra_info": {"duration": 15.16, "events": ["A group of people dances indoors.", "A man in a spacesuit dances indoors."], "n_events": 2, "n_shots": 1, "n_subjects": 2}, "vid": 760}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A family sits around on a picnic blanket, the woman takes out bread from the basket, while the man takes out plates to put down bananas, and then takes out two bowls, places an apple into each, and hands them to two girls. They kneel and sit; the girl in front picks up her hair and turns around to look, and the girl in the back reaches out to take some food.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 761, "extra_info": {"duration": 13.32, "events": ["Family sits on picnic blanket.", "Woman takes out bread from basket.", "Man takes out plates.", "Man places bananas on plates.", "Man takes out two bowls.", "Man places an apple into each bowl.", "Man hands bowls to two girls.", "Two girls kneel and sit.", "Girl in front picks up her hair.", "Girl in back reaches out for food."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 761}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The fighter jet moves forward, taking off into the sky, with thick smoke appearing on the runway as the camera pans to the right.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 762, "extra_info": {"duration": 6.97, "events": ["The fighter jet moves forward.", "The fighter jet takes off into the sky.", "Thick smoke appears on the runway."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 762}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The smoke drifts upward, entering the smoke detector, and then the red light on the alarm flashes a few times before lighting up.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 763, "extra_info": {"duration": 20.85, "events": ["Smoke drifts upward", "Smoke enters the smoke detector", "Red light on the alarm flashes", "Red light stays illuminated"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 763}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person kicks a football to a man, the man catches the ball, takes a jump forward and kicks towards the goal. There are two other men blocking, but the man manages to kick the ball into the goal and jogs forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 764, "extra_info": {"duration": 6.72, "events": ["A person kicks a football to another man.", "The man catches the football.", "The man takes a jump forward.", "The man kicks the ball towards the goal.", "Two other men attempt to block.", "The ball goes into the goal.", "The man jogs forward."], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 764}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A close-up shot of a person shaking hands with three people in turn.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 765, "extra_info": {"duration": 11.3, "events": ["Person initiates handshake with first individual.", "Person completes handshake with first individual.", "Person initiates handshake with second individual.", "Person completes handshake with second individual.", "Person initiates handshake with third individual.", "Person completes handshake with third individual."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 765}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man lifts the blanket with both hands and embraces the woman in front of him, while the woman wraps her arms around herself. They lean their heads close together and smile. The woman turns around to make eye contact with the man, and he drapes the blanket over both of their heads, leaning back with his body as the woman extends her left leg.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 766, "extra_info": {"duration": 10.14, "events": ["The man lifts the blanket with both hands.", "The man embraces the woman.", "The woman wraps her arms around herself.", "They lean their heads close together.", "They smile at each other.", "The woman turns around to make eye contact.", "The man drapes the blanket over both heads.", "The man leans back with his body.", "The woman extends her left leg."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 766}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An ant, gripping a green leaf, flipped over the leaf to climb onto its surface and continued forward, passing by other ants along the way.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 767, "extra_info": {"duration": 8.98, "events": ["An ant gripped a green leaf.", "The ant flipped over the leaf.", "The ant climbed onto the leaf's surface.", "The ant continued forward.", "The ant passed by other ants."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 767}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person takes out several bags and boxes from a shopping cart and places them into the trunk, then reaches out to close the trunk.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 768, "extra_info": {"duration": 17.02, "events": ["A person takes out several bags from a shopping cart.", "A person takes out several boxes from a shopping cart.", "A person places bags into the trunk.", "A person places boxes into the trunk.", "A person reaches out to close the trunk."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 768}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Someone uses scissors to cut the green cardstock into the shape of a Christmas tree, removes the excess parts, sets down the scissors, takes out a piece of cardboard and places it on the table, then puts the Christmas tree on the cardboard. Afterwards, they take out colorful ornaments and decorate it by placing them on top.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 769, "extra_info": {"duration": 29.28, "events": ["Someone cuts the green cardstock into a Christmas tree shape.", "Excess parts are removed from the green cardstock.", "Scissors are set down on the table.", "Someone takes out a piece of cardboard.", "Cardboard is placed on the table.", "Christmas tree is put on the cardboard.", "Someone takes out colorful ornaments.", "Ornaments are placed on top of the Christmas tree."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 769}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A suction cup swings back and forth, sucking away all the coins on the table, and in the end, the screen fades to black.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 770, "extra_info": {"duration": 19.92, "events": ["A suction cup swings back and forth.", "The suction cup sucks away all the coins.", "The screen fades to black."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 770}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person, holding a curved stick, runs forward, then comes to a sudden stop, and turns around to run back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 771, "extra_info": {"duration": 5.0, "events": ["A person runs forward holding a curved stick.", "The person comes to a sudden stop.", "The person turns around.", "The person runs back."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 771}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman, holding the girl, sits on the sofa, waving together at someone in front of them. During this time, the woman turns her head to glance at the girl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 772, "extra_info": {"duration": 6.38, "events": ["The woman sits on the sofa.", "The woman holds the girl.", "The woman and the girl wave together.", "The woman turns her head.", "The woman glances at the girl."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 772}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person opens their left hand, reaches out with their right hand to take a pill, and then closes their left hand again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 773, "extra_info": {"duration": 5.04, "events": ["A person opens their left hand.", "The person reaches out with their right hand.", "The person takes a pill.", "The person closes their left hand."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 773}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man wearing red pants is walking along the platform, the subway train approaches from the distance, arriving at the platform and coming to a stop. Then, several people enter the frame from the right side, the subway doors open, and people board and disembark the train.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 774, "extra_info": {"duration": 28.95, "events": ["Man walks along the platform", "Subway train approaches from the distance", "Subway train arrives at the platform", "Subway train comes to a stop", "People enter the frame from the right side", "Subway doors open", "People board the train", "People disembark the train"], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 774}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The person clad in red clothing is holding a stopwatch while standing in the background, as two women crouch down in a starting position and then run forward together, with the camera following their movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 775, "extra_info": {"duration": 6.9, "events": ["Person holds a stopwatch.", "Two women crouch down.", "Women run forward together."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 775}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two people reach out to grab a block-shaped object and place it on top of an already constructed object, then withdraw their hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 776, "extra_info": {"duration": 11.78, "events": ["Two people reach out to grab a block-shaped object", "They place the block-shaped object on top of another object", "They withdraw their hands from the block-shaped object"], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 776}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A yellow cat is lying on a person's legs, fiddling with a toy with its paws, then falls off and climbs back up to continue playing. Finally, the cat jumps off the person's legs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 777, "extra_info": {"duration": 25.56, "events": ["A yellow cat lies on a person's legs", "The cat fiddles with a toy", "The cat falls off the person's legs", "The cat climbs back up onto the person's legs", "The cat continues playing with the toy", "The cat jumps off the person's legs"], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 777}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A group of people in uniform, fully armed and carrying shields, surrounded another group holding batons, and a series of conflicts broke out between them. The camera captured the scenes of the conflict from various angles. The latter hit the shields with their batons, and then one of them was pushed to the ground, followed by others piling on top. Afterwards, another person kicked a shield.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 778, "extra_info": {"duration": 22.42, "events": ["Group in uniform surrounded another group holding batons.", "Groups engaged in a series of conflicts.", "Group holding batons hit the shields.", "Uniformed person pushed a baton-holder to the ground.", "Others piled on top of the fallen person.", "Person kicked a shield."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 778}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The audience cheered, some waving flags. Photographers on the field ran forward, the camera panned left, showing two teams in competition, players dressed in black passing the soccer ball between them. A referee stood nearby as players in red attempted to block. One of them fell down, and then players from both teams also tumbled to the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 779, "extra_info": {"duration": 18.55, "events": ["The audience cheered.", "Some audience members waved flags.", "Photographers ran forward on the field.", "Players dressed in black passed the soccer ball.", "The referee stood nearby.", "Players in red attempted to block.", "One player in red fell down.", "Players from both teams tumbled to the ground."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 779}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The white smoke moves quickly, with two holographic images of airplanes appearing in succession and flying to the left and right, while the image of the Earth in the center continues to rotate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 780, "extra_info": {"duration": 10.0, "events": ["White smoke moves quickly.", "First holographic airplane image appears.", "Second holographic airplane image appears.", "First airplane image flies to the left.", "Second airplane image flies to the right.", "Image of Earth continues to rotate."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 780}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The subtitles move from left to right in sequence, consisting of the red \"warning,\" the yellow \"alert,\" the green \"stand by,\" and the blue \"online.\"", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 781, "extra_info": {"duration": 9.48, "events": ["The subtitles move from left to right.", "The red 'warning' appears sequentially.", "The yellow 'alert' follows after the red 'warning'.", "The green 'stand by' comes next in sequence.", "The blue 'online' displays lastly."], "n_events": 5, "n_shots": 1, "n_subjects": 4}, "vid": 781}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two men shake hands, bump fists, point at each other, swaying their bodies.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 782, "extra_info": {"duration": 5.67, "events": ["Two men shake hands.", "They bump fists.", "They point at each other.", "Men sway their bodies."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 782}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera angle is a high-angle shot moving forward to capture the car driving on the road, surrounded by various buildings.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 783, "extra_info": {"duration": 16.73, "events": ["Camera moves forward", "Car drives on the road"], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 783}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person is continuously hitting a punching bag in a dark environment.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 784, "extra_info": {"duration": 17.22, "events": ["A person hits a punching bag.", "Punching bag swings upon impact."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 784}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person in a spacesuit is driving a four-wheeled vehicle on a planet in outer space, and the camera switches to a close-up shot of the person's face and the front of the vehicle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 785, "extra_info": {"duration": 16.98, "events": ["A person drives a four-wheeled vehicle on a planet."], "n_events": 1, "n_shots": 2, "n_subjects": 2}, "vid": 785}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "After the plane landed, it continued to taxi forward, with dust and smoke rising from all around. The camera follows the movement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 786, "extra_info": {"duration": 18.45, "events": ["The plane landed.", "The plane taxied forward.", "Dust rose from the ground.", "Smoke rose from the ground."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 786}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman bent down and said something to the baby, swayed her body, then opened her mouth wide, looked up, lifted the baby up with both hands, and kissed the baby's belly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 787, "extra_info": {"duration": 21.62, "events": ["The woman bent down.", "The woman said something to the baby.", "The woman swayed her body.", "The woman opened her mouth wide.", "The woman looked up.", "The woman lifted the baby up with both hands.", "The woman kissed the baby's belly."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 787}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A close-up of a man's hands being handcuffed, two other men grabbing this man's arms and walking forward together. The camera follows the movement until they stop at the doorway, where the man on the right reaches into his pocket searching for something, while the other two men turn to the left.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 788, "extra_info": {"duration": 19.72, "events": ["Hands being handcuffed", "Two men grab the man's arms", "Group walks forward together", "They stop at the doorway", "Man on the right reaches into his pocket", "Other two men turn to the left"], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 788}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Red smoke wafts through the air as a group of soldiers carrying shields and holding tools advance forward, with other pedestrians walking ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 789, "extra_info": {"duration": 10.91, "events": ["Red smoke wafts through the air", "Soldiers advance forward carrying shields", "Soldiers hold tools", "Pedestrians walk ahead"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 789}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Four people are queueing up in a frenzy, a man with his arms raised swaying back and forth, dancing, while the other three are holding microphones and singing. They make eye contact and interact from time to time, then they raise their glasses, clink them together, and continue to sway their bodies while singing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 790, "extra_info": {"duration": 18.65, "events": ["Four people queue up in a frenzy.", "A man dances with arms raised.", "Three individuals sing into microphones.", "People make eye contact periodically.", "Individuals interact during singing.", "All raise their glasses.", "They clink glasses together.", "People continue to sway while singing."], "n_events": 8, "n_shots": 1, "n_subjects": 4}, "vid": 790}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The Canadian flag flutters in the wind, the camera cuts to a close-up to show the image of the Canadian flag fluttering in the breeze.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 791, "extra_info": {"duration": 11.08, "events": ["The Canadian flag flutters in the wind."], "n_events": 1, "n_shots": 2, "n_subjects": 1}, "vid": 791}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera moves downward, displaying a panoramic view of Earth, with subtitles and icons appearing in the middle. The camera zooms in, the subtitles and icons disappear, and the image changes from blue to green and then turns completely white.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 792, "extra_info": {"duration": 11.04, "events": ["The camera moves downward.", "Subtitles appear in the middle.", "Icons appear in the middle.", "The camera zooms in.", "The subtitles disappear.", "The icons disappear.", "The image changes from blue to green.", "The image turns completely white."], "n_events": 8, "n_shots": 2, "n_subjects": 1}, "vid": 792}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The man is wearing headphones, dancing with his arms spread out, moving back and forth, walking forward, and dancing face to face with another man. They high-five and then hug each other, with other people dancing around them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 793, "extra_info": {"duration": 24.46, "events": ["The man dances with arms spread out.", "The man moves back and forth.", "The man walks forward.", "Two men dance face to face.", "The men high-five each other.", "The men hug each other.", "Other people dance around them."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 793}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The numbers appear in sequence enlarged at the center of the screen, and finally the numbers disappear, turning the screen white.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 794, "extra_info": {"duration": 11.32, "events": ["Numbers appear in sequence.", "Numbers become enlarged.", "Numbers disappear.", "Screen turns white."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 794}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The giraffe runs forward on the savannah, with the camera following the movement, and then other giraffes appear. Some are standing still, while others are running along with it. Then it stops, turns its head, and looks to the right, and the other giraffes walk forward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 795, "extra_info": {"duration": 30.16, "events": ["The giraffe runs forward on the savannah.", "Other giraffes appear.", "Some giraffes are standing still.", "Other giraffes are running.", "The giraffe stops.", "The giraffe turns its head.", "The giraffe looks to the right.", "Other giraffes walk forward."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 795}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the bustling street, a red bus approaches, with motorcycles passing by in front, followed by some cars driving behind. A person riding a bicycle appears from the right and heads forward, while pedestrians move about on the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 796, "extra_info": {"duration": 20.19, "events": ["A red bus approaches.", "Motorcycles pass by in front.", "Cars drive behind the bus.", "A person on a bicycle appears.", "The person rides the bicycle forward.", "Pedestrians move about on the road."], "n_events": 6, "n_shots": 1, "n_subjects": 6}, "vid": 796}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The camera pans left, showing the vast plains and lake water as well as distant mountains, with several houses around.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 797, "extra_info": {"duration": 18.12, "events": ["The camera pans left."], "n_events": 1, "n_shots": 1, "n_subjects": 1}, "vid": 797}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A man and woman sit facing each other in conversation. The woman says something as she looks up at the man and then raises her left hand to signal him to leave. The man angrily slams the pen in his hand down on the table, then stands up and walks away. The woman watches the scene of the man leaving and then withdraws her hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 798, "extra_info": {"duration": 12.92, "events": ["Woman says something to man.", "Woman looks up at man.", "Woman raises left hand to signal man.", "Man slams pen down on table.", "Man stands up.", "Man walks away.", "Woman watches man leaving.", "Woman withdraws hand."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 798}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The woman, while speaking to the horse, caressed it as it chewed on its fodder.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 799, "extra_info": {"duration": 28.9, "events": ["The woman speaks to the horse.", "The woman caresses the horse.", "The horse chews on its fodder."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 799}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Multiple screens arranged in a matrix are moving up and down within the frame, each screen displaying different content that is constantly changing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/videvo", "task": "video/caption", "idx": 800, "extra_info": {"duration": 14.25, "events": ["Multiple screens are moving up.", "Multiple screens are moving down.", "Each screen displays different content.", "Content on screens is constantly changing."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 800}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a clear day where a red car rushed toward some individuals and a bicycle. The people managed to evade the car, however, the bicycle was utterly destroyed in the process. The bicycle was noticeably yellow and black in color.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 801, "extra_info": {"duration": 10.01, "events": ["A clear day presents.", "A red car rushes toward individuals.", "Individuals evade the red car.", "The red car heads toward a bicycle.", "The red car destroys the yellow and black bicycle."], "n_events": 5, "n_shots": 10, "n_subjects": 3}, "vid": 801}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures that at a traffic light junction, an orange taxi and an orange truck passed when the light turned green, followed by a red truck with black tires. When the light is yellow, the red truck stops and waits for the green light. When the light is green, the red truck and another orange truck behind the truck pass through the junction.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 802, "extra_info": {"duration": 12.0, "events": ["Traffic light turns green.", "Orange taxi passes through the junction.", "Orange truck follows the orange taxi.", "Red truck with black tires passes through the junction.", "Traffic light turns yellow.", "Red truck stops at the yellow light.", "Traffic light turns green again.", "Red truck resumes motion through the junction.", "Another orange truck passes the junction."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 802}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video records that a sunny day, a person with black sunglasses, stud ears, a gray coat and a shaggy face, standing besides the motorcycle, threw a small cigarette case or box casually to the ground under the trees with left hand. Then this person raised the left hand to keep out the wind. The person's right hand turned on the lighter and lighted the cigarette. The green lighter and began to smoke on a road surrounded by thick green trees.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 803, "extra_info": {"duration": 6.97, "events": ["Person throws cigarette case to ground", "Person raises left hand to block wind", "Person turns on lighter with right hand", "Person lights cigarette", "Person begins to smoke"], "n_events": 5, "n_shots": 5, "n_subjects": 1}, "vid": 803}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features three animated characters: an elderly individual with glasses sitting in a red manual wheelchair; a young person sporting a ponytail in a red motorized wheelchair; and a third character with a prosthetic right foot, standing to the left of the second person.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 804, "extra_info": {"duration": 8.0, "events": [], "n_events": 0, "n_shots": 1, "n_subjects": 3}, "vid": 804}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video illustrates a person cooking in the kitchen. In the first snippet, an induction cooker is observed on the kitchen counter and a grey-blue casseroid dish is placed on it. The person proceeds to add ingredients to the pot. The subsequent clip shows the person chopping a sausage. The person is clad in thin black gloves, with a black watch featured on their right wrist. They demonstrate the method of cutting the sausage, gripping it on a cutting board with their left hand while slicing it into pieces with a knife held in their right hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 805, "extra_info": {"duration": 11.98, "events": ["Person places casserole dish on induction cooker", "Person adds ingredients to the pot", "Person chops a sausage", "Person grips sausage with left hand", "Person slices sausage with knife"], "n_events": 5, "n_shots": 2, "n_subjects": 1}, "vid": 805}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays an individual with dark skin, curly hair, and a white T-shirt, utilizing an electric toothbrush to brush their teeth. Following this, they turn on the bathtub tap, situated to their right, and the shower starts running. The turning of the faucet to the right is intended to source water for the bath. Concurrently, a basket with toiletries can be seen stored.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 806, "extra_info": {"duration": 11.97, "events": ["Individual brushes teeth with electric toothbrush.", "Individual turns on bathtub tap.", "Shower starts running.", "Basket with toiletries appears."], "n_events": 4, "n_shots": 3, "n_subjects": 1}, "vid": 806}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures a football match taking place on a green field between two teams. One team is dressed in yellow attire with matching yellow socks, and the other team sports green clothes complimented with green socks. The goalkeeper from the yellow team intercepts the ball, thereby defending the goal. The green team's goal is under assault but manages to prevent a score, thanks to a well-placed block by a player wearing a green jersey. Ultimately, no points are accrued by either team.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 807, "extra_info": {"duration": 10.0, "events": ["Football match takes place on field.", "Team in yellow attire intercepts the ball.", "Goalkeeper from yellow team defends goal.", "Green team's goal faces assault.", "Player in green jersey blocks the ball."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 807}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This animated short film features an individual discussing three methods to unwind and alleviate stress. The first method is reading a book on a sofa. The second is taking a warm bath in a bathtub. Finally, the third method is listening to music while lying on a bed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 808, "extra_info": {"duration": 9.97, "events": ["Individual discusses stress relief methods.", "Individual reads a book on a sofa.", "Individual takes a warm bath in a bathtub.", "Individual listens to music on a bed."], "n_events": 4, "n_shots": 4, "n_subjects": 2}, "vid": 808}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a black and white dog being leashed by its owner, with a grey basket or container affixed on the rear wheel of a bicycle. Throughout the video, the owner rides the bicycle, prompting the dog to jump into the basket from the ground, and subsequently jump back onto the ground from the basket.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 809, "extra_info": {"duration": 8.0, "events": ["Owner leashes the dog.", "Owner affixes grey basket on the bicycle rear wheel.", "Owner rides the bicycle.", "Dog jumps into the basket from the ground.", "Dog jumps onto the ground from the basket."], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 809}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "From the video, we can see the dog is kept on a leash by the owner who trys to pull the dog behind a circle with purple, white and yellow. Then the dog jumped out of the circle, and then climbed the stairs.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 810, "extra_info": {"duration": 9.99, "events": ["The owner keeps the dog on a leash.", "The owner tries to pull the dog behind a circle.", "The dog jumps out of the circle.", "The dog climbs the stairs."], "n_events": 4, "n_shots": 2, "n_subjects": 2}, "vid": 810}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video records On the badminton court, a player wearing a red shirt and a player wearing white autumn clothes playing badminton. There were a few referees sitting around and spectators watching the game. The badminton fell to the ground during the ninth pass. The badminton ball was hit to the ground once by the person in red clothes. The badminton drops once in front of the person in white clothes and blue pants. The white team loses points. The player failed to catch the badminton for one time after the badminton flew over the net. The badminton fell down between two white lines for once.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 811, "extra_info": {"duration": 9.0, "events": ["Player in red shirt hits badminton.", "Badminton falls to ground on ninth pass.", "Referees sit around the court.", "Spectators watch the game.", "Person in red clothes strikes badminton to ground.", "Badminton drops in front of person in white clothes.", "White team loses point.", "Player fails to catch flying badminton.", "Badminton lands between two white lines."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 811}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows an individual wearing a black, woolly sun-protective sleeve unlocking their car and shutting the door from the inside. Subsequently, they hold a piece of paper with a QR code. The top section of the door's interior is grey, and the bottom is black. A car key with a red plastic ring and two attached cards is visible. The key is primarily black, with a smidgen of red in the middle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 812, "extra_info": {"duration": 5.99, "events": ["Individual wearing sun-protective sleeve unlocks the car", "Individual shuts the car door from the inside", "Individual holds a piece of paper with a QR code", "Car key with red plastic ring becomes visible"], "n_events": 4, "n_shots": 4, "n_subjects": 2}, "vid": 812}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures a pair standing adjacent to a shelf in a room. The person on the right of the frame is donned in a shirt with blue and white vertical stripes, brown footwear, and carries a black balloon affixed to a stick. The person on the left, having long blonde hair, is attired in a pink and white checked dress complemented by golden sandals. The individual with long hair pops the black balloon, leading to a shower of small yellow papers.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 813, "extra_info": {"duration": 6.01, "events": ["Person on the right carries a black balloon.", "Person with long hair pops the balloon.", "Small yellow papers shower from the popped balloon."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 813}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A conversation is happening between people which is incidental to five cars moving by in the background. Four cars are moving from the right to the left of the frame and one car from the left to the right. Of those four cars, the first is black, the second and the third are grey, and the fourth car is white.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 814, "extra_info": {"duration": 9.99, "events": ["People engage in conversation.", "First black car moves right to left.", "Second grey car moves right to left.", "Third grey car moves right to left.", "Fourth white car moves right to left.", "One car moves left to right."], "n_events": 6, "n_shots": 1, "n_subjects": 4}, "vid": 814}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features two individuals engaged in a friendly boxing match, seven spectators are observing the scene attentively. One of the fighters is attired in a dark blue t-shirt, blue pants, and red boxing gloves, while the other is dressed in a white t-shirt, black pants, and black boxing gloves. They swiftly exchange blows, showcasing strong camaraderie and sportsmanship. The fighter in dark blue outperforms his counterpart by successfully landing six attacks, despite the fighter in white's earnest attempts to evade them. In the midst of the action, a spectator wearing a white t-shirt is seen capturing the thrilling match on his mobile phone. The dark blue fighter is particularly adept at targeting the face of the fighter in white, landing a total of six punches throughout the contest.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 815, "extra_info": {"duration": 10.01, "events": ["Two individuals engage in a friendly boxing match.", "Seven spectators observe the match attentively.", "Fighter in dark blue lands six attacks.", "Fighter in white attempts to evade attacks.", "Spectator captures the match on mobile phone.", "Fighter in dark blue targets the other's face."], "n_events": 6, "n_shots": 4, "n_subjects": 3}, "vid": 815}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a pet shop with an array of orange and white birds in an aviary. There are also black and white cats seen in a litter box and different types of fishes swimming in fish tanks. The order of their appearance is: first, the birds; second, the cats; and third, the fish. The nouns must appear in an answer in the same sequence - birds followed by cats and then fish.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 816, "extra_info": {"duration": 11.01, "events": ["Orange and white birds perch in an aviary.", "Black and white cats scratch in a litter box.", "Different types of fish glide through water in tanks."], "n_events": 3, "n_shots": 7, "n_subjects": 5}, "vid": 816}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a sunny day where two individuals are practicing their rugby football skills on a field. The rugby football they're using is brown. There's one individual, identified as person A, with their hair tied in a ponytail, donned in a white T-shirt, blue pants, white socks, and gray shoes. They also sport a black watch on their right hand. The other, identified as person B, is noticeable with numerous black braids, a blue coat, gray shorts, and stud earrings. The video begins with person A sending the first pass and person B skillfully catching it. They proceed to pass the ball back and forth a total of five times, enjoying their game.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 817, "extra_info": {"duration": 11.0, "events": ["Person A sends the first pass", "Person B catches the ball skillfully", "They pass the ball back and forth", "Person A catches the return pass", "Person B receives another pass", "The ball is thrown to Person A again", "Person B makes a successful catch", "Another pass is completed to Person B", "Person A receives the ball once more", "They enjoy their game of rugby football"], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 817}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a football match taking place on a green football field. The teams playing are the green team and the blue team. The players in the green team are dressed in green uniforms, green socks, and black trousers, whilst those in the blue team are wearing blue uniforms, trousers, and socks. In the game, the blue team loses possession of the ball, giving the green team an opportunity to strike. The orange-dressed goalkeeper fails to save the ball, allowing the green team's player, numbered 8, to score a goal. The green team erupts in excitement, while the blue team fails to defend their goal resulting in the green team scoring a point.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 818, "extra_info": {"duration": 10.01, "events": ["The football match takes place.", "The blue team loses possession.", "The green team seizes opportunity.", "The green player number 8 shoots.", "The orange-dressed goalkeeper fails to save.", "The ball crosses the goal line.", "The green team scores a goal.", "The green team celebrates excitedly.", "The blue team fails to defend."], "n_events": 9, "n_shots": 2, "n_subjects": 3}, "vid": 818}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The animation features two individuals garbed in white. To the left stands a slender figure and to the right, a corpulent one. The person on the right holds a controller in hand and guides it towards a chicken. With a press of a button on the controller, a burst of yellow light is emitted which consequently transforms the chicken into an egg. Meanwhile, a person wearing black-framed glasses and a white cap, presumably in a lab setting, points a similar machine at the chicken.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 819, "extra_info": {"duration": 10.0, "events": ["The slender figure stands to the left.", "The corpulent individual stands to the right.", "The corpulent individual holds a controller.", "The corpulent individual guides the controller towards the chicken.", "The corpulent individual presses a button on the controller.", "A burst of yellow light is emitted from the controller.", "The chicken transforms into an egg.", "A person wears black-framed glasses and a white cap.", "The person points a machine at the chicken."], "n_events": 9, "n_shots": 4, "n_subjects": 3}, "vid": 819}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures an individual with long hair, dressed in a striking red wrap dress, strutting on stage with an enchanting smile. This person is seen wearing a ring on their right ring finger and a piece of gold jewelry on their left wrist. They gracefully show off their beautiful dress through a variety of poses. Taking on the role of a model during a T station show, they stir the interest of the audience, many of whom are pictured capturing their performance on camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 820, "extra_info": {"duration": 5.96, "events": ["The individual struts on stage.", "The individual smiles enchantingly.", "The individual wears a ring.", "The individual showcases a gold bracelet.", "The individual poses with the dress.", "The individual assumes a model role.", "The audience captures the performance."], "n_events": 7, "n_shots": 4, "n_subjects": 2}, "vid": 820}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features one person clad in a blue shirt and white pants with brown hair and red nail polish. Famously, there are four parrots in view. The person uses food to lure the parrots. Of note is the third parrot from the left, or second from the right. This particular parrot spreads its wings and notably flies to the person's right hand from atop the cage. Distinguishing it from the rest are green feathers and an orange bosom.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 821, "extra_info": {"duration": 10.01, "events": ["One person lures parrots with food.", "Third parrot from left flies to person's right hand."], "n_events": 2, "n_shots": 1, "n_subjects": 3}, "vid": 821}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video presents an individual in the process of creating flower specimens. The person, who has blond hair and is wearing a black T-shirt, opens a hefty book and places some pink flowers inside. They then close the book and exert pressure on it by using their right elbow. The person continues to use their elbow to press down on the book several times, effectively pressing the flowers within. They put significant effort into pressing the book.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 822, "extra_info": {"duration": 9.99, "events": ["Individual opens a hefty book.", "Individual places pink flowers inside book.", "Individual closes the book.", "Individual presses on book with right elbow.", "Individual repositions elbow to press book multiple times.", "Individual exerts significant effort pressing the book."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 822}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video is set in a classroom where a person, wearing a red coat and a white T-shirt underneath, ignites a piece of paper on a table using a lighter. The paper promptly catches fire, causing shock and consternation among the four individuals sitting nearby.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 823, "extra_info": {"duration": 10.0, "events": ["Person in red coat ignites paper with lighter", "Paper catches fire on table", "Four individuals react to fire", "Shock appears on individuals' faces", "Consternation spreads among the individuals"], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 823}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a person wearing a green T-shirt, black shorts, gray sneakers, a ring on the left ring finger, and a black watch on the left wrist. This individual is performing easy push-ups in a room filled with sports equipment. A total of five push-ups were performed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 824, "extra_info": {"duration": 9.01, "events": ["Person starts first push-up.", "Person reaches lowest point of first push-up.", "Person completes first push-up.", "Person starts second push-up.", "Person completes second push-up.", "Person starts third push-up.", "Person completes third push-up.", "Person starts fourth push-up.", "Person completes fourth push-up.", "Person starts fifth push-up."], "n_events": 10, "n_shots": 2, "n_subjects": 1}, "vid": 824}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A black and brown cat is seen standing upright in a room, with a transparent, plastic storage box in front of it. The box contains brown food and has a blue lid. The cat reaches inside the box with its right paw, takes out some food, and proceeds to eat it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 825, "extra_info": {"duration": 6.0, "events": ["The cat stands upright.", "The cat reaches inside the box.", "The cat takes out some food.", "The cat eats the food."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 825}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video showcases a team battle in the game League of Legends. The red team progress forward, killing the blue team's characters one by one without sustaining any casualty. Various spells are cast by the red team, with animations of these spells visible on-screen. Notably, two players from the red team are responsible for eliminating three characters from the blue team. In this four on four matchup, the red team successfully takes down three characters from the blue team, leaving them with just one.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 826, "extra_info": {"duration": 7.0, "events": ["The red team advances forward", "The red team eliminates the first blue character", "The red team kills the second blue character", "The red team takes out the third blue character", "Various spells are cast by the red team", "Animations of spells appear on-screen", "Two red team players eliminate three blue characters", "The red team engages in a four on four matchup", "The red team leaves the blue team with one character"], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 826}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The individual is practicing boxing at home. The routine involves two right-hand punches followed by a right, left, and another right-hand punch combined with a squat. This pattern is then repeated using both right and left hands. The person moves to the right, executing two right-hand punches then steps back and repeats two right-hand punches again.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 827, "extra_info": {"duration": 11.0, "events": ["Individual practices boxing at home.", "Individual throws two right-hand punches.", "Individual performs a combo: right, left, right-hand punch with a squat.", "Pattern is repeated with both hands.", "Individual moves to the right.", "Individual executes two right-hand punches.", "Individual steps back.", "Individual repeats two right-hand punches."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 827}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A child clad in gray attire gleefully runs towards a house's door, intending to prank its occupants. The mischievous child rings the doorbell and then conceals himself. A bemused individual, dressed in green, opens the door to find nobody there. As confusion grows, this individual sticks their head out the door to take a thorough look around their vicinity, perplexed about who might be playing a prank or who could have mistakenly rung their doorbell. Despite their efforts, they are left clueless and shut the door.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 828, "extra_info": {"duration": 11.0, "events": ["A child runs towards a house's door.", "The child rings the doorbell.", "The child hides.", "An individual opens the door.", "The individual looks around.", "The individual shows confusion.", "The individual sticks their head out.", "The individual searches the vicinity.", "The individual finds no one.", "The individual closes the door."], "n_events": 10, "n_shots": 6, "n_subjects": 2}, "vid": 828}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The individual clothed in black and wearing gloves tosses a ball. Another person, attired in white and positioned upright, intercepts and throws the ball towards a different direction. The person in black executes a dive in an attempt to catch the ball but misses. Simultaneously, the individual in white, also wearing gloves, succeeds in catching the ball.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 829, "extra_info": {"duration": 7.0, "events": ["Individual in black tosses a ball", "Person in white intercepts the ball", "Person in white throws the ball", "Individual in black executes a dive", "Individual in black misses the ball", "Person in white catches the ball"], "n_events": 6, "n_shots": 2, "n_subjects": 8}, "vid": 829}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The car is moving in a straight line on a road. At a junction, the car veers to the left, moving onto another road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 830, "extra_info": {"duration": 10.0, "events": ["The car moves in a straight line.", "The car veers to the left.", "The car enters another road."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 830}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a living room where some pieces of food are scattered on the floor. A brown and white dog is seen lying down while munching on these floor foods. Apart from this dog, there is another smaller dog with black and white fur. This small canine is being hand-fed by a person.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 831, "extra_info": {"duration": 10.01, "events": ["Pieces of food scatter on the floor.", "A brown and white dog lies down.", "The brown and white dog munches on floor foods.", "A smaller black and white dog appears.", "A person hand-feeds the smaller black and white dog."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 831}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a street, a motorcycle carrying two people is seen. The rider is donned in a blue cowboy outfit, while the passenger, who has long hair, is dressed in pink and is seated behind the rider. When the traffic light turns red, causing the other vehicles to halt, this motorcycle blatantly flouts the traffic rule and runs the red light. An attempt by the traffic police to intervene is completely ignored by the rider in the blue cowboy suit. Disregarding the police warning, the rider continues his journey, showing a grave lack of respect towards the law.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 832, "extra_info": {"duration": 6.01, "events": ["A motorcycle carries two people on a street.", "The rider wears a blue cowboy outfit.", "The passenger in pink sits behind the rider.", "The traffic light turns red.", "Other vehicles halt at the red light.", "The motorcycle runs the red light.", "The traffic police attempt to intervene.", "The rider in the blue cowboy suit ignores the police warning.", "The rider continues the journey."], "n_events": 9, "n_shots": 6, "n_subjects": 4}, "vid": 832}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays a basketball match between a team in white uniforms and one in blue uniforms. The player in blue obtains the ball and jumps, simultaneously pursued by two players in white, attempting to intercept the ball. The player in blue proceeds to take a shot but fails. As both the players in white and blue make an attempt to secure the rebound, the player in blue tumbles to the ground. The player in white unknowingly turns and is tripped over by the fallen player in blue.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 833, "extra_info": {"duration": 7.0, "events": ["Player in blue obtains the ball.", "Player in blue jumps.", "Two players in white attempt to intercept.", "Player in blue takes a shot.", "Player in blue fails the shot.", "Players scramble for the rebound.", "Player in blue tumbles to the ground.", "Player in white turns.", "Player in white is tripped over.", "Fallen player causes the trip."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 833}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video takes place on a vibrant green soccer field where an intense game is underway. One team is dressed in white jerseys and socks, while the opposing team is sporting red and black attire, complemented with black socks. A player from the white team skillfully controls the ball and successfully shoots it towards the rival team's goal. The goalkeeper, donned in orange, fails to stop the ball, resulting in a goal for the white team. The black and red team drops a point. In response, the crowd rises to their feet in an outpour of joy and merriment.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 834, "extra_info": {"duration": 6.0, "events": ["Soccer game is underway on green field", "Player from white team controls the ball", "Player from white team shoots towards goal", "Goalkeeper in orange fails to stop the ball", "White team scores a goal", "Red and black team drops a point", "Crowd rises to their feet", "Crowd expresses joy and merriment"], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 834}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, the person is riding a red motorcycle and a yellow bus with two front lights on is coming. To avoid hitting the yellow bus, the person riding a motorcycle chooeses to turn left. If not, there would be an accident. Some people including drivers or passengers would get hurt or be in danger. And the bus driver would be frightened.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 835, "extra_info": {"duration": 6.0, "events": ["Person rides red motorcycle", "Yellow bus approaches with lights on", "Motorcycle turns left", "Bus driver gets frightened"], "n_events": 4, "n_shots": 1, "n_subjects": 3}, "vid": 835}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features a white and blue bus approaching. To the right of the bus, there's a group of cyclists riding. If these cyclists were to suddenly make a left turn, it may cause an accident. Such a crash could potentially harm multiple individuals, including drivers, passengers, and cyclists, and damage the bus or bicycles.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 836, "extra_info": {"duration": 5.0, "events": ["White and blue bus approaches", "Group of cyclists rides to the right", "Cyclists make a left turn", "Accident occurs", "Individuals get potentially harmed", "Bus sustains damage", "Bicycles get damaged"], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 836}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features a football game involving two teams dressed in red and black respectively. A certain player from the red team, identifiable by his white socks, abruptly hits a player from the black team. Following this incident, the black team member is seen on the ground, prompting celebratory reactions from the black team. The referee intervenes to adjudicate a committed foul. The player in red jersey with '5' as his number is deemed to have broken the rules, thus relegating the fault to the red team and not their opponents, the black team.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 837, "extra_info": {"duration": 11.0, "events": ["A player from the red team hits a player from the black team.", "The player from the black team falls to the ground.", "The black team members react with celebration.", "The referee intervenes to adjudicate the foul.", "The referee identifies the player in red jersey number 5.", "The referee attributes the foul to the red team."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 837}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, a football match is taking place between two teams. A player from the red team strikes the ball towards the goal, but the goalkeeper from the blue team skillfully intercepts it, preventing the red team from scoring. If the goalkeeper had failed to catch the ball, allowing it to cross the goal line, the red team's score would have increased by one point. Despite the red team's efforts, their score remained unchanged, while the blue team's score also remained the same.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 838, "extra_info": {"duration": 6.0, "events": ["A football match takes place.", "Player from red team strikes ball.", "Football heads towards blue team's goal.", "Goalkeeper from blue team intercepts ball.", "Goalkeeper prevents scoring.", "Red team's score remains unchanged.", "Blue team's score remains the same."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 838}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a person first uses a knife to cut tofu into cubes on a cutting board. Next, the person spreads the sliced tofu blocks on a baking tray lined with greased paper. Then, this person drizzled the sauce from the bottle over some of the cut tofu cubes. Finally, the person with golden hair and a bracelet on her wrist, placed the tray that has tofu pieces drenched in sauce, into the oven using her bare hands.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 839, "extra_info": {"duration": 12.0, "events": ["Person cuts tofu into cubes.", "Person spreads tofu cubes on baking tray.", "Person lines baking tray with greased paper.", "Person drizzles sauce over tofu cubes.", "Person with golden hair places tray in oven."], "n_events": 5, "n_shots": 2, "n_subjects": 1}, "vid": 839}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays the preparations of firefighters before embarking on a mission. Each frame distinctly highlights the firefighters in their respective gear. These include individuals dressed in red safety helmets, some donning yellow helmets, a single individual wearing a white safety helmet, and notably, one person not adhering to the safety standard by wearing a black cap instead of a helmet.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 840, "extra_info": {"duration": 8.0, "events": ["Firefighters prepare before a mission", "Individuals dress in red safety helmets", "Some firefighters don yellow helmets", "A person wears a white safety helmet", "One person dons a black cap"], "n_events": 5, "n_shots": 3, "n_subjects": 2}, "vid": 840}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual pours blue curacao from a small bottle into a glass situated on a table. Following this, the same individual adds simple syrup to the glass, using the same bottle. Lastly, lemon juice is added to the glass from the same bottle. Throughout the video, specific liquid measurements are displayed at the top left corner of the screen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 841, "extra_info": {"duration": 8.0, "events": ["Individual pours blue curacao into a glass.", "Individual adds simple syrup using the bottle.", "Individual adds lemon juice from the bottle.", "Liquid measurements display at top left corner."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 841}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, an individual dressed in a white vest and blue pants is performing sit-ups on a black mat. They are also wearing black shoes. Over the span of nine seconds, the person performs a total of three sit-ups.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 842, "extra_info": {"duration": 9.0, "events": ["Individual lies down on a black mat.", "Individual secures feet on the ground.", "Individual places hands behind head.", "Individual lifts torso upwards.", "Individual completes one sit-up.", "Individual lowers torso back down.", "Individual performs second sit-up.", "Individual returns to starting position.", "Individual completes third sit-up.", "Individual finishes the exercise."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 842}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, an individual places a rubber band around the middle finger and thumb of their right hand. They then begin to spin the rubber band using their left hand. Unexpectedly, the rubber band slips off the middle finger.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 843, "extra_info": {"duration": 5.99, "events": ["Individual places a rubber band around middle finger and thumb.", "Individual begins to spin the rubber band with left hand.", "Rubber band slips off the middle finger."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 843}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip depicts an individual eating in a car that is not being driven by anyone. One might perceive that there may be potential for an accident or other hazards due to the absence of a driver.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 844, "extra_info": {"duration": 6.0, "events": ["Individual eats in the car.", "Car moves without a driver."], "n_events": 2, "n_shots": 2, "n_subjects": 1}, "vid": 844}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A monkey clad in a green suit climbs onto the lap of an individual wearing a blue T-shirt. Subsequently, the monkey leaps onto an orange chair before jumping onto the floor beside the person. After stumbling briefly, the monkey descends from the chair. As the monkey gradually moves away from the person, the individual gestures a wave towards the monkey. The monkey then hops down a short step and continues to saunter away. The monkey strides past a vibrant painting towards the exit.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 845, "extra_info": {"duration": 12.0, "events": ["Monkey climbs onto individual's lap.", "Monkey leaps onto orange chair.", "Monkey jumps onto floor.", "Monkey stumbles briefly.", "Monkey descends from chair.", "Individual gestures a wave towards monkey.", "Monkey hops down a step.", "Monkey saunters away.", "Monkey strides past painting.", "Monkey heads towards exit."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 845}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video depicts an individual clad in black attire, carrying a red bag, engaging in conversation with a few drivers on the road. Initially, the individual attempts to converse with the driver of a white car; however, the outcome seems unfavorable. They continue their efforts to communicate with another driver of a white vehicle, but again, it appears unsuccessful. The video suggests that the individual is trying to secure a free ride.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 846, "extra_info": {"duration": 11.83, "events": ["Individual wearing black approaches white car.", "Individual attempts conversation with first white car driver.", "First conversation appears unsuccessful.", "Individual approaches another white vehicle.", "Individual attempts conversation with second vehicle's driver.", "Second conversation seems unsuccessful.", "Individual carries red bag throughout."], "n_events": 7, "n_shots": 2, "n_subjects": 1}, "vid": 846}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip showcases a person sitting in the passenger seat of a moving car, filming. As the car moves forward, the trees appear to be moving backward.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 847, "extra_info": {"duration": 5.0, "events": ["A person sits in the passenger seat.", "The car moves forward.", "Trees appear to move backward."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 847}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip depicts a person enjoying a meal at a restaurant. On the table, there's a dish filled with a variety of cakes, a spoon, and some red sauce. The assorted cakes include a yellow pancake and a macaron. The person initially presents the yellow pancake to the camera and proceeds to break it into two halves. Following this, they apply some red sauce to one half of the pancake.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 848, "extra_info": {"duration": 10.01, "events": ["A person enjoys a meal at a restaurant", "Dish displays a variety of cakes", "Person presents yellow pancake to the camera", "Person breaks yellow pancake into two halves", "Person applies red sauce to half of the pancake"], "n_events": 5, "n_shots": 3, "n_subjects": 3}, "vid": 848}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases many individuals, both children and adults. A portion of these people are dressed in pants, while the remaining are garbed in swimwear like bikinis and other types of swimsuits. With a delighted spirit, they all run in the same direction, evidently heading towards a swimming or beach activity. Those clad in swimsuits could possibly be participating in an organized event or activity.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 849, "extra_info": {"duration": 9.01, "events": ["Individuals run towards a destination.", "Children participate in the activity.", "Adults join the running group.", "People dressed in pants run.", "Others wear swimwear.", "Participants head for swimming.", "Adults and children show excitement.", "People possibly join an event.", "Swimwear-clad individuals prepare for activity."], "n_events": 9, "n_shots": 1, "n_subjects": 1}, "vid": 849}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts an individual attired in black shorts-overalls, wearing a necklace and a headset. This individual is seen collecting various sheets from their bedroom for laundering. Initially, the person is shown standing by a nightstand near the room's door, removing a left-side pillowcase which is to be washed. They then proceed to remove the right-side pillowcase from another pillow and toss the pillow back onto the bed. Subsequently, the individual moves to the side of the bed closest to the window, gathers the bedding and holds them in their grasp. These sheets are intended to be laundered in a washing machine.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 850, "extra_info": {"duration": 8.98, "events": ["Individual removes a left-side pillowcase.", "Individual removes a right-side pillowcase.", "Individual tosses pillow onto bed.", "Individual gathers bedding from bed.", "Individual holds bedding for laundry."], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 850}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An elder with blonde hair, donned in a blue sweater and a blue apron with flower print, is seen holding a vacuum-sealed package of plant-based fries. He unrolls the four corners of the package over the kitchen sink.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 851, "extra_info": {"duration": 4.0, "events": ["The elder holds a vacuum-sealed package.", "The elder unrolls the package over the sink."], "n_events": 2, "n_shots": 1, "n_subjects": 1}, "vid": 851}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a recording from a car's perspective. It suddenly decelerates, even coming to a full stop in the middle of the road. As a result, the vehicle following behind collides with it. After the impact, the car spins to face the adjacent lane where a white truck is approaching and ends up in a head-on collision with it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 852, "extra_info": {"duration": 7.01, "events": ["The car decelerates suddenly.", "The car comes to a full stop on the road.", "The following vehicle collides with the stopping car.", "The stopping car spins post-collision.", "The spinning car faces the adjacent lane.", "A white truck approaches in the adjacent lane.", "The spinning car collides head-on with the white truck."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 852}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures an intersection on a crisp day with both lateral and longitudinal traffic in view. There are traffic signals in place to regulate the flow. The video displays a considerable amount of traffic. It features a grey car ahead of a red one, with several cars moving left to right. An individual is seen crossing the street behind a white car, also moving from left to right, indicating the traffic lights allow passage in that direction. The grey car in front of the red car is making a turn, suggesting the red car can do the same. However, the red car abruptly moves straight ahead, failing to make the turn, and collides with the white car traversing from left to right, leading to a traffic mishap.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 853, "extra_info": {"duration": 6.0, "events": ["Lateral and longitudinal traffic flows at intersection.", "Traffic signals regulate the flow.", "A grey car turns at the intersection.", "A red car moves straight ahead.", "Several cars move from left to right.", "An individual crosses the street.", "The white car moves from left to right.", "Pedestrians given passage by traffic lights.", "The grey car precedes the red car.", "The red car collides with the white car."], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 853}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features two teams, one dressed in white and the other in red. A member of the red team attempts to prevent a player from the white team from scoring a goal. Despite the red team's efforts, the white team successfully scores a goal, to which the audience reacts with enthusiastic applause and cheer. The crowd seems particularly delighted with the goal, possibly because they are backing the white team. They express their joy towards the team which scored and for the player who made the score.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 854, "extra_info": {"duration": 5.0, "events": ["Player from the red team attempts to block the white team member.", "White team member shoots towards the goal.", "Red team fails to prevent the goal.", "White team scores a goal.", "Audience reacts with enthusiastic applause.", "Crowd cheers loudly.", "Fans express joy towards the white team.", "Supporters celebrate the player who scored."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 854}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip features two football teams, one dressed in yellow shirts while the other in white and blue. A player from the team dressed in blue and white successfully scores a goal despite the yellow team's attempt to block him. However, they fail to stop him. The crowd erupts in cheers for the renowned football player Messi.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 855, "extra_info": {"duration": 10.0, "events": ["Two football teams compete on the field.", "One team wears yellow shirts.", "The other team dons white and blue.", "A player from the blue and white team shoots.", "The player scores a goal.", "The yellow team attempts to block the shot.", "The block attempt by the yellow team fails.", "The crowd erupts in cheers.", "The crowd celebrates football player Messi."], "n_events": 9, "n_shots": 5, "n_subjects": 4}, "vid": 855}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a basketball game between two teams: one clad in blue uniforms and the other in white uniforms. The team in white is on defense while a player from the blue team, designated with the number 27, holds the ball. The blue team player makes an attempt to swerve past the defensive line set by the white team. This culminates into the player shooting a successful three-pointer. The crowd instantly erupts into a chorus of cheers, rising to their feet in applause as the blue team secures a victory. It's evident that the audience is exuberant about supporting the winning team.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 856, "extra_info": {"duration": 6.01, "events": ["The basketball game is ongoing.", "The team in blue is in possession of the ball.", "Player 27 from the blue team holds the ball.", "Player 27 attempts to swerve past the white team's defense.", "Player 27 shoots a three-pointer.", "Player 27 scores the three-pointer.", "The crowd erupts into cheers.", "Spectators rise to their feet.", "Spectators applaud the blue team.", "The blue team secures a victory."], "n_events": 10, "n_shots": 3, "n_subjects": 4}, "vid": 856}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts an individual outfitted in a sleeveless shirt, denim shorts, and slippers. The person is seen setting up a trap system - placing the final piece and throwing a carrot into a self-made cage trap. The person tactfully imitates a bird's movements as they handle the carrot. The self-built cage houses three green bamboo sticks. Upon the carrot's entry into the bamboo cage, a wire mesh door promptly shuts.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 857, "extra_info": {"duration": 6.01, "events": ["Individual sets up a trap system.", "Individual places the final piece of trap.", "Individual throws a carrot into the cage.", "Individual imitates a bird's movements.", "The carrot enters the bamboo cage.", "Wire mesh door shuts promptly."], "n_events": 6, "n_shots": 3, "n_subjects": 1}, "vid": 857}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a conversation between two individuals. The individual on the right is dressed in a yellow T-shirt, a blue denim jacket, and black sneakers. He sports brown curly hair, sunglasses, and a black waist bag. On the left, the second person is dressed in a blue T-shirt, blue pants, and gray shoes. This person has a short black beard but is bald. At a certain point, both individuals stand up and the person in the blue T-shirt picks up a phone and an army green cap that were placed on a rock. They then walk across the lawn and the individual in the blue T-shirt gestures with his right hand to place the cap on his head.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 858, "extra_info": {"duration": 4.97, "events": ["Two individuals converse.", "Individual in yellow T-shirt stands up.", "Individual in blue T-shirt stands up.", "Individual in blue T-shirt picks up phone.", "Individual in blue T-shirt picks up army green cap.", "Individuals walk across the lawn.", "Individual in blue T-shirt gestures to place cap on head."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 858}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features individuals dressed in red and green dresses, black t-shirts, and blue hoodies, all of them smiling cheerfully and applauding. It also shows two distinctive individuals seated beside each other— the video's author, sporting a gray hoodie, holds a knife, and another person wearing glasses, clothed in a black T-shirt containing white script. The person in glasses, garbed in the black T-shirt, looks towards the table, while the individual in the gray hoodie extends the knife towards the cake, ready to cut it. All these individuals have congregated around the table to mark the birthday of the person clad in the gray hoodie, who is also adorned with earrings.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 859, "extra_info": {"duration": 9.0, "events": ["People dressed in colorful attire applaud.", "The video's author holds a knife.", "A person in glasses looks towards the table.", "The individual in the gray hoodie extends the knife.", "Individuals congregate around the table.", "A cake is ready to be cut."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 859}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a car behind a grey vehicle that fails to stop at a red light signal and proceeds to drive through it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 860, "extra_info": {"duration": 5.0, "events": ["The car follows a grey vehicle.", "Grey vehicle approaches a red light.", "Red light signal activates.", "Grey vehicle fails to stop.", "Grey vehicle drives through the red light."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 860}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a red car preparing to make a right turn. Simultaneously, a grey car emerges from the right side of the red car. The red car then halts to yield to the grey car.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 861, "extra_info": {"duration": 8.98, "events": ["Red car prepares to make a right turn.", "Grey car emerges from right side of red car.", "Red car halts to yield to grey car."], "n_events": 3, "n_shots": 1, "n_subjects": 2}, "vid": 861}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip illustrates a scene at an intersection where multiple cars are waiting for the signal. A black car unexpectedly drives on the wrong path, ignoring the red traffic signal. Simultaneously, a gray car  is preparing to cross the intersection from the right and narrowly avoids a collision with the errant black car. The gray car manages to brake in time while the black car swerves to the left, preventing an accident. The incident concludes with the gray car and the following vehicles safely proceeding through the intersection.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 862, "extra_info": {"duration": 9.01, "events": ["Cars wait for the signal at an intersection.", "A black car drives on the wrong path.", "The black car ignores the red traffic signal.", "A gray car prepares to cross the intersection.", "The gray car narrowly avoids a collision.", "The gray car manages to brake in time.", "The black car swerves to the left.", "The incident prevents an accident.", "The gray car proceeds through the intersection.", "The following vehicles safely continue."], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 862}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a scene on a road where a vehicle progressively slows down, coming to a halt at a red traffic light, adjacent to a black car. Both vehicles are poised to make a left turn, awaiting the traffic signal to turn green. Simultaneously, a white vehicle from the right intersection moves horizontally past. The black car along with the car next to it remains stationary until the light turns green, post the transit of the white vehicle. Various traffic signs are also detectable in the landscape.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 863, "extra_info": {"duration": 6.01, "events": ["A vehicle slows down on the road.", "The vehicle comes to a halt at a red light.", "A black car waits at the same light.", "The intended direction for both cars is a left turn.", "A white vehicle moves horizontally past from the right.", "Both the black car and the camera's car stay still.", "The traffic light turns green after the white car passes.", "The black car and the camera's car start to turn left."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 863}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays an individual wearing glasses, a cap, green t-shirt and shorts, white socks, and shoes, who is jogging at a consistent speed on a deserted road bounded by barriers on one side. This person is running alone. The surrounding area is teeming with verdant trees and grass on both sides of the road. After jogging vigorously along this pathway, the person proceeds to ascend the stairs leading towards a hill blanketed by green grass.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 864, "extra_info": {"duration": 6.0, "events": ["Individual wearing glasses jogs on deserted road.", "Individual runs at consistent speed.", "Road is bounded by barriers.", "Jogger ascends stairs.", "Stairs lead towards a grass-blanketed hill."], "n_events": 5, "n_shots": 4, "n_subjects": 1}, "vid": 864}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows eight swimmers competing in a two-hundred-meter race. The swimmer in lane number four outperforms the others, swimming faster and finishing first. This swimmer is wearing a white swimming cap. Trailing behind is the contestant in lane number five. The swimmer in lane number four completes the course in one minute and forty-two seconds. Representatives are stationed at the finish point, waiting for all swimmers to finish.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 865, "extra_info": {"duration": 7.0, "events": ["Eight swimmers compete in a race.", "Swimmer in lane four outperforms the others.", "Swimmer in lane four finishes first.", "Swimmer in lane five trails behind.", "Swimmer in lane four completes the course.", "Representatives wait at the finish point.", "All swimmers finish the race."], "n_events": 7, "n_shots": 1, "n_subjects": 8}, "vid": 865}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A basketball player, wearing jersey number twenty-three in white, tries to bypass an opponent in a blue jersey, number fifteen, to swiftly approach the basket. However, he misses the layup. The player's teammate, wearing jersey number fifteen in white, secures the rebound and scores the point. The crowd erupts in cheers, delighted by the scored point. Subsequently, player number thirty-three in a blue jersey readies himself to set the ball in motion from the sidelines.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 866, "extra_info": {"duration": 6.0, "events": ["Basketball player in jersey number twenty-three attempts bypass.", "Player in jersey number twenty-three misses the layup.", "Teammate in jersey number fifteen secures the rebound.", "Teammate in jersey number fifteen scores the point.", "The crowd erupts in cheers.", "Player in jersey number thirty-three readies to inbound ball."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 866}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The individual dressed in khaki attire manipulates a blue phone using their index finger to enable silent mode. Following this action, the individual flips the phone upside down and tosses it toward the bed. The person then proceeds to push the keyboard, which is black, white, and red in color, forward. After a small hand gesture, the person picks up a blue book titled \"Osce Revision Guide\" and holds it up to the camera.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 867, "extra_info": {"duration": 9.01, "events": ["Individual enables silent mode on blue phone", "Individual flips phone upside down", "Individual tosses phone toward bed", "Individual pushes keyboard forward", "Individual makes small hand gesture", "Individual picks up blue book", "Individual holds blue book up to camera"], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 867}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a scene of snowy landscapes, with an orange deer lying on the ice, eyes half-closed. Suddenly, the deer opens its eyes wide and notices a small grey-and-white penguin walking along the railroad tracks toward a plant bearing red fruit. A black train swiftly approaches the penguin from a distance on the same tracks.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 868, "extra_info": {"duration": 5.0, "events": ["An orange deer lies on the ice.", "The orange deer opens its eyes wide.", "A small grey-and-white penguin walks along the railroad tracks.", "The small grey-and-white penguin heads toward a plant.", "A plant bears red fruit.", "A black train swiftly approaches on the tracks.", "The black train moves toward the small grey-and-white penguin."], "n_events": 7, "n_shots": 3, "n_subjects": 2}, "vid": 868}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video explains how to land quickly in the PubG game through a screen recording. It commences with the player deploying a parachute. After it is opened, the player scans for nearby players. Next, the player swiftly glides towards a predetermined spot, intermittently checking the large map displayed on the screen. The intended landing point is the rooftop of a housing structure adorned with a Hyundai billboard. The player spots a rival just as he is about to touch down.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 869, "extra_info": {"duration": 8.0, "events": ["Player deploys parachute", "Parachute opens", "Player scans for nearby players", "Player glides towards a spot", "Player intermittently checks the map", "Landing point is a rooftop", "Rooftop adorned with a Hyundai billboard", "Player spots a rival", "Player is about to touch down"], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 869}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, player number two from the blue team skillfully manages an airborne ball, then proceeds towards the goal of the orange team. With the defense of the orange team closing in, player number two swiftly pulls back the ball, masterfully fakes a move, and drives the ball towards the goal once again. Following another fake, player number two outsmarts player number four of the orange team, causing him to topple. Subsequently, player number two passes the ball inward, and player number twenty-one of the blue team kicks the ball into the net, scoring a goal. The player who scored the goal then begins to run in excitement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 870, "extra_info": {"duration": 11.98, "events": ["Player number two skillfully manages airborne ball.", "Player number two advances towards orange team's goal.", "Player number two performs swift pullback of the ball.", "Player number two fakes a move against the defense.", "Player number two drives the ball towards the goal again.", "Player number two executes another fake.", "Player number two causes player number four to topple.", "Player number two passes the ball inward.", "Player number twenty-one kicks the ball into the net.", "Player number twenty-one starts running in excitement."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 870}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a long-haired blonde individual is in a household bathroom. They are garbed in a gray outfit. To the left of the frame is a partially ajar door. The room itself exudes a white ambiance. Behind the person is a shower, with a towel draped on the wall to their right. They are holding a white toothbrush and are in the act of brushing their teeth. The caption \"DAY 1 OF MY CANDIDA CLEANSE\" appears at the bottom. Subsequently, they comb their lengthy hair using a black comb with both hands. Following this, the scene transitions to a bedroom where we see the individual asleep in a bed draped with white sheets, positioned on the right side. Center screen, a hand gestures a half-heart symbol. The scene then changes again, and the individual is seen standing in a kitchen, holding a yellow lemon squeezer and extracting juice from a lemon into a glass.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 871, "extra_info": {"duration": 6.01, "events": ["The individual brushes teeth with a toothbrush", "The individual combs hair with a black comb", "The individual sleeps in bed", "A hand gestures a half-heart symbol", "The individual squeezes lemon juice into a glass"], "n_events": 5, "n_shots": 5, "n_subjects": 1}, "vid": 871}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, a blue vehicle is shot down while hovering over a desert. After it's hitting, the blue vehicle catches fire and emits smoke while it continues to traverse the desert.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 872, "extra_info": {"duration": 9.0, "events": ["A blue vehicle hovers over a desert.", "The blue vehicle is shot down.", "The blue vehicle catches fire.", "The blue vehicle emits smoke.", "The blue vehicle continues to traverse the desert."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 872}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the featured video clip, two teams are engaged in a soccer match. Player number 23, donned in a white shirt and green shorts, stands out of bounds, attempting to pass the ball to a teammate clad in white attire, green pants, black socks, and green shoes. Concurrently, an opposing player wearing a red shirt and shorts makes an effort to steal the ball from player 23's teammate.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 873, "extra_info": {"duration": 5.99, "events": ["Player 23 attempts to pass the ball", "Opposing player tries to steal the ball", "Teammate waits for the pass"], "n_events": 3, "n_shots": 2, "n_subjects": 2}, "vid": 873}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two individuals with long hair are strolling down the street. The one on the left has blond, curly hair and is wearing a beige coat, while the individual on the right, engrossed in a smartphone held in their left hand, is dressed in a gray suit, complete with gray clothing, pants, and a purple necktie. Despite attempts at conversation by their companion, the individual in the suit remains engrossed in the phone. Approaching them is a bald individual wearing a red t-shirt, blue jeans, and sunglasses. A collision occurs between the person in red and the individual in the suit, specifically with their left shoulders. The person in red raises their hands in apology, to which the suit-clad individual responds with anger, turning and shouting at the apologetic person in red.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 874, "extra_info": {"duration": 4.96, "events": ["Two individuals stroll down the street.", "Individual with blond hair wears beige coat.", "Individual in gray suit uses smartphone.", "Blond individual attempts conversation.", "Individual in gray suit ignores attempts.", "Bald individual approaches wearing red t-shirt.", "Bald individual walks wearing blue jeans.", "Bald individual sports sunglasses.", "Collision occurs between individuals in suit and in red.", "Individual in red raises hands in apology."], "n_events": 10, "n_shots": 3, "n_subjects": 3}, "vid": 874}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the darkness of the night, a white car proceeds along the road. Unexpectedly, a gray car emerges from the fork in the road ahead, darting from right to left. The white car has virtually no time to brake, and the distance between the two vehicles narrows dangerously. On the brink of collision, the video ends.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 875, "extra_info": {"duration": 5.0, "events": ["A white car proceeds along the road.", "A gray car emerges from the fork.", "The gray car darts from right to left.", "The white car brakes.", "The distance between the two cars narrows.", "The video ends before collision."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 875}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A car is observed moving forward on a road with a red car to its left. Both cars proceed along the road, with the red car moving at a faster pace. Suddenly, the red car swerves to the right, coming to a halt directly in front of the first car. The driver of the first car fails to brake in time, subsequently colliding with the red car. This impact results in a shattered window and a dented front door on the red car.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 876, "extra_info": {"duration": 6.0, "events": ["A car moves forward on a road.", "A red car proceeds alongside to the left.", "The red car moves at a faster pace.", "The red car swerves to the right.", "The red car comes to a halt in front.", "The first car fails to brake in time.", "The first car collides with the red car.", "A window shatters on the red car.", "A front door dents on the red car."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 876}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, a black car is smoothly driving on the left, nearing a cross-section with a fresh green traffic light. Suddenly, a large white truck emerges from the right intersection. The black car attempts to swerve to the left to avoid a collision, but fails as the white truck collides with it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 877, "extra_info": {"duration": 6.0, "events": ["A black car drives on the left.", "The black car nears a cross-section.", "A traffic light turns green.", "A large white truck emerges from the right.", "The black car swerves to the left.", "The white truck collides with the black car."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 877}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a person playing the game Pubg. After a brief moment of running, the gamer momentarily halts to open the M416 scope, targeting an adversary crouched behind a house, wearing a white helmet. There is a blue house situated on the player's right, and a land filled with trees on the left side. The player initiates an attack on the opponent, which results in a green smoke as the bullets make contact. Despite landing several shots accurately, the opponent is only knocked out, not completely eliminated. The player then proceeds to run towards the wounded opponent, across a white tarp.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 878, "extra_info": {"duration": 5.0, "events": ["Person plays Pubg", "Gamer runs in-game", "Gamer opens M416 scope", "Gamer targets adversary behind house", "Adversary crouches wearing white helmet", "Gamer attacks opponent", "Bullets create green smoke", "Opponent gets knocked out", "Gamer runs towards wounded opponent", "Gamer crosses white tarp"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 878}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the game, a character is seen pointing a red gun out of a window. Suddenly, the character is shot and jumps down to the grass from the second floor. The surroundings depict broken walls around the lawn. The character moves towards the wall and spots an enemy shooting from the second floor of the building opposite. The character returns fire towards the enemy in the opposite building.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 879, "extra_info": {"duration": 8.01, "events": ["A character points a red gun out of a window.", "The character is shot.", "The character jumps down to the grass from the second floor.", "The character moves towards the wall.", "The character spots an enemy shooting from the opposite building.", "The character returns fire towards the enemy."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 879}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays two people, clad in red and white garments, holding hands on a seaside surrounded by other individuals. The sky is heavily clouded with a torrential downpour. One person, donned in white clothes and yellow pants, is seen holding a black umbrella over the pair. Another person, notable for standing near a table on the far right, holds an open umbrella. Amid the palm trees, a photographer in white captures the moment. Suddenly, a bolt of lightning strikes, causing everyone to react in fear and direct their attention towards the lightning. Despite the frightful situation, the individual wearing a yellow dress and another in a wedding dress continue to hold hands. Following the alarming event, the photographer departs from the scene.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 880, "extra_info": {"duration": 6.0, "events": ["Two people hold hands on a seaside.", "Individual in white clothes holds a black umbrella.", "Person near a table holds an open umbrella.", "Photographer captures the moment among palm trees.", "Lightning strikes, causing a reaction from everyone.", "Individual in yellow dress and another continue to hold hands.", "Photographer departs from the scene."], "n_events": 7, "n_shots": 1, "n_subjects": 5}, "vid": 880}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays an individual wearing a long-sleeved swimsuit in hues of orange, yellow, and gray. This person, characterized by long hair, is spotted placing their hands on both sides of a blue surfboard, slowly lifting their body. Gradually, they arch their back, lean forward, and stand upright, their feet perfectly aligned with the surfboard's centerline. Subsequently, they stand sideways on the moving board, extending their arms and lowering their center of gravity to maintain optimal balance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 881, "extra_info": {"duration": 5.0, "events": ["Individual places hands on surfboard sides.", "Individual lifts body above surfboard.", "Individual arches back on surfboard.", "Individual leans forward on surfboard.", "Individual stands upright on surfboard.", "Individual aligns feet with surfboard's centerline.", "Individual stands sideways on moving surfboard.", "Individual extends arms for balance.", "Individual lowers center of gravity on surfboard."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 881}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows three individuals next to a caravan. A child dressed in a purple T-shirt is seen standing on tiptoe, struggling to open the black door of a caravan that is equipped with a white, foldable partition. After a while, the child finally manages to get it open. An adult wearing a blue hat and a red jacket then enters the frame taking a selfie. Several caravans can be seen in the background. As the adult pans his camera around, another adult wearing a pink T-shirt pops into the scene behind him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 882, "extra_info": {"duration": 9.98, "events": ["A child struggles to open a caravan door.", "The child opens the caravan door.", "An adult enters the frame taking a selfie.", "The adult pans his camera around.", "Another adult pops into the scene."], "n_events": 5, "n_shots": 2, "n_subjects": 3}, "vid": 882}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Player number twenty-one from the team in yellow attire takes a corner kick. Numerous players from both teams, blue and yellow, are congregated in the penalty area. Yellow team's player number two fails to connect with the ball and it is subsequently met by player number five, who's behind and possesses a taller stature. He heads the ball and it sails over the goalie of the blue team, finding its way into the net and resulting in a score for the yellow team.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 883, "extra_info": {"duration": 5.0, "events": ["Player number twenty-one takes corner kick.", "Players congregate in penalty area.", "Player number two misses the ball.", "Player number five heads the ball.", "Ball sails over the goalie.", "Ball finds its way into the net.", "Yellow team scores."], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 883}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a circular ash-filled hole situated within a brush in a forest. An individual pours water into this ash-filled hole to deter any potential fires. Following that, a person clad in black attire, a hat, and a knife nestled in their pocket, proceeds to gather the surrounding dirt from the ash-filled hole using a stick. The individual then collects the combined ashes and soil and discards them.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 884, "extra_info": {"duration": 6.0, "events": ["An individual pours water into the ash-filled hole.", "The individual in black gathers dirt with a stick.", "The individual collects mixed ashes and soil.", "The individual discards the ashes and soil."], "n_events": 4, "n_shots": 4, "n_subjects": 1}, "vid": 884}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts two stainless steel pots on a heated black stove. One pot is covered and boiling, while the other is filled with boiling water containing four ears of corn. A person uses tongs to turn the corn, then sets the tongs aside, picks up the lid, and secures it on the pot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 885, "extra_info": {"duration": 6.0, "events": ["A stove heats two stainless steel pots.", "One pot covered and boiling.", "Boiling water contains four ears of corn in the other pot.", "A person turns the corn with tongs.", "The person sets the tongs aside.", "The person picks up the lid.", "The person secures the lid on the pot with corn."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 885}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a person, clothed in a brown suit coat and a red tie, sitting on a train seat and appearing drowsy. They raise their right hand and rub their right eye. Suddenly, they observe a child, clad in white and sporting short hair, walking past the window outside the train. Seeing this, the person has the urge to disembark the train to locate this child.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 886, "extra_info": {"duration": 9.97, "events": ["Person sits on train seat.", "Person appears drowsy.", "Person raises right hand.", "Person rubs right eye.", "Child walks past train window.", "Person observes the child.", "Person feels urge to disembark.", "Person attempts to locate child."], "n_events": 8, "n_shots": 3, "n_subjects": 4}, "vid": 886}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A cook, dressed in a black t-shirt, chef's hat, and gray apron, drains noodles into a plate containing a liquid dish on a kitchen counter. The scene then transitions to an array of colourful foods and four ramen bowls. The ensuing shot reveals side dishes including meat, black fungus, scallions, and sesame seeds on the bowls. The cook then places seaweeds, marked with a logo, into the ramen bowls. A close-up shot of a ramen bowl follows.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 887, "extra_info": {"duration": 11.0, "events": ["Cook drains noodles into a plate", "Scene transitions to colourful foods and ramen bowls", "Side dishes added to the bowls", "Cook places logo-marked seaweeds into ramen bowls", "Close-up of ramen bowl occurs"], "n_events": 5, "n_shots": 3, "n_subjects": 1}, "vid": 887}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a competitive game, two players engage in a shootout. One of the players alters their position to lie flat on the ground while shooting. Ultimately, the player who chose to lie down on the ground is the victorious one.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 888, "extra_info": {"duration": 6.0, "events": ["Two players engage in a shootout.", "One player alters their position.", "Player lies flat on the ground.", "Player shoots from the ground.", "The lying player wins the shootout."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 888}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a gameplay sequence, an airplane is soaring through the sky. The player pulls out a map to pinpoint their location.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 889, "extra_info": {"duration": 10.01, "events": ["An airplane soars through the sky.", "The player pulls out a map.", "The player pinpoints their location on the map."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 889}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An individual dressed in orange is engaged in a shooting game. He triumphantly celebrates after making a kill, expressing his excitement with a loud exclamation of 'wow', removing his headphones, and cheerfully jumping onto the bed.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 890, "extra_info": {"duration": 5.0, "events": ["Individual in orange engages in shooting game.", "Individual makes a kill in the game.", "Individual exclaims 'wow' loudly.", "Individual removes headphones.", "Individual jumps onto the bed."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 890}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "There are two people in the video, one in the blue with short hair is playing the game and the other with the long hair in the grey with blue icon is explaining the game. In the game, the game character with gray hair and a cartoon-patterned backpack on their back squatted down and moved to the left. Suddenly, a blue wall appeared, blocking a thrown grenade just in time. The commentator with long hair was shocked to see this scene and almost thought the companion was about to be accidentally hit by a grenade and was frightened to death. Then, the in-game character stood up and aimed their gun at the opponent to continue shooting.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 891, "extra_info": {"duration": 5.0, "events": ["Person with short hair plays game.", "Person with long hair explains game.", "Game character squats down.", "Game character moves left.", "Blue wall appears.", "Wall blocks thrown grenade.", "Commentator expresses shock.", "Game character stands up.", "Game character aims gun.", "Game character continues shooting."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 891}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip,the person in blue vest is running so fast from the left. Then a police car pulles up and some policemen in green vests get out of the car. These policemen start to run towards the direction where the person in blue runs towards. Obviously, the person in blue seems to have commited a crime and these policemen is catching the person. Afraid of being caught by the police, the person abandoned the car and ran.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 892, "extra_info": {"duration": 11.0, "events": ["Person in blue vest runs from left.", "Police car pulls up.", "Policemen in green vests exit police car.", "Policemen start running towards person in blue vest's direction.", "Person in blue vest seems to have committed a crime.", "Policemen are chasing person in blue vest.", "Person in blue vest is afraid.", "Person in blue vest abandons a car.", "Person in blue vest runs away."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 892}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip showcases a person driving a motorcycle at a high speed. The rapidly increasing speed is evident from the dash board readings. The motorcycle then collides with a white car and flips over. In the end, only some lights can be seen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 893, "extra_info": {"duration": 6.97, "events": ["Person drives motorcycle at high speed.", "Speed increases on dashboard readings.", "Motorcycle collides with white car.", "Motorcycle flips over.", "Lights become visible at the end."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 893}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video begins with cars moving in a customary and orderly fashion. Unexpectedly, a large white truck with red tires speeds up from the left rear, colliding with the first white car to its side. It then goes on to hit the second car which is blue and white. This causes the blue and white car to veer right and forward, initially colliding with a transport vehicle carrying a green car ahead.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 894, "extra_info": {"duration": 4.0, "events": ["Cars move in orderly fashion.", "Large white truck speeds up.", "Truck collides with first white car.", "Truck hits second blue and white car.", "Blue and white car veers right.", "Blue and white car moves forward.", "Blue and white car collides with transport vehicle.", "Transport vehicle carries green car."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 894}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On the right side of the video, an individual carrying luggage can be seen standing in front of a store while using a mobile phone. Suddenly, a white car halts on the right side of the road, obstructing the path of a white FedEx truck behind it. The FedEx logo is evident on the side of the truck in vivid shades of purple and green. We can infer that the FedEx truck may need to overtake the white car to continue on its path.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 895, "extra_info": {"duration": 6.01, "events": ["Individual stands in front of a store.", "Individual uses mobile phone.", "White car halts on the right side of the road.", "White car obstructs the path of the FedEx truck.", "FedEx truck is behind the white car.", "FedEx truck may need to overtake the white car."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 895}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a badminton match between two pairs - one from the white team and the other from the red team. The score is currently at 14:16, with the white team trailing. The red team is dressed in red t-shirts and black shorts, while the white team soldiers on in white t-shirts and black shorts. In the ongoing rally, the red team serves first. The white then returns the shuttlecock only for the red team to reply shortly after. A player from the white team, wearing a red headband, makes a heroic dive to return the shuttlecock but fails as it had already hit the ground. Consequently, the white team loses the point, pushing the score to 14:17, now in favor of the red team.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 896, "extra_info": {"duration": 5.0, "events": ["The score reaches 14:16.", "The red team serves.", "The white team returns the shuttlecock.", "The red team replies with a hit.", "A player from the white team dives.", "The shuttlecock hits the ground.", "The red team scores a point.", "The score updates to 14:17."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 896}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip shows heavy traffic. A black car in the left lane is initially moving at a high speed, but suddenly decelerates. Subsequently, a gray car trailing the black car collides with it, causing the black car to flip over and hit the guardrail. It's unclear from the video if the gray car had accelerated or not.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 897, "extra_info": {"duration": 9.01, "events": ["Heavy traffic flows.", "Black car moves at high speed.", "Black car suddenly decelerates.", "Gray car collides with black car.", "Black car flips over.", "Black car hits the guardrail."], "n_events": 6, "n_shots": 1, "n_subjects": 2}, "vid": 897}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip shows a racing game with two competing black cars that are angled side by side. Neither driver is willing to concede to the other and their persistent side-by-side driving makes them susceptible to flipping over, thus potentially leading to an imminent accident.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 898, "extra_info": {"duration": 5.0, "events": ["Two black cars race side by side.", "Drivers refuse to concede to each other.", "Both cars maintain persistent side-by-side position.", "Cars risk flipping over due to their angle.", "The likelihood of an accident increases."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 898}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a game character named iUseNoMouse, represented by green health points, attacking a tower. Suddenly, a character with red health points charges forward, striking iUseNoMouse. Just before being attacked, iUseNoMouse collects a life capsule under the turret and escapes towards his birthpoint.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 899, "extra_info": {"duration": 5.0, "events": ["iUseNoMouse attacks a tower", "Character with red health points charges forward", "Character with red health points strikes iUseNoMouse", "iUseNoMouse collects a life capsule", "iUseNoMouse escapes towards his birthpoint"], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 899}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person with braided hair, wearing a light pink t-shirt, gray trousers, and a black bag with yellow stripes is skateboarding amidst a crowd. This individual is laughing with a classmate who has long black hair, wears a necklace, and is dressed in a black t-shirt. Four people are walking together in a location resembling a school garden. The first person on the left has blond hair, wears glasses, a black t-shirt, and carries a green backpack. The second individual is the person with the braided hair who is also seen laughing. The third individual has short black hair and is wearing a gray hoodie with orange text on it. They also have keys hanging around their neck as a form of necklace and is holding a mobile phone. The last person has long black hair, wears a black t-shirt along with a gray jacket, and has a cell phone in their back pocket. This person is seen smiling. All of them are working on the same whiteboard within a classroom, indicating they are classmates attending the same class.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 900, "extra_info": {"duration": 6.97, "events": ["Person with braided hair skateboards amidst a crowd.", "Individual with braided hair laughs with classmate with long black hair.", "Four people walk together in a school garden.", "Person on the left carries a green backpack.", "Individual with short black hair holds a mobile phone.", "Person with long black hair smiles.", "Classmates work on the same whiteboard in a classroom."], "n_events": 7, "n_shots": 7, "n_subjects": 4}, "vid": 900}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a cooking process where garlic is sautéed, together with butter in the pan. The cook consistently stirs this mix with a spatula causing the butter to gradually melt. As the liquid volume increases, the mixture begins to bubble and the ingredients turn mushy. The pan's contents continually bubble due to being boiled. Finally, shrimps and seasonings are added once the garlic and butter have fully melted.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 901, "extra_info": {"duration": 10.0, "events": ["Cook sautés garlic in the pan.", "Butter is added to the pan.", "Cook stirs the mixture with a spatula.", "Butter gradually melts in the pan.", "Liquid volume increases in the pan.", "The mixture begins to bubble.", "Ingredients turn mushy.", "Contents boil in the pan.", "Shrimps are added to the pan.", "Seasonings are introduced to the mixture."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 901}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a series of vehicles travelling along an asphalt road, lined on either side by forest. Two cars, one gray and one black, pass the camera from inside a third vehicle which is following a yellow truck along the same route. Abruptly, the black car switches to an incorrect lane, causing the yellow truck to swerve to the right in an attempt to avoid a collision. Despite the truck's efforts, a collision occurs between the black car and the truck. The impact sends fragments of both vehicles flying, and the black car spins out of control. The vehicle with the camera applies the brakes and comes to a stop.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 902, "extra_info": {"duration": 5.0, "events": ["Two cars pass the camera.", "Gray car and black car travel along the road.", "Third vehicle follows a yellow truck.", "Black car switches to an incorrect lane.", "Yellow truck swerves to the right.", "Collision occurs between the black car and the truck.", "Fragments fly from both vehicles.", "Black car spins out of control.", "Vehicle with camera applies the brakes.", "Vehicle with camera comes to a stop."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 902}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "On a verdant playing field, two American football teams, clad in diverging uniforms, advance towards each other. One team is adorned in green and white, while the other sports black and red attire. The player sporting the number two on the red team launches the ball towards the upper right corner of the field. Simultaneously, at the center of the field, players from the opposing teams block each other's paths and engage in tussles. Two players from the white team attempt to intercept the airborne ball but fail to secure it. Subsequently, a player with the number eight on the red team successfully catches the ball mid-air, earning a point for his side. Consequently, the red team's score increases from 3 to 9.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 903, "extra_info": {"duration": 6.0, "events": ["Two football teams approach each other on the field.", "Player number two launches the ball to the field's corner.", "Players engage in blocks and tussles at the center.", "Two white team players attempt to intercept the ball.", "Red team's player number eight catches the ball mid-air.", "Red team's score increases from 3 to 9."], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 903}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The orange chameleon is quietly staying on the tree. The chameleon then changes color from dark red to pink as it crawls forward.The background of the image shows a water drop hanging on a tree, not on the chameleon.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 904, "extra_info": {"duration": 7.0, "events": ["The orange chameleon stays on the tree.", "The chameleon changes color from dark red to pink.", "The chameleon crawls forward.", "A water drop hangs on a tree."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 904}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features a glass filled with ice. Over time, the ice slowly begins to sink in the water as it melts.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 905, "extra_info": {"duration": 12.0, "events": ["Ice is present in the glass.", "Ice slowly starts to melt.", "Melted ice causes water level to rise.", "Ice cubes begin sinking.", "Final ice cube disappears into water."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 905}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts metal strips covered in rust. A man initially sprays a particular solution onto the metal strip, followed by using a high-pressure water gun for cleaning. The rust is then removed by the water, resulting in a color change to gray.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 906, "extra_info": {"duration": 10.01, "events": ["A man sprays a solution onto the rust-covered metal strip.", "A man uses a high-pressure water gun on the metal strip.", "The water removes the rust from the metal strip.", "The metal strip changes color to gray."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 906}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video presents a blue-gray voltage tester. The tester is inserted into a socket located on a white wall, upon which it instantly flashes a red light and produces a sound.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 907, "extra_info": {"duration": 6.97, "events": ["The voltage tester is presented.", "The voltage tester is inserted into a socket.", "The voltage tester flashes a red light.", "The voltage tester produces a sound."], "n_events": 4, "n_shots": 1, "n_subjects": 1}, "vid": 907}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays an individual clad in a white coat who has a black wristwatch on their left wrist. In the foreground, test tubes connected to brown pipes are visible. A bottle labelled \"lime water\" can be seen on the left side of the video. Carbon dioxide is introduced into the test tube through a flexible tube. As a result, the clear liquid in the test tube becomes cloudy and bubbles emerge from the surface.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 908, "extra_info": {"duration": 11.98, "events": ["Individual enters wearing a white coat.", "Person puts on a black wristwatch.", "Test tubes connect to brown pipes.", "Bottle labeled 'lime water' appears.", "Carbon dioxide flows through a flexible tube.", "Clear liquid in test tube turns cloudy.", "Bubbles emerge from the liquid's surface."], "n_events": 7, "n_shots": 2, "n_subjects": 1}, "vid": 908}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video clip, a table is set with three clear glasses of water, a model helicopter, and a Barbie doll. A person wearing an orange glove methodically drops white objects into each glass. Upon immersion, the water appears to boil and emit a white gas, much like dry ice.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 909, "extra_info": {"duration": 9.0, "events": ["A table is set with glasses, a helicopter, and a Barbie doll.", "A person wears an orange glove.", "The person drops white objects into the first glass of water.", "Water in the first glass appears to boil.", "White gas emits from the first glass.", "The person drops white objects into the second glass of water.", "Water in the second glass appears to boil.", "White gas emits from the second glass.", "The person drops white objects into the third glass of water.", "Water in the third glass appears to boil."], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 909}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "An individual of Caucasian descent, attired in black clothing, is seen gripping a knife in one hand and a white cloth in the other. They begin to wrap the cloth around their hand and then elevate the knife, executing a slashing move towards an African-American individual, also adorned in black clothing. The latter individual raises their left arm in defence, bending it as they do so, with a black cloth swathed around their left hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 910, "extra_info": {"duration": 9.01, "events": ["Individual grips a knife in one hand.", "Individual wraps a white cloth around their hand.", "Individual elevates the knife.", "Individual executes a slashing move towards another individual.", "African-American individual raises their left arm in defence.", "African-American individual bends their left arm.", "African-American individual has a black cloth around their left hand."], "n_events": 7, "n_shots": 1, "n_subjects": 2}, "vid": 910}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, we are navigated through a dimly lit maze with gray walls that are adorned with bright blue lights. The only source of illumination comes from the direction our character is facing. This character keeps moving forward, turning left twice, then making a right turn, finally appearing at the exit where a huge glowing sphere stands amidst towering trees.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 911, "extra_info": {"duration": 8.0, "events": ["Video navigates through a dimly lit maze.", "Character moves forward within the maze.", "Character turns left.", "Character turns left again.", "Character makes a right turn.", "Character reaches the exit.", "A glowing sphere stands at the exit.", "Towering trees surround the sphere."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 911}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts an individual extracting a wire panel. Upon removal of this wire board, the red and white wires appear unprotected, devoid of any electrical tape. The red wire, also known as the live or active wire, and the white wire, known as the neutral wire, are exposed. This leaves potential for electrical mishaps. If these exposed wires are inadvertently touched, the person may suffer from an electric shock and life-threatening consequences may ensue, requiring immediate medical attention and potentially an ambulance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 912, "extra_info": {"duration": 5.0, "events": ["Individual extracts a wire panel.", "Red and white wires are exposed.", "Person may inadvertently touch the wires.", "Person suffers from an electric shock.", "Life-threatening consequences ensue.", "Immediate medical attention is required.", "An ambulance may be needed."], "n_events": 7, "n_shots": 1, "n_subjects": 1}, "vid": 912}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip features a football game between a team in white tops and dark blue shorts and another team dressed in black jerseys. The player in white, wearing No. 8, attempts a shot at the goal which is effectively blocked by the goalkeeper clothed in yellow. This negates a score for the white team. The goalkeeper, a member of the black team donned in a yellow jersey, can be seen smiling broadly in the video.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 913, "extra_info": {"duration": 5.0, "events": ["Player in white wearing No. 8 attempts a shot at goal", "Goalkeeper clothed in yellow effectively blocks the shot", "Goalkeeper from black team prevents a score for the white team", "Goalkeeper in yellow jersey smiles broadly"], "n_events": 4, "n_shots": 2, "n_subjects": 3}, "vid": 913}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Two players, one in red jerseys representing Hungary and the other in yellow jerseys representing Brazil, are engaged in a game of teqball. The player in the red jersey and white shoes lifts the ball into the air, while his teammate, wearing a red jersey and blue shoes, deftly controls the ball with his foot and redirects it into the opponent's area. The ball bounces twice on the yellow team's side, thereby earning a point for the red team. In celebration, the player in the red jersey and blue shoes pumps his fists in triumph and embraces his teammate. He then quickly shakes his head and moves his fingers in excitement.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 914, "extra_info": {"duration": 6.97, "events": ["Player in red jersey lifts the ball into the air", "Teammate in red jersey controls the ball with foot", "Teammate redirects the ball into the opponent's area", "Ball bounces twice on the yellow team's side", "Red team earns a point", "Player in red jersey and blue shoes pumps fists in triumph", "Player in red jersey and white shoes embraces teammate", "Player in red jersey and blue shoes shakes head", "Player in red jersey and blue shoes moves fingers in excitement"], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 914}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video illustrates an 8-person race, with the athlete wearing the number 184 in white leading and eventually winning. Overwhelmed by excitement, this athlete raises their hand, displays the national flag to the crowd, and kisses it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 915, "extra_info": {"duration": 11.01, "events": ["Athlete number 184 leads the race.", "Athlete number 184 wins the race.", "Athlete number 184 raises their hand in excitement.", "Athlete number 184 displays the national flag to the crowd.", "Athlete number 184 kisses the national flag."], "n_events": 5, "n_shots": 2, "n_subjects": 8}, "vid": 915}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a running competition where players vie for the top spot. All lanes except for the sixth one are filled with competitors. As the race takes a turn, the participant in the fifth lane breaks away from the pack. This racer, wearing number 492 and identified by long black hair, a pink T-shirt, black shorts, and shoes to match, surges ahead of the other contestants and finishes first. Overcome with joy, they raise their hand in jubilation upon crossing the finish line.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 916, "extra_info": {"duration": 7.97, "events": ["Competition begins with a running race.", "Players compete for the top spot.", "Sixth lane remains empty.", "Race takes a turn.", "Participant in fifth lane breaks away.", "Racer number 492 surges ahead.", "Racer number 492 finishes first.", "Racer raises hand in jubilation."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 916}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a person wearing a gray t-shirt with a spider emblem on it and headphones is watching an anime video. In the video, a character kicks a grey-clothed character in the face and the gray-clothed character is left in a difficult situation. The camera then zooms out and a person with red hair and red clothes is seen surrounded by several gray-clothed people lying on the ground. Then, in another scene, this person is surrounded by many people wearing grey clothes and holding shields in their hands. Meanwhile, the person in the red outfit is leaning against a wall and smoking a cigarette. Then the person in the black t-shirt watching the video laughs for the character in Anime is smoking and there is a gathering army around him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 917, "extra_info": {"duration": 5.97, "events": ["A person watches an anime video.", "A character kicks a grey-clothed character in the face.", "The grey-clothed character remains in a difficult situation.", "A person with red hair stands surrounded by fallen people.", "Many people hold shields around the red-clothed person.", "The red-clothed person leans against a wall and smokes.", "The person in a black t-shirt laughs."], "n_events": 7, "n_shots": 1, "n_subjects": 4}, "vid": 917}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Players in red t-shirts, black shorts, black socks and black shoes are jumping and moving forward.They pass a yellow dummy while jumping. In addition to the same players, another player in a black t-shirt, black tracksuit and black shoes formed a circle. They raise their hands up and spread their legs to the sides.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 918, "extra_info": {"duration": 5.0, "events": ["Players jump and move forward.", "Players pass a yellow dummy while jumping.", "Another player joins to form a circle.", "Players raise their hands up.", "Players spread their legs to the sides."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 918}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person with black hair, donned in a red T-shirt, black shorts, and black shoes is running towards an orange cone. This individual starts at the beginning of a track and halts at a pontoon. Another participant, wearing a T-shirt patterned with red and pink colors bearing a world logo, an Olympic's emblem, and a 'UCS' insignia on its back, appears from this person's right. The person on the left is merely stretching their legs while the individual wearing the green shoes is engaging in jumps to warm up their body in preparation for the forthcoming exercises.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 919, "extra_info": {"duration": 10.01, "events": ["Person runs towards orange cone.", "Individual halts at pontoon.", "Participant appears from right.", "Person stretches legs.", "Individual wearing green shoes jumps."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 919}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the hurdle race, the princess takes the lead, followed by the turtle in second and Super Mario in the last position as they clear the first hurdle. Super Mario then accelerates, overtaking the turtle to claim second place. They all proceed to pass the second hurdle. As Super Mario continues to increase his speed, he begins to close in on the princess. All three continue to race and successfully pass the third hurdle. Just before the fourth hurdle, Super Mario takes over the lead from the princess.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 920, "extra_info": {"duration": 8.0, "events": ["Princess takes the lead.", "Turtle follows in second.", "Super Mario clears first hurdle.", "Super Mario overtakes turtle.", "All pass second hurdle.", "Super Mario closes on princess.", "All clear third hurdle.", "Super Mario leads before fourth hurdle."], "n_events": 8, "n_shots": 1, "n_subjects": 3}, "vid": 920}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a horse race. A racer, clad in yellow, is riding a horse originating from the eighth track, identified by the horse's yellow hooves. Despite the start of the race, the racer continues to maintain the last position.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 921, "extra_info": {"duration": 5.0, "events": ["A horse race starts.", "Racer in yellow rides a horse.", "Horse's hooves touch the track.", "Racer maintains last position."], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 921}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video begins with a visual of a white trash can filled with a bottle having a yellow cap, a black bag, and trash held by a person's hand. A text reading, \"Your bin is filled to the brim\" appears on the right. This text soon changes to \"But you are too lazy\". As the person's hand comes closer, the lid of the trash can opens wider, and the person throws their trash. The text on the right side now reads, \"What options do you have?\", which is eventually replaced by a text saying \"check out this\" against a white background. The latter part of the video shows a person pressing the trash can button which closes the trash lid, automatically securing the garbage bag inside. When the process is completed, the person retrieves the garbage bag and removes it from the trash can.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 922, "extra_info": {"duration": 10.98, "events": ["Trash can lid opens wider", "Person's hand throws trash into the can", "Text on video changes", "Person presses trash can button", "Trash can lid closes automatically", "Person secures the garbage bag inside", "Person retrieves the garbage bag", "Person removes the garbage bag from the trash can"], "n_events": 8, "n_shots": 3, "n_subjects": 1}, "vid": 922}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video takes place in a living room where an adult with short hair, dressed in black shorts and a red T-shirt, descends a set of brown stairs. The adult is carrying a small monkey, which is decked out in white clothes, on a couch. Also in the living room are two long-haired children engaged in play. The kid on the right, dressed in a white sleeveless T-shirt and red shorts, has their hair styled in a ponytail and is seated on a yellow and blue toy car. The kid on the left, donned in a white sleeveless T-shirt along with white shorts, approaches on foot. Both kids are barefoot. Spotting the adult carrying the monkey downstairs, the children draw near. The one with the toy car drives over, while the other walks. They both halt on reaching the adult. The child dressed in a white sleeveless T-shirt and pink shorts shows a desire to extend a hug to the monkey.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 923, "extra_info": {"duration": 5.0, "events": ["Adult descends brown stairs.", "Adult carries monkey.", "Child on toy car drives over.", "Child on foot approaches.", "Children halt near adult.", "Child in white and pink expresses desire to hug monkey."], "n_events": 6, "n_shots": 1, "n_subjects": 3}, "vid": 923}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Three individuals are exercising side by side on a road. Among them, there is an adult dressed in a short-sleeve gray T-shirt and black shorts. Two children, with their hair styled in ponytails, are wearing white short-sleeved T-shirts and shorts. A tricycle painted in yellow and blue stands between the two kids. The child on the right sports pink slippers, while the one on the left has on white slippers. The adult, attired in black shorts, a gray short-sleeved T-shirt and brown slippers, is situated to the right of the children. They all are engaged in leg jumping exercises on the roadside. A monkey clothed in a white short-sleeved T-shirt and yellow shorts jumps from the steering wheel of a stroller onto the middle child who is wearing glasses, thus interrupting the child's workout. The adult, with a smile, picks up the monkey with both hands and repositions it on the wheel of the stroller.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 924, "extra_info": {"duration": 9.0, "events": ["Three individuals exercise on a road", "Adult dressed in gray T-shirt stands right of children", "Two children wear white T-shirts and shorts", "A tricycle stands between two kids", "Child on the right wears pink slippers", "Child on the left wears white slippers", "Adult wears brown slippers", "They all engage in leg jumping exercises", "Monkey jumps from stroller onto middle child", "Adult picks up monkey and repositions it on stroller"], "n_events": 10, "n_shots": 1, "n_subjects": 4}, "vid": 924}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features two children in blue outfits seated at a table, possibly siblings. The child in the blue skirt is pulling something out. There's also an individual in a blue shirt and another in black attire in the kitchen. The person in black, presumably the mother, is opening the refrigerator to find something to eat. From the photographs displayed on the wall, it's discernible that the four individuals are likely a family, with two adults presumably the parents, and the children their offspring.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 925, "extra_info": {"duration": 10.0, "events": ["Child in blue skirt pulls something out.", "Individual in blue shirt sits at table.", "Person in black attire opens refrigerator.", "Mother searches for something to eat."], "n_events": 4, "n_shots": 2, "n_subjects": 4}, "vid": 925}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a room setting, a lengthy-haired individual, donned in a gray T-shirt, stands in front of a camera. The individual, whose back faces the camera, restfully leans against a pool table, located on the right, with hands behind their back, holding a thin, rectangular, brown envelope or paper bag. The individual also sports long blonde hair. A second person, dressed in a black T-shirt, blue shorts, and black shoes, emerges from a staircase positioned upfront. The person gradually descends the stairs, strolling past the camera whilst pointing at it with their right hand and subsequently gesturing a greeting by shaking their left hand at the camera. The person in black situates on the left, while the person in gray positions on the right. They both display joyous smiles on their faces. The individual wearing gray finally brings forth the hidden brown object from behind their back.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 926, "extra_info": {"duration": 8.99, "events": ["Long-haired individual leans against a pool table.", "Individual holds a brown envelope behind their back.", "Second person emerges from the staircase.", "Second person descends the stairs.", "Second person points at the camera with their right hand.", "Second person shakes their left hand at the camera.", "Both individuals position themselves on opposite sides.", "Both individuals display smiles.", "Long-haired individual reveals the brown object."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 926}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a swimming pool, a kid outfitted with swimming goggles and donned in red shorts, struggles in the water, nearly drowning. The child's hands are continuously seen bobbing on the surface as he attempts to rise, yet sinks further in. Nearby, another individual, identifiable via a long braid and a swimming suit with goggles resting on their head, stands at the pool's edge. They reach out with their left hand to aid the struggling kid but are just out of reach. Swiftly, they turn to the nearby fence to retrieve tools to assist the child in peril.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 927, "extra_info": {"duration": 6.97, "events": ["Kid outfitted with swimming goggles struggles in water", "Kid's hands bob on the surface", "Kid attempts to rise in water", "Kid sinks further in the pool", "Individual stands at the pool's edge", "Individual reaches out with left hand", "Individual is out of reach of kid", "Individual turns to the fence", "Individual retrieves tools from the fence", "Tools are intended to assist the struggling kid"], "n_events": 10, "n_shots": 2, "n_subjects": 3}, "vid": 927}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A child donned in a yellow T-shirt with white dots walks up to another child who is attired in a white T-shirt, emblazoned with a unicorn, paired with a hijab that covers her braided hair. This child is also holding a phone. Present is another child, wearing a half-pink, half-white striped T-shirt and a backpack, the child points at the hijab-wearing, unicorn T-shirt clad child, laughing. As this child continues to laugh, the child in the yellow T-shirt starts dancing in front of the child wearing the hijab and the unicorn T-shirt, evoking laughter as well. This subsequently depresses and saddens the child wearing the hijab and the white T-shirt with a unicorn in the middle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 928, "extra_info": {"duration": 10.01, "events": ["Child in yellow T-shirt walks up to another child.", "Child in hijab holds a phone.", "Child in striped T-shirt points at child in hijab.", "Child in striped T-shirt laughs.", "Child in yellow T-shirt starts dancing.", "Child in hijab starts laughing.", "Child in hijab becomes sad."], "n_events": 7, "n_shots": 4, "n_subjects": 3}, "vid": 928}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures a scene in a classroom where a teacher with long brown hair, dressed in a white shirt, pants, and a purple jacket, is chatting and laughing with the students. A student dressed in a black uniform is also shown laughing while seated. The teacher then places a bag on the table upfront, next to a blackboard that has writings like \"Mr. Finn\" and \"Notes Quiz Friday - History\". In the next scene, a glasses-wearing, blonde student wearing a grey uniform is seen handling a toy rocket which suddenly launches, trailing smoke and hitting the wall near the blackboard. This action draws the attention of all the students towards the blonde student. Upon turning around, the teacher appears visibly shocked and frightened due to the unexpected rocket launch.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 929, "extra_info": {"duration": 6.97, "events": ["Teacher chats and laughs with students.", "Student laughs while seated.", "Teacher places a bag on the table.", "Toy rocket launches in classroom.", "Rocket trails smoke.", "Rocket hits the wall near the blackboard.", "Students turn attention to blonde student.", "Teacher turns around.", "Teacher appears shocked.", "Teacher appears frightened."], "n_events": 10, "n_shots": 7, "n_subjects": 4}, "vid": 929}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video clip features two individuals situated in a room. The first individual can be seen clad in a pink and gray jacket paired with blue jeans, while the other is donning a purple jacket and white pants. The younger of the two individuals appears to be drying their hair with a blue towel, whilst the older one appears to be preparing some food. The person in the purple jacket proceeds to pick up a yellow plate off the table and takes a seat. Based on visual cues, it's possible that they could either eat the meal from the orange plate in their hand, or choose to consume food from the plate on the table, which has a set of cutlery beside it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 930, "extra_info": {"duration": 11.97, "events": ["Individual dries hair with towel", "Older individual prepares food", "Person picks up yellow plate", "Individual takes a seat"], "n_events": 4, "n_shots": 3, "n_subjects": 2}, "vid": 930}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual with short hair, wearing blue jeans and a white T-shirt, is seen dribbling a ball. A second person with long hair, dressed in a white T-shirt, yellow vest, black pants, and a hat, tries to play defense. The short-haired person knocks the long-haired person to the ground with their left hand, jumps to take a shot, and scores. Triumphantly, this person raises their right hand as a sign of success. Clearly, scoring the basket brings this person great joy.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 931, "extra_info": {"duration": 5.97, "events": ["Individual with short hair dribbles a ball.", "Second person with long hair plays defense.", "Short-haired individual knocks long-haired person to the ground.", "Short-haired individual jumps to take a shot.", "Short-haired individual scores.", "Short-haired individual raises right hand in triumph."], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 931}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features two individuals seated at a table. On the right is a person with short hair, garbed in a black t-shirt and gray jeans, accessorized by green sunglasses. This person is seated in a chair with an empty chair next to them, reading a newspaper with their right hand and holding a yellow pen in their left. Suddenly, another individual with long hair enters from the left. Dressed in a white shirt, black skirt, pink pantyhose, and black shoes, this person places both hands on the table and requests to use the empty chair. However, the person on the right refrains from responding, instead resting their feet on the chair and nonchalantly twirling the pen in their left hand, clearly indicating refusal. The person on the left expresses disappointment by raising both hands and walking away.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 932, "extra_info": {"duration": 5.0, "events": ["Person reads a newspaper with right hand.", "Person holds a yellow pen with left hand.", "Individual with long hair enters from the left.", "Individual with long hair places both hands on the table.", "Individual with long hair requests to use the empty chair.", "Person rests feet on the chair.", "Person twirls the pen in left hand.", "Individual with long hair raises both hands.", "Individual with long hair walks away."], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 932}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "This video clip features a person wearing a blue shirt and black pants, and a baby monkey wearing colorful clothing. The person places a glass of milk on the table and takes a grape from the monkey's hand. He then proceeds to peel the grape while the monkey eats another grape. It is clear that the person is peeling the grape to feed it to the baby monkey.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 933, "extra_info": {"duration": 9.0, "events": ["A person places a glass of milk on the table", "The person takes a grape from the baby monkey's hand", "The person peels the grape", "The baby monkey eats another grape"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 933}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a person wearing a grey t-shirt, who turns on the tap near a sink surrounded by various cleaning products. The individual rolls up their sleeves and pours a skin cleanser, specifically Cetaphil, into their hand. Then, the person rubs the hands together and proceeds to wash the face. After washing, they turn off the tap and dry their hands with a white towel. Finally, the person dries their face with a green towel.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 934, "extra_info": {"duration": 9.0, "events": ["Person turns on the tap", "Person pours Cetaphil into hand", "Person rubs hands together", "Person washes face", "Person turns off the tap", "Person dries hands with white towel", "Person dries face with green towel"], "n_events": 7, "n_shots": 8, "n_subjects": 1}, "vid": 934}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a black car is driving on a two-way lane, passing by grassy lawns and small houses. Ultimately, it circles around in front of an open gray garage, adjusting its direction to parallel itself with the garage. The car is going to park inside the garage.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 935, "extra_info": {"duration": 6.97, "events": ["A black car drives on a two-way lane.", "Car passes by grassy lawns.", "Car passes by small houses.", "Car circles around in front.", "Car adjusts its direction to parallel.", "Car prepares to park.", "Car enters the open gray garage."], "n_events": 7, "n_shots": 5, "n_subjects": 1}, "vid": 935}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts two individuals in suits standing on opposite sides of a table, each extending their right hand for a handshake. Upon completing the handshake, the person wearing a black suit on the right gestures with his right hand again, palm facing up, inviting the individual in the gray suit with long hair on the left to sit down instead of standing.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 936, "extra_info": {"duration": 5.0, "events": ["Two individuals stand on opposite sides of a table.", "Each extends their right hand.", "They complete the handshake.", "Person in black suit gestures with right hand.", "Gesture invites individual in gray suit to sit down."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 936}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video starts with a group of people walking along the beach. The scene then cuts to a long-haired child donned in a black hoodie and matching hat, taking a selfie on a beach by the sea. The child is seen reaching out with the left hand, while the right hand is holding the selfie device. The video's view initially elevates, circles the child, and finally lowers offering a sweeping view of the sea, sunset, lush greenery and the beach.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 937, "extra_info": {"duration": 5.0, "events": ["People walk along the beach", "Child in black hoodie takes a selfie", "Child reaches out with left hand", "Child holds selfie device with right hand", "View elevates around the child", "View circles the child", "View lowers to show the sea", "Sunset becomes visible", "Lush greenery becomes visible", "Beach comes into sweeping view"], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 937}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual, clad in a black T-shirt with a yellow emblem, is situated in the bus's luggage compartment. Fearful of being spotted by the authorities, the individual reclines and seeks cover behind the stored luggage. The camera shifts focus to the trunk's opening where assorted luggage can be seen. Suddenly, the baggage handler dressed in white catches sight of the individual in black attempting to take a red suitcase. It's clear the individual in black risks getting caught by the bus driver.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 938, "extra_info": {"duration": 8.6, "events": ["Individual reclines in luggage compartment.", "Individual seeks cover behind luggage.", "Camera shows trunk's opening with luggage.", "Baggage handler catches sight of individual.", "Individual attempts to take red suitcase.", "Individual risks getting caught by bus driver."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 938}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person donned in a fluorescent yellow life vest and a blue helmet attempts to leap from one red ball to another, but fails, falling into the water instead. Concurrently on the shore, three individuals clad in yellow life vests are seen laughing heartily. One of them wears a fluorescent yellow life vest with pink stripes and sports grey pigtails, while the next is in a fluorescent yellow life vest with grey stripes, and the last one wears a fluorescent yellow life vest with blue stripes. Also, a person in a red helmet and a fluorescent green life jacket attempts the same jump between the red balls, similarly failing and falling into the water.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 939, "extra_info": {"duration": 5.97, "events": ["Person attempts to leap from one red ball to another.", "Person falls into the water.", "Three individuals laugh heartily on the shore.", "Individual with grey pigtails wears fluorescent life vest.", "Individual with grey stripes on life vest laughs.", "Individual with blue stripes on life vest chuckles.", "Person in red helmet attempts the jump.", "Person in fluorescent green life jacket falls into water."], "n_events": 8, "n_shots": 5, "n_subjects": 6}, "vid": 939}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, an individual wearing a black glove is seen handling some meat. They use a knife to cut the meat on a white chopping board. Once cut, the meat is placed in a transparent bowl on the board. Lastly, they add various spices into a separate transparent bowl.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 940, "extra_info": {"duration": 9.0, "events": ["Individual wears black glove.", "Individual handles meat.", "Knife cuts the meat.", "Meat is cut on chopping board.", "Cut meat is placed in bowl.", "Individual prepares spices.", "Spices are added to separate bowl."], "n_events": 7, "n_shots": 2, "n_subjects": 1}, "vid": 940}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an elderly individual is wearing brown pants, a shirt, and a vest. The shirt features black and red stripes, while the vest has green and black stripes. The person has white hair and is wearing a black hat. He place a skinned fish on the lower grill rack of an oven. Other fish can be seen on the upper grill rack of the oven. Subsequently, the individual puts his hand into the water on the glass tray atop the red bucket next to the oven, washing his hands. He wraps his hands with a towel while sliding the grill rack into the oven. Positioned atop the oven are a bottle and a white bowl. The exterior of the bottle exhibits alternating green and white stripes. The towel is adorned with white and blue stripes. Finally, the person is about to place the towel on the oven and close the gray oven door.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 941, "extra_info": {"duration": 9.98, "events": ["Elderly individual places a skinned fish on lower grill rack.", "Other fish are on upper grill rack.", "Elderly individual washes hands in water on glass tray.", "Elderly individual wraps hands with striped towel.", "Elderly individual slides grill rack into the oven.", "A bottle is positioned atop the oven.", "Elderly individual is about to place towel on the oven.", "Elderly individual closes gray oven door."], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 941}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows contestants competing on trike model motorcycles. While navigating through a bend, they adjust their leg positions tactfully to make the turn. The competitor wearing grey shoes, who is recording the video, overtakes the contestant labeled number one from the left. Surrounded by pastures on either side, they gear up to make a left turn. As they move, they dynamically lift their right leg during a left turn and their left leg during a right turn.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 942, "extra_info": {"duration": 10.01, "events": ["Contestants compete on trike model motorcycles.", "Riders adjust leg positions through a bend.", "Competitor overtakes from the left.", "Competitor wearing grey shoes records the video.", "Riders prepare for a left turn.", "Riders lift right leg during left turn.", "Riders lift left leg during right turn."], "n_events": 7, "n_shots": 1, "n_subjects": 3}, "vid": 942}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a shark preparing to attack a person on the left, who is outfitted in a black wetsuit and carries an oxygen tank on their back. As the shark opens its mouth to attack, an individual on the right thrusts a round hook into the shark's mouth. Immediately, the person on the left maneuvers swiftly, taking shelter beneath the shark's body.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 943, "extra_info": {"duration": 6.01, "events": ["Shark prepares to attack person on left", "Person on left carries oxygen tank", "Shark opens mouth", "Individual on right thrusts hook into shark's mouth", "Person on left maneuvers swiftly", "Person on left takes shelter beneath shark's body"], "n_events": 6, "n_shots": 2, "n_subjects": 3}, "vid": 943}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video captures a boxing match between two competitors. The boxer wearing white shorts labeled ''INDIA'' and red gloves attempts to land punches on his opponent, donned in black shorts and blue gloves. The blue-gloved boxer adopts a defensive stance, raising his arms to protect his face, thereby mitigating potential injury such as a beat to the head, a broken nose, or a nose bleed. A person wearing headphones is positioned behind the boxers, recording the match. When the red-gloved boxer is unable to strike his opponent's face, he switches tactics, aiming blows at the abdominal region before attempting another headshot. However, the blue-gloved boxer manages to deflect these attacks successfully, to the cheers of the spectators.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 944, "extra_info": {"duration": 12.0, "events": ["Boxer in white shorts attacks with red gloves.", "Boxer in black shorts raises arms to protect face.", "Boxer in white shorts aims punches at opponent's abdomen.", "Boxer in white shorts attempts headshot on opponent.", "Boxer in black shorts deflects the punches.", "Spectators cheer during the match.", "Person in headphones records the match."], "n_events": 7, "n_shots": 2, "n_subjects": 4}, "vid": 944}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a light brown snake with a dark brown pattern, coiled around a deer's neck and limbs in the grass. The snake proceeds to open its large mouth, beginning to swallow the deer from the muzzle, before moving onto the eye region, which remains open.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 945, "extra_info": {"duration": 8.01, "events": ["Snake coils around deer's neck and limbs.", "Snake opens its large mouth.", "Snake begins to swallow the deer from the muzzle.", "Snake proceeds to the deer's eye region.", "Deer's eye remains open."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 945}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The yellow dinosaur, dressed in a police uniform, is sitting inside a blue police car and is in pursuit of a brown-red truck driven by a blue dinosaur wearing red clothing and a red hat. Inside the brown-red truck is an open bag of money. The blue dinosaur in red clothing and a red hat, along with four other blue dinosaurs dressed in red, proceeds to bomb the bank. Then, they make their getaway in the truck. Subsequently, the police car chases them and collides with their vehicle, causing the money to spill out.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 946, "extra_info": {"duration": 6.0, "events": ["Yellow dinosaur drives police car in pursuit.", "Blue dinosaur operates brown-red truck.", "Brown-red truck contains open bag of money.", "Blue dinosaur bombs the bank.", "Four blue dinosaurs assist in bank bombing.", "Blue dinosaur and accomplices escape in truck.", "Police car chases brown-red truck.", "Police car collides with brown-red truck.", "Money spills out from brown-red truck."], "n_events": 9, "n_shots": 3, "n_subjects": 5}, "vid": 946}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a person with long hair dressed in a red and blue suit, white pants, and gray shoes. This individual is seen running as a swiftly charging black bull narrowly misses them. With their hands raised above their head, chest pressing forward, and head inclined backward, the person skillfully dodges the bull's assault. Simultaneously, two others, dressed similarly and waving either a red or yellow cloth, are seen running towards the bull from behind.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 947, "extra_info": {"duration": 5.0, "events": ["Person runs from charging bull.", "Bull narrowly misses person.", "Person dodges bull's assault.", "Person's hands raised above head.", "Person presses chest forward.", "Person inclines head backward.", "Two others run towards bull.", "First other waves red cloth.", "Second other waves yellow cloth."], "n_events": 9, "n_shots": 3, "n_subjects": 3}, "vid": 947}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a semi-nude individual, clad in black trousers, a black hat, and black shoes. This person takes a bold leap from the roof of a green multi-storey building, landing on the rooftop of another building which hosts an antenna. The jumper carries no safety gear throughout this stunt. Soon after, another person comes into view, adorned with white makeup and several faux scars made with the same. Two more people are seen fastening some sort of white safety gear on a person in a green T-shirt. Meanwhile, the individual with makeup raises their hands in the air.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 948, "extra_info": {"duration": 6.0, "events": ["Individual jumps from one rooftop to another.", "Person lands on a building hosting an antenna.", "Another person with white makeup comes into view.", "More people fasten safety gear on a person.", "Person with makeup raises hands in the air."], "n_events": 5, "n_shots": 4, "n_subjects": 4}, "vid": 948}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a blue barrel with a brown interior. Inside the barrel, an explosive fuse, which is yellow on the inside and orange on the outside, is shown. A small hole is present at the bottom of the barrel, and there's a transfer pipe through the hole. Inside the barrel, there is a metal piece hanging. The camera angle switches to show the exterior of the blue barrel placed among miscellaneous warehouse items. An imminent dust explosion is anticipated.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 949, "extra_info": {"duration": 8.0, "events": ["A blue barrel with a brown interior appears.", "An explosive fuse burns inside the barrel.", "The fuse color transitions from yellow to orange.", "A small hole at the barrel's bottom is revealed.", "A transfer pipe runs through the barrel's hole.", "A metal piece hangs inside the barrel.", "The blue barrel is situated in a warehouse.", "Objects surround the blue barrel in the warehouse."], "n_events": 8, "n_shots": 3, "n_subjects": 2}, "vid": 949}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person clad in a yellow shirt and black hat faces the camera and makes remarks. Up next, an individual wearing a black apron and a grey t-shirt, brandishes a transparent cup filled with pink smoothie in their left hand. The kitchen counter is laden with assorted foods and culinary utensils. In an unexpected turn, the person upends the cup as though intending to pour the smoothie over their head, but interestingly, the smoothie does not gush out for several ensuing seconds.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 950, "extra_info": {"duration": 6.01, "events": ["A person makes remarks facing the camera.", "An individual brandishes a transparent cup.", "The cup contains pink smoothie.", "The individual upends the cup.", "The smoothie does not gush out."], "n_events": 5, "n_shots": 2, "n_subjects": 2}, "vid": 950}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person dressed in a white Reebok t-shirt is captured riding a motorcycle in a selfie video. The individual is not wearing a helmet while on the motorcycle. The video then transitions to a shot of the road and the motorcycle. To the left side of the accelerating motorcycle, there are houses, while a verdant hill lies to the right. A little further, the motorcycle transits into a forested path. Transitioning back to a selfie video, the camera captures the individual who is now next to an electrical transformer, still observing the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 951, "extra_info": {"duration": 5.0, "events": ["Person rides motorcycle in selfie video.", "Person does not wear helmet on motorcycle.", "Motorcycle accelerates on road.", "Houses appear on left side of motorcycle.", "Verdant hill lies to right of motorcycle.", "Motorcycle enters forested path.", "Person stands next to electrical transformer."], "n_events": 7, "n_shots": 3, "n_subjects": 1}, "vid": 951}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a pan containing some sort of liquid mixture positioned on a stove. A rope is hung over the pan. Behind the stove, a yellow bag of potato chips with an attached string can be seen. Next to these items on the kitchen counter, there are red potato chips and different kinds of boxed food items. The pan contents are heated, forming a black substance. As the heat intensifies, smoke begins to leak and the black substance amasses at the bottom of the pan.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 952, "extra_info": {"duration": 11.0, "events": ["A pan with liquid mixture is positioned on a stove", "A rope is hung over the pan", "A yellow bag of potato chips is seen with an attached string", "Red potato chips and boxed food items are placed on the counter", "The pan's contents are heated", "A black substance forms in the pan", "Smoke begins to leak from the pan", "The black substance amasses at the pan's bottom"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 952}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases four motorcycle riders racing on a dirt track. Each rider expertly drifts their motorcycles around the bends on the track. The rider with the white helmet initially leads the race but is overtaken as the rider with the blue helmet successfully maneuvers a bending corner. Later, while traversing a flat stretch, they enter another bend; the rider with the red helmet now surpasses the white helmet rider. Consequently, the blue helmet rider leads the race, the white helmet rider is in second place, and the red helmet rider is in third place.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 953, "extra_info": {"duration": 11.0, "events": ["Four motorcycle riders race on a dirt track.", "Riders drift motorcycles around bends.", "Rider with white helmet leads the race.", "Rider with blue helmet overtakes on a corner.", "Riders traverse a flat stretch.", "Rider with red helmet surpasses the white helmet rider.", "Blue helmet rider leads the race.", "White helmet rider is in second place.", "Red helmet rider takes third place."], "n_events": 9, "n_shots": 2, "n_subjects": 4}, "vid": 953}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a person with brown hair, sitting on the rocks positioned above a stream in a forest. The person is surrounded by numerous trees and sizable boulders, stretching their feet towards the stream. The viewer can see the individual's blue shorts as they lower their left leg into the cool stream water, apparently preparing to immerse themselves in it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 954, "extra_info": {"duration": 5.0, "events": ["Person sits on rocks.", "Person stretches feet towards stream.", "Person lowers left leg into water."], "n_events": 3, "n_shots": 4, "n_subjects": 1}, "vid": 954}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a person with long hair tied back using a blue headdress cord, wearing blue and white striped pajamas. The setting is a bedroom with a gray bed featuring a gray headboard and white bedding. The bed is flanked by gray lights on either side. Firstly, the person places 4 gray pillows on the ground to the left of the bed. Next, the person moves to the right side of the bed to continue repositioning more pillows on the ground. Subsequently, a gray quilt is spread out over the bed. Lastly, the 4 pillows initially set on the ground are divided into two piles and stacked neatly.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 955, "extra_info": {"duration": 8.0, "events": ["Person places 4 gray pillows on the ground.", "Person moves to the right side of the bed.", "Person repositions more pillows on the ground.", "Person spreads a gray quilt over the bed.", "Person stacks 2 piles of pillows neatly."], "n_events": 5, "n_shots": 1, "n_subjects": 1}, "vid": 955}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual with short hair is seen sporting a necklace, a white T-shirt, jeans, and glasses standing in a bathroom. They showcase a 'Loreal' brand mascara to the camera, which they are holding in their hand. On the bathroom counter, one can notice a makeup bag and a small stick immersed in some kind of yellow liquid. The camera angle then shifts, showing this person adjusting the white mirror, removing their glasses, and preparing to apply makeup.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 956, "extra_info": {"duration": 6.0, "events": ["The individual showcases a 'Loreal' mascara to the camera.", "The individual adjusts the white mirror.", "The individual removes their glasses.", "The individual prepares to apply makeup."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 956}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual with long hair is seen. They are wearing a red t-shirt, which has the word 'family' written on it in white lettering and depicts a knight's helmet symbol. They also have on black tights displaying the Nike symbol, and are accessorizing with a white watch. The person walks by a beige sofa, an armchair, and a black table, all situated on a terrace, while carrying a tin container and a cloth bag. They proceed to descend wooden stairs, clad in their white shoes. Further on, this person heads towards a chicken coop to feed the chickens with the contents from the bag. The video shows the chickens inside the coop, near the fence. The scene also includes trees inside the coop, under which the chickens are gathered.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 957, "extra_info": {"duration": 6.01, "events": ["Individual walks by a beige sofa.", "Individual walks by an armchair.", "Individual walks by a black table.", "Individual carries a tin container.", "Individual carries a cloth bag.", "Individual descends wooden stairs.", "Individual heads towards a chicken coop.", "Individual feeds the chickens.", "Chickens gather under the trees."], "n_events": 9, "n_shots": 4, "n_subjects": 1}, "vid": 957}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a projection of text combined with an \"eighty-five percent\" sign. Several individuals, seated, are seen applauding. Subsequently, an individual with long hair, attired in a pink jacket, white trousers, and white shoes approaches the projected image amidst the sound of applause.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 958, "extra_info": {"duration": 5.0, "events": ["Text projects onto a screen.", "An 'eighty-five percent' sign appears.", "Several individuals applaud while seated.", "An individual with long hair walks forward.", "The individual wears a pink jacket.", "The individual dons white trousers and shoes.", "The individual approaches the projected image.", "Applause sounds in the room."], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 958}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features two children, both outfitted in red hoodies with black squares. The child on the left is sporting a red Christmas hat and headphones while engaging in computer games and football. The child on the right, with curly and voluminous hair, presents a yellow cylindrical object and urges the child on the left to unbox it. As the child on the left lifts the lid of the box, a large grey object appearing from inside startles them and they leave the scene. Meanwhile, the child on the right bursts into a joyful dance.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 959, "extra_info": {"duration": 9.0, "events": ["Child on the left engages in computer games.", "Child on the left plays with a football.", "Child on the left wears a red Christmas hat.", "Child on the left uses headphones.", "Child on the right presents a yellow cylindrical object.", "Child on the right urges the other to unbox it.", "Child on the left lifts the lid off the box.", "A large grey object appears from inside the box.", "Child on the left is startled and leaves the scene.", "Child on the right bursts into a dance."], "n_events": 10, "n_shots": 2, "n_subjects": 2}, "vid": 959}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "There is an animated moon, a house and trees on the screen. Then, a blonde animated character wearing a red cloak with black dots and a blue outfit appears to be doing something with hands on a white area with black parts on it. The character with a bag with a picture of a cat and its paw on it is seen from different angles, then the character's black bandana and t-shirt with a smiley face symbol appear while the character laughs and runs on a gray floor. Then the character goes through a door with a black frame and a white middle.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 960, "extra_info": {"duration": 7.01, "events": ["The moon appears on the screen.", "A house and trees appear on the screen.", "A blonde animated character wearing a red cloak with black dots emerges.", "The character does something with hands on a white area with black parts.", "The character with a cat picture bag is seen from different angles.", "The character wearing a black bandana and t-shirt laughs.", "The character runs on a gray floor.", "The character goes through a door with a black frame."], "n_events": 8, "n_shots": 3, "n_subjects": 3}, "vid": 960}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person with blonde hair, wearing a black T-shirt, white bracelet and black shorts throws off an orange bedspread and slowly gets out of bed. This person goes to the window, opens the curtain and fills it with sunlight. Then, this person walks to the other window next to the mirror and opens the curtain of that window. Then this person enters the bathroom through a door with towels hanging on it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 961, "extra_info": {"duration": 10.01, "events": ["Person throws off orange bedspread", "Person gets out of bed", "Person opens the curtain", "Sunlight fills the room", "Person walks to the other window", "Person opens the second curtain", "Person enters the bathroom", "Towels hang on the door"], "n_events": 8, "n_shots": 4, "n_subjects": 1}, "vid": 961}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts an individual with curly hair, wearing a white t-shirt, carrying another person sporting a pink bandana and a pink t-shirt on their back in a kitchen. The curly-haired individual appears to be jostling the other person. Subsequently, a third person, a child in a black and pink hoodie, appears in a different room. The curly-haired individual then shares some words that lead the child in the black and pink hoodie to begin crying on a black sofa. The child wears a white bandana on their head.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 962, "extra_info": {"duration": 10.01, "events": ["Individual with curly hair carries person with pink bandana.", "Curly-haired individual jostles person on their back.", "Child in black and pink hoodie appears in room.", "Curly-haired individual speaks to child.", "Child begins crying on black sofa."], "n_events": 5, "n_shots": 4, "n_subjects": 3}, "vid": 962}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays Formula One vehicles speeding on a track, with a large forest in the background. As the leading cars zip around the bend at high speed, the trailing vehicles are also negotiating the curve. Some cars take a broader path around the next corner, while others, aiming to be faster and billowing smoke, navigate the turn more narrowly. Meanwhile, a blue car assumes the lead in the race.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 963, "extra_info": {"duration": 6.0, "events": ["Formula One vehicles speed on a track", "Cars zip around the bend", "Leading cars navigate the curve at high speed", "Trailing vehicles negotiate the curve", "Some cars take a broader path around the corner", "Other cars navigate the turn more narrowly", "Cars billow smoke while turning", "A blue car assumes the lead"], "n_events": 8, "n_shots": 1, "n_subjects": 1}, "vid": 963}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a person's video camera and the inside of a car. First of all, the steering wheel with the AS emblem on of the black-coated car is visible, and then the instrument panel. Then the front view of this car with red exterior trim is shown. Afterwards, a shot is seen from the side mirrors, and at this time, it is seen that a white car is coming behind the red car. Afterwards, the front part of this white car is visible from the exterior shot and the rims of this car are black. Afterwards, his path is seen from a different perspective. Afterwards, various lighting and a bridge appear on another road where it is dark. The grey rim of this Alfa Romeo brand car and the emblem and red part inside is shown. Then, finally, the rearview and exhaust of the red car are shown.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 964, "extra_info": {"duration": 11.0, "events": ["Camera pans from the steering wheel to the instrument panel.", "Camera captures the front view of the red-trimmed car.", "White car appears in the side mirror.", "Front of the white car becomes visible from the outside.", "Camera view switches to a different road perspective.", "Various lighting and a bridge appear on the road.", "Camera shows the Alfa Romeo's grey rim and red interior.", "Camera reveals the rearview and exhaust of the red car."], "n_events": 8, "n_shots": 10, "n_subjects": 1}, "vid": 964}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a brown-haired person wearing a Superman costume and a blonde person wearing a gray-throated hudi. The person wearing the Superman costume is trying to eat a yellow meal from a blue plate on a burgundy table with a knife in his hand. There is a broken eggshell next to this yellow dish. Meanwhile, the person with the grey hood laughs. Behind them are firefighter helmets and a colourful ball.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 965, "extra_info": {"duration": 4.97, "events": ["Brown-haired person tries to eat a yellow meal", "Brown-haired person holds a knife", "Broken eggshell rests beside the yellow dish", "Blonde person laughs", "Firefighter helmets are located behind the people", "Colourful ball is present in the background"], "n_events": 6, "n_shots": 5, "n_subjects": 2}, "vid": 965}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual with short hair is seen wearing a dark blue t-shirt with black stripes, black shorts, and dark blue shoes featuring leaf pattern. On the right wrist of the individual is a red watch, while on the left wrist there is a black watch. Both hands also sport a ring each. Additionally, a red cord is tied around the right foot. The individual is spotted sliding leftwards on a black skateboard. After crouching down briefly, the person jumps up to the first step, glides along the edge of the stairs to some distance, pivots, and leaps down once more. The movement of this person is from the right towards the left. Due to the potential risk involved, it is advised against imitating such high-risk maneuvers on skateboard.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 966, "extra_info": {"duration": 4.96, "events": ["Individual slides leftwards on a skateboard", "Individual crouches down briefly", "Individual jumps up to the first step", "Individual glides along the edge of the stairs", "Individual pivots", "Individual leaps down from the stairs"], "n_events": 6, "n_shots": 2, "n_subjects": 2}, "vid": 966}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts two individuals on the front lawn of a house. The individual on the left is seen with short hair, protective glasses, ear protectors, a green t-shirt featuring black text on the back, and plaid gray shorts. They wield a black handgun. The second individual, positioned on the right and towards the fence, is dressed in a black and white cap, dark green t-shirt, blue jeans, and ear protectors. They instinctively shield their right arm before collapsing onto the grass, subsequently kneeling. In response, the person on the left rushes over to their side.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 967, "extra_info": {"duration": 8.0, "events": ["The individual on the left wields a black handgun.", "The second individual shields their right arm.", "The second individual collapses onto the grass.", "The second individual subsequently kneels.", "The individual on the left rushes over."], "n_events": 5, "n_shots": 1, "n_subjects": 2}, "vid": 967}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a white candle, ensconced atop a brown table, whose flame suddenly extinguishes. The wick, however, continues to glow and burn for a few more seconds before turning black and leaving behind a thin trail of smoke.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 968, "extra_info": {"duration": 5.0, "events": ["A white candle rests atop a brown table.", "The candle's flame suddenly extinguishes.", "The candle's wick continues to glow.", "The wick burns for a few more seconds.", "The wick turns black.", "A thin trail of smoke rises from the wick."], "n_events": 6, "n_shots": 1, "n_subjects": 1}, "vid": 968}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A hand, clad in a blue shirt, is seen holding a plastic container filled with a turmeric-colored liquid. The individual shakes the container before placing it on a table. Subsequently, a second person with dark skin inserts a pink straw into the container and begins to blow into it. The liquid changes color to red, but after continued blowing, it reverts back to its original turmeric color.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 969, "extra_info": {"duration": 12.0, "events": ["A hand holds a plastic container.", "The individual shakes the plastic container.", "The hand places the plastic container on a table.", "A second person inserts a pink straw into the container.", "The second person blows into the straw.", "The liquid changes color to red.", "The liquid reverts back to turmeric color."], "n_events": 7, "n_shots": 2, "n_subjects": 2}, "vid": 969}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "Very close-view of the setting sun, depicted as a bright, partially obscured object. The upper portion of the sun is a blend of yellow and orange with a few dark spots, while the lower sections exhibit a redder hue. Two dots can be seen on the sun. The surrounding scenery is enveloped in darkness. The sun, appearing as a luminous object, progressively moves towards the lower right, gradually disappearing from sight. It gives the impression of a glowing entity slowly descending.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 970, "extra_info": {"duration": 11.0, "events": ["The sun sets.", "The sun moves towards the lower right.", "The sun gradually disappears from sight."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 970}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video commences with scenes of a cloudy, lightning-filled atmosphere. The setting is a grassy area where the weather transitions from lightning conditions to an intense tornado, extending from thunderstorms down to the ground. The scene is filled with high-velocity winds swirling around nearby houses. In this tumult, two individuals in blue wind-resistant clothing struggle against the hostile conditions. One of these individuals is seen with a watch on his left hand.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 971, "extra_info": {"duration": 10.0, "events": ["Clouds fill the atmosphere with lightning.", "Weather changes to an intense tornado.", "Tornado extends from thunderstorms to the ground.", "High-velocity winds swirl around houses.", "Two individuals struggle against the winds.", "One individual checks the watch on left hand."], "n_events": 6, "n_shots": 3, "n_subjects": 2}, "vid": 971}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts an interaction between an individual with long hair and one with short hair. The long-haired person says something, resulting in the short-haired person seeming helpless and unhappy.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 972, "extra_info": {"duration": 8.0, "events": ["Individual with long hair speaks.", "Individual with short hair appears helpless.", "Individual with short hair looks unhappy."], "n_events": 3, "n_shots": 4, "n_subjects": 2}, "vid": 972}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "At home, there are two kids sitting on the sofa, both with blue shirt and curly black hair. The loser of them, who is on the left side wears blue tape on the head, a necklace, and a watch on their right hand. The kids are playing rock-paper-scissors, so both of them show the gestures of scissors after hitting their left hands with their right hands for 3 times, and then for the fourth time, the kid on the right shows the gesture of scissors while the kid on the left shows the paper. Then the kid on the right is happy and the kid on the left with the necklace is getting upset because the kid on the right won the game of rock-paper-scissors after 3 rounds draw. Then the other kid started smiling.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 973, "extra_info": {"duration": 8.98, "events": ["Two kids sit on the sofa.", "Both kids hit left hands with right hands three times.", "Kids show scissors gesture simultaneously.", "Kid on the right shows scissors again.", "Kid on the left shows paper.", "Kid on the right becomes happy.", "Kid on the left with a necklace gets upset.", "Kid on the right starts smiling."], "n_events": 8, "n_shots": 7, "n_subjects": 2}, "vid": 973}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features two pandas sitting on the grass attempting to solve a puzzle. The puzzle consists of four pieces that together form a picture of a watermelon. The finished version of the puzzle is visible in the top left corner of the screen. Both pandas are actively engaging in matching the puzzle pieces, with their activities indicated by a blinking red cross on the screen. The panda on the right, notable for its pink bow, is struggling with the placement of puzzle pieces. On the left, the other panda is seen laughing at the challenge the right panda seems to be experiencing. The setting includes green trees in the backdrop, with scattered daisies on the grassy floor.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 974, "extra_info": {"duration": 6.0, "events": ["Two pandas sit on the grass.", "Pandas attempt to solve a puzzle.", "Puzzle pieces form a picture of a watermelon.", "A finished puzzle version is visible.", "A blinking red cross indicates activities.", "Panda with a pink bow struggles with pieces.", "Other panda laughs at the challenge.", "Green trees stand in the backdrop.", "Daisies scatter on the grassy floor."], "n_events": 9, "n_shots": 2, "n_subjects": 2}, "vid": 974}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In a Minecraft game scenario, another player controls their character, a green entity with a cube-shaped head wearing brown overalls. The character shakes their head left and right, moving around a green area aiming to reach a stand labeled \"life\". Unexpectedly, the character begins to fly. It remains static with its head, yet rotates its limbs and torso in an impossible real-life manner, ascending into the blue sky above. In the same stand, the text \"How did she?\" can be seen.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 975, "extra_info": {"duration": 8.0, "events": ["Character shakes head left and right.", "Character moves around a green area.", "Character aims to reach a stand labeled 'life'.", "Character unexpectedly begins to fly.", "Character rotates limbs and torso while ascending.", "Character ascends into the blue sky."], "n_events": 6, "n_shots": 4, "n_subjects": 1}, "vid": 975}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video portrays an individual in a lilac cardigan, dusting a green nightstand. They use a green duster and a white napkin in their cleaning process. The nightstand is adorned with various decorative items and electronics. Behind the nightstand is a window, with a white curtain embroidered in yellow hanging adjacent to it.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 976, "extra_info": {"duration": 4.97, "events": ["Individual dusts a green nightstand with a green duster.", "Individual wipes the nightstand with a white napkin.", "Curtain hangs adjacent to the window."], "n_events": 3, "n_shots": 1, "n_subjects": 1}, "vid": 976}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases a blue car parked on a black asphalt road enveloped in grey smoke. Following this, another car can be seen driving along a three-lane asphalt road, occupying the middle lane. After overtaking a few stationary cars in the left lane, the car approaches a highway, seemingly about to collide with the blue car that's moving erratically across its path.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 977, "extra_info": {"duration": 10.0, "events": ["Blue car parked on the road.", "Grey smoke envelops the blue car.", "Another car drives along the three-lane road.", "The car occupies the middle lane.", "The car overtakes stationary cars in the left lane.", "The car approaches a highway.", "Blue car moves erratically.", "Cars seem about to collide."], "n_events": 8, "n_shots": 2, "n_subjects": 3}, "vid": 977}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video the driver is driving normally, the snow on the windshield may affect the line of sight, in the intersection appeared a car driving with lights on and this car is turning. The oncoming car with lights on should stop and wait the other car to pass through according to traffic rules but it didn't. The vehicle shooting video had no time to brake , and the two cars is about to have an accident at the cross.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 978, "extra_info": {"duration": 7.0, "events": ["Driver drives with snow on windshield", "Snow affects line of sight", "Car at intersection turns with lights on", "Oncoming car with lights on breaks traffic rule", "Oncoming car with lights on fails to stop", "Vehicle recording video approaches the intersection", "Driver in video has no time to brake", "Two cars are about to collide at the cross"], "n_events": 8, "n_shots": 1, "n_subjects": 2}, "vid": 978}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video demonstrates a vehicle, from which the recording is taking place, attempting to overtake a black car on the right. Unexpectedly, the black car swerves to the left causing an unforeseen accident. Losing control over its direction, the recording vehicle crashes into the trees on the right side of the road.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 979, "extra_info": {"duration": 5.0, "events": ["The vehicle attempts to overtake a black car on the right", "The black car swerves to the left unexpectedly", "The vehicle loses control", "The vehicle crashes into the trees on the right side"], "n_events": 4, "n_shots": 1, "n_subjects": 2}, "vid": 979}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a room with light gray walls. A white wardrobe stands at one end, with various toys scattered across the floor in front of it. Two white windows, adorned with gauzy white curtains, let in daylight. A circular, brown storage shelf is positioned under the window, with a white unicorn decoration on the upper right wall. Two framed pictures hang on the right wall. A toy box brimming with toys is tucked into the bottom right corner of the room, flanked by two light khaki polka-dotted pillows. Adjacent to the toy box, a white bed is furnished with two pink pillows and blankets in shades of pink, blue, and velvet. A pink quilt speckled with white polka dots covers miscellaneous items, hangers, and unwashed clothes. Beside the bed, a white dresser stands with six drawers. On top of it, a white lamp and other assorted items rest. A small round table, hosting a potted plant and random knick-knacks, is placed on the left of the dresser. Above the dresser, the wall features a round mirror framed by strings of tiny light bulbs. A small white chair is situated on the right of the dresser.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 980, "extra_info": {"duration": 11.01, "events": [], "n_events": 0, "n_shots": 2, "n_subjects": 1}, "vid": 980}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a kitchen with red wooden flooring and white doors. Upon entering, there is a long black countertop positioned on the left, with a trash can tucked beneath it. Several kitchen papers and two white plastic bags are scattered on the countertop. Encased between the plastic bags and kitchen papers are a small white mop and a small green broom, both with wooden handles. A grey toaster is situated in front of the wall. The countertop houses two sinks with a faucet located between them, facing the window. Some white flowers in a glass vase and a bunch of bananas are displayed in front of the window. In the top right corner of the connected sinks, a yellow dish soap bottle and a blue dish soap bottle can be seen. The left sink houses white dishes and three glass cups that require cleaning. A black refrigerator is positioned near the table.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 981, "extra_info": {"duration": 8.01, "events": [], "n_events": 0, "n_shots": 1, "n_subjects": 1}, "vid": 981}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video displays a player attacking an opponent using dual guns. The opponent retaliates by firing back. Subsequently, the opponent ducked, quickly rolled behind a stone carving for cover, and surprisingly reemerged close to the player, attacking fiercely. Meanwhile, the player skillfully twirled the guns, fired back and got hit.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 982, "extra_info": {"duration": 6.0, "events": ["Player attacks opponent with dual guns", "Opponent fires back at player", "Opponent ducked for cover", "Opponent rolled behind a stone carving", "Opponent reemerged near the player", "Opponent attacked player fiercely", "Player twirled the guns", "Player fired back at opponent", "Player got hit by opponent"], "n_events": 9, "n_shots": 1, "n_subjects": 3}, "vid": 982}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The footage showcases gameplay featuring the character Jjonak from the game Overwatch. The player deftly backtracks, wields the sword, and then proceeds toward the adversaries through a tunnel. The character has a blue circle adorning its back. Simultaneously, the grey adversary in front of the yellow character fires red rays at the yellow character. Swiftly, the yellow character lunges forward, shattering the grey opponent's shield, thereby emitting a yellow glow. The yellow character then strikes the opponent against the wall near the open door, causing the opponent's defeat.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 983, "extra_info": {"duration": 5.0, "events": ["Jjonak backtracks in the gameplay.", "Jjonak wields the sword.", "Jjonak moves through the tunnel.", "Blue circle adorns Jjonak's back.", "Grey adversary fires red rays at Jjonak.", "Jjonak lunges forward.", "Jjonak shatters the grey adversary's shield.", "Shield emits a yellow glow.", "Jjonak strikes the adversary against the wall.", "Grey adversary is defeated near the open door."], "n_events": 10, "n_shots": 1, "n_subjects": 3}, "vid": 983}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, an individual donned in a red sweater and checkered pants takes note of the Christmas tree in the room, which is aflame. The person appears terrified and begins to frantically search the room for something useful.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 984, "extra_info": {"duration": 5.0, "events": ["Individual takes note of the aflame Christmas tree.", "Individual appears terrified.", "Individual begins to search the room.", "Individual frantically searches for something useful."], "n_events": 4, "n_shots": 2, "n_subjects": 1}, "vid": 984}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, a player dressed in a green t-shirt, black shorts and white shoes is seen holding a ping-pong ball, poised to serve it towards a black ping-pong table. To the right of the player, there is a purple table and a referee dressed in a blue shirt. The player in the blue shirt and shorts receives the ball and hits it once before landing a powerful hit that scores them a point. This player, visibly elated, lets out a shout, clenching their fist in celebration. A participant in a black shirt and others in grey cardigans mimic this celebratory gesture. The champion player is seen at the top of the game screen, aligned with the players in black off the sidelines.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 985, "extra_info": {"duration": 10.0, "events": ["Player holds a ping-pong ball.", "Player serves the ball towards the table.", "Opponent in blue shirt receives the ball.", "Opponent hits the ball once.", "Opponent lands a powerful hit.", "Opponent scores a point.", "Opponent in blue shirt shouts in celebration.", "Opponent clenches their fist.", "Participant in black shirt mimics the celebration.", "Others in grey cardigans mimic the celebration."], "n_events": 10, "n_shots": 4, "n_subjects": 3}, "vid": 985}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a sinister scene where a colossal snake's head descends from the ceiling, fiercely bites into a blue-shirted blue-collar worker, drawing blood and dragging the worker towards the ceiling. On the video's right side, two individuals, who both have black-strapped backpacks on, stand rooted at the door of the cockpit, faces etched with horror. On the left, one individual with long hair is attired in a green jacket, white T-shirt and blue jeans. The other person, with a short haircut, wears a gray T-shirt under a black coat and extends a hand protectively towards the long-haired individual.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 986, "extra_info": {"duration": 7.0, "events": ["Snake's head descends from the ceiling.", "Snake bites into a blue-shirted worker.", "Snake draws blood from the worker.", "Snake drags the worker towards the ceiling.", "Two individuals stand at the cockpit door.", "Individuals' faces show horror.", "Person in green jacket appears on the left.", "Person with short haircut extends a hand protectively."], "n_events": 8, "n_shots": 3, "n_subjects": 4}, "vid": 986}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, an individual wearing glasses along with a white gown and red vest is shown receiving information from another person sporting a gray coat and a beard. The bearded person sits opposite the glasses-wearing individual. Secretly, the individual in glasses gestures with his fingers and left hand towards a distant sniper. Spotting the signal, the sniper who previously had his gun targeted, discontinues his action and refrains from taking the shot.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 987, "extra_info": {"duration": 6.0, "events": ["Individual in glasses receives information from bearded person", "Bearded person sits opposite individual in glasses", "Individual in glasses gestures with fingers", "Individual in glasses gestures with left hand", "Individual in glasses signals to distant sniper", "Sniper previously targets with gun", "Sniper spots the signal", "Sniper discontinues his action", "Sniper refrains from taking the shot"], "n_events": 9, "n_shots": 4, "n_subjects": 2}, "vid": 987}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows an individual wearing blue pants, a red shirt, and a red cap participating in a race. He is initially behind six other runners – three on his left and three on his right – in an orange field. This individual, referred to as Mario, performs a spinning motion while sprinting ahead, successfully overtaking all the runners and securing the first position, leaving a trail of blue circles behind him.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 988, "extra_info": {"duration": 5.0, "events": ["Mario participates in a race.", "Mario wears blue pants, a red shirt, and a cap.", "Mario starts behind six other runners.", "Runners are in an orange field.", "Mario performs a spinning motion.", "Mario sprints ahead of the competitors.", "Mario overtakes all the runners.", "Mario secures the first position.", "Mario leaves a trail of blue circles."], "n_events": 9, "n_shots": 1, "n_subjects": 2}, "vid": 988}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In the video, the dark-skinned person with long orange hair, wearing a red shirt and black jacket and purple glasses, says a few words and immediately transforms into a white person with long blond hair and a ponytail wearing a blue headband, white top and blue skirt in the grassy area.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 989, "extra_info": {"duration": 5.0, "events": ["The person speaks.", "The person transforms.", "The transformed person appears in a grassy area."], "n_events": 3, "n_shots": 2, "n_subjects": 2}, "vid": 989}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features a brown cat, donned in a white hat, licking its left front paw inside a glass box. In an unexpected turn of events, a black mouse drops from the top right. The cat, which is bedecked with a white napkin around its neck and a petite white hat, watches as the mouse frantically tries to escape. Abruptly, two cartoon characters pop into sight, both sporting brilliant white attire and hats against their black skin. The character found on the right then takes a shot at the mouse with a yellow pistol, unleashing a laser beam. The mouse, exposed to the laser, diminishes immediately in size.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 990, "extra_info": {"duration": 9.0, "events": ["Brown cat licks its left front paw.", "Black mouse drops from the top right.", "Brown cat watches the mouse trying to escape.", "Two cartoon characters pop into sight.", "Cartoon character on the right takes a shot at the mouse.", "Yellow pistol unleashes a laser beam.", "Laser beam hits the mouse.", "Mouse immediately diminishes in size."], "n_events": 8, "n_shots": 4, "n_subjects": 4}, "vid": 990}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows astronauts in the cockpit of a spacecraft, manipulating several buttons on the control panel which results in the spacecraft shaking violently. An astronaut tries to stabilize the craft by adjusting a control lever, but their terrified and scared expressions indicate the dire state of the situation.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 991, "extra_info": {"duration": 10.0, "events": ["Astronauts manipulate buttons on the control panel.", "Control panel usage causes spacecraft to shake violently.", "Astronaut adjusts a control lever.", "Astronauts display terrified expressions.", "Astronauts attempt to stabilize the spacecraft."], "n_events": 5, "n_shots": 6, "n_subjects": 2}, "vid": 991}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, an individual donned in a brown suit jacket is seen obstinately attempting to operate a television using a remote control. Another individual, clad in a black suit jacket, repeatedly advises him that the remote does not control the TV. Despite this, the person continues to depress the remote's buttons. Eventually, a transparent swan ornament, perched atop the TV, is ejected by a suddenly-opening cover. The swan ornament collides with a hanging glass photo frame, containing a picture of a family – two people and a white dog – causing the glass to shatter.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 992, "extra_info": {"duration": 8.01, "events": ["Individual tries to operate TV with remote.", "Other individual advises remote doesn't control the TV.", "Individual presses remote's buttons.", "TV cover suddenly opens.", "Swan ornament is ejected from TV.", "Swan ornament collides with photo frame.", "Glass photo frame shatters."], "n_events": 7, "n_shots": 9, "n_subjects": 3}, "vid": 992}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, a person wearing a brown suit is observing an oil painting hung on a red wall. The painting has a wooden frame and it portrays an elderly person with long white hair, dressed in a black gown. A green plant can be seen beside the painting, placed on the left side. A gray wall is depicted in the painting's background, with a small picture hung in a black frame at the upper left corner. Suddenly, the person in the brown suit sneezes towards the painting, and upon contact, the spray lands on the painting's face, leaving it marked with water and dirt. The person, instinctively touching their nose after the sneeze, is astonished to see the blemish on the painting.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 993, "extra_info": {"duration": 6.01, "events": ["A person observes an oil painting.", "The oil painting portrays an elderly person.", "A green plant is placed beside the painting.", "The person in the brown suit sneezes towards the painting.", "Spray lands on the painting's face.", "The painting is marked with water and dirt.", "The person touches their nose after sneezing.", "The person is astonished to see the blemish."], "n_events": 8, "n_shots": 4, "n_subjects": 2}, "vid": 993}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, an individual dressed in a white shirt, black tie, and black pants approaches a vending machine, clutching a piece of paper bearing a black circle. The individual places the paper on the glass cabinet door of the machine. Astonishingly, they extend their right hand into the black circle on the paper and pass through the glass door, thereby directly retrieving a snack from the machine.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 994, "extra_info": {"duration": 7.0, "events": ["Individual approaches a vending machine.", "Individual clutches a piece of paper.", "Piece of paper bears a black circle.", "Individual places the paper on the glass door.", "Individual's right hand extends towards the paper.", "Hand passes through the glass door.", "Individual retrieves a snack from the machine."], "n_events": 7, "n_shots": 4, "n_subjects": 1}, "vid": 994}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "A person wearing yellow-striped flip-flops and black socks bends down, breaks off the sodium metal from the tip of a stick on a newspaper, and prepares it to throw it into the water in a plastic bottle. The person performs this experiment on a soil surface with fallen tree leaves. To be careful, the person wearing flip-flops and a gray tracksuit plucks the metal from the end of the stick with their right hand, bends down, cautiously extends their right arm, and puts it into the plastic bottle. Reacting violently with water, this large piece of metal first emits white smoke and then explodes, sending the bottle crashing to the ground.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 995, "extra_info": {"duration": 12.0, "events": ["Person breaks off sodium metal on newspaper", "Person prepares sodium for tossing into water", "Person plucks sodium metal with right hand", "Person bends down with sodium metal", "Person cautiously extends right arm with sodium", "Person puts sodium into the plastic bottle", "Sodium reacts violently with water", "White smoke emits from the bottle", "Sodium explosion occurs", "Bottle crashes to the ground"], "n_events": 10, "n_shots": 1, "n_subjects": 1}, "vid": 995}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "In this video, an individual donned in an orange shirt and gray shorts is seen placing a black and white football on the grass. A referee dressed in a blue shirt and black pants stands behind the player. The player in orange takes a run up and kicks the ball with his right foot. The ball soars towards the goal, and the goalkeeper clad in yellow clothing and blue gloves raises both hands in an attempt to block it, but fails to do so. The ball zooms past the goalkeeper, into the net, catching fire and then transforming into a burst of vibrant fireworks. A person dressed in red cheers on the grass, raising both hands in celebration.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 996, "extra_info": {"duration": 8.0, "events": ["Player places football on grass", "Player kicks ball with right foot", "Ball soars towards goal", "Goalkeeper attempts to block ball", "Goalkeeper fails to stop ball", "Ball zooms into net", "Ball catches fire", "Ball transforms into fireworks", "Person in red cheers raising hands"], "n_events": 9, "n_shots": 4, "n_subjects": 4}, "vid": 996}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video showcases an individual on the left with long, curly brown hair, dressed in a white sweater, carrying books, walking towards the right. An individual on the right, outfitted in a black coat and blue jeans, possesses a monitor for a head, displaying a constant succession of different profile pictures. The individual on the right abruptly knocks the books from the hands of the person on the left and aggressively shoves the latter against a red wall.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 997, "extra_info": {"duration": 6.01, "events": ["Individual with books walks towards the right.", "Individual with monitor head displays profile pictures.", "Individual with monitor head knocks books from individual's hands.", "Individual with monitor head shoves individual against red wall."], "n_events": 4, "n_shots": 4, "n_subjects": 2}, "vid": 997}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video shows a child with short blond hair in a room, wearing a beige t-shirt, and carrying pants, walking towards a chest of drawers. A second child, with curly black hair dressed in a pink dress, is also present. The room features a chest of drawers, toys, and dolls scattered on the floor. Additionally, two children in purple pyjamas are trying to climb the white chest of drawers, introducing the risk of it falling over them, near a yellow toy truck. One of these children opens the uppermost drawer, while the child in the pink dress attempts to climb onto a black table situated under a screen. These activities are considered high-risk due to the potential of injury from tumbling furniture.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 998, "extra_info": {"duration": 6.0, "events": ["Child with short blond hair walks towards chest of drawers.", "Child with curly black hair tries to climb black table.", "Two children in purple pyjamas try to climb chest of drawers.", "One child in purple pyjamas opens uppermost drawer of chest.", "Toys and dolls are scattered on the floor.", "Child in pink dress approaches black table.", "Child risks furniture tumbling while climbing.", "Child potentially injures self by climbing furniture.", "Child in beige t-shirt carries pants.", "Child near yellow toy truck introduces falling risk."], "n_events": 10, "n_shots": 6, "n_subjects": 4}, "vid": 998}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video depicts a blonde-haired person, sporting a black swimsuit and a ponytail, standing on an orange jumping platform. With their back to the right, they face left and raise their hands. Following that, they jump, wrapping their arms around their legs and performing a spin in mid-air. They conclude by diving into the water, hand-first and feet-up, creating a considerable splash.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 999, "extra_info": {"duration": 6.0, "events": ["Blonde-haired person stands on jumping platform.", "Person faces left.", "Person raises hands.", "Person jumps from platform.", "Person wraps arms around legs.", "Person performs a spin in mid-air.", "Person dives into the water.", "Person enters water hand-first.", "Person's feet go up.", "Splash occurs in water."], "n_events": 10, "n_shots": 1, "n_subjects": 2}, "vid": 999}
{"messages": [{"content": [{"reference": null, "text": null, "type": "video", "video": {"video_file": "<placeholder>"}}, {"reference": null, "text": "Describe the video in detail.", "type": "text", "video": null}], "role": "user"}, {"content": [{"reference": "The video features animated characters: a large, overweight gray rabbit and a purple squirrel in a brown field surrounded by greenery and trees. The gray rabbit directs the purple squirrel to a certain location. The purple squirrel starts heading there with a hazelnut in its hand, displaying signs of fear. Stopping at a small green bush, the squirrel seems to inspect it. The camera then pulls back, revealing the bush as a wooden trap. The trap springs, launching the purple squirrel into the air and making it drop the hazelnut. The scene then shifts to the squirrel flying through the air.", "text": "", "type": "text", "video": null}], "role": "assistant"}], "dataset": "DREAM/youtube", "task": "video/caption", "idx": 1000, "extra_info": {"duration": 12.0, "events": ["Gray rabbit directs purple squirrel to a location.", "Purple squirrel carries hazelnut.", "Purple squirrel displays fear.", "Purple squirrel stops at green bush.", "Purple squirrel inspects green bush.", "Bush reveals itself as wooden trap.", "Wooden trap springs.", "Wooden trap launches purple squirrel into the air.", "Purple squirrel drops hazelnut."], "n_events": 9, "n_shots": 2, "n_subjects": 2}, "vid": 1000}
